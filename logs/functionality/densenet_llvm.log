nohup: ignoring input
[16:04:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_multiply_add_multiply_add_nn_relu"
[16:04:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 248, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 248, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 248, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 248, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 248, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 248, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 248, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 248, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[16:04:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 248, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 992, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 7, 7, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 248, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 4, 1, 1, 7, 1, 1, 62, 1, 1, 1, 4, 1, 7, 1, 16, 1, 1, 1, 8, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_0])
                    ic = T.axis.reduce(992, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 7, 7, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[62, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 248, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 4, 1, 1, 7, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(62, 1, 1, 1, 4, 1, 7, 1, 16, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_0])
                        ic = T.axis.reduce(992, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 7, 7, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 7, 1):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(7, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(7, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[62, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 248, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 1, 62, 1, 1, 1, 4, 1, 7, 1, 16, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_0])
                        ic = T.axis.reduce(992, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 7, 7, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 1):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[62, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_multiply_add_multiply_add_nn_relu_1"
[16:04:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[16:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 960, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 2, 1, 4, 1, 1, 1, 240, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 8, 1, 7, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(960, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[240, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 1, 2, 1, 4, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(240, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 8, 1, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(960, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(7, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[240, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 7, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 1, 240, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 8, 1, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(960, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 7, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(7, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[240, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_multiply_add_multiply_add_nn_relu_2"
[16:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 232, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 232, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 232, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 232, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 232, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 232, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 232, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 232, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
[16:04:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 232, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 928, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 7, 7, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 232, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 928, 1, 1, 1, 2, 1, 7, 2, 1, 1, 1, 1, 4, 7, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_2 * 4 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(928, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 7, 7, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[928, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 232, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 4, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(928, 1, 1, 1, 2, 1, 7, 2, 1, 1, 1, 1, 4, 7, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(928, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 7, 7, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 8 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[928, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 232, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 1, 928, 1, 1, 1, 2, 1, 7, 2, 1, 1, 1, 1, 4, 7, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(928, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 7, 7, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[928, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_multiply_add_multiply_add_nn_relu_3"
[16:04:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 224, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 224, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 224, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 224, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 224, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 224, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 224, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 224, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
[16:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 224, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 896, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 7, 7, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 224, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 1, 1, 1, 7, 7, 1, 16, 1, 1, 1, 2, 1, 1, 2, 56, 1, 1, 1, 4, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 4 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(896, i5_0 * 56 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 7, 7, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 56])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 224, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 1, 1, 1, 7, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 1, 1, 2, 56, 1, 1, 1, 4, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(896, i5_0 * 56 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 7, 7, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 56])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 224, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 7, 1, 16, 1, 1, 1, 2, 1, 1, 2, 56, 1, 1, 1, 4, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(896, i5_0 * 56 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 7, 7, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 56])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_multiply_add_multiply_add_nn_relu_4"
[16:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 216, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 216, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 216, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 216, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 216, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 216, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 216, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 216, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
[16:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 216, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 864, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 7, 7, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 216, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 2, 1, 1, 1, 1, 2, 96, 1, 1, 1, 4, 1, 1, 1, 9, 1, 1, 1, 8, 1, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(864, i5_0 * 9 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 7, 7, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 9])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 216, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 1, 2, 1, 1, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(96, 1, 1, 1, 4, 1, 1, 1, 9, 1, 1, 1, 8, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(864, i5_0 * 9 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 7, 7, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 7, 1):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(7, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 9])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 216, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 7, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 96, 1, 1, 1, 4, 1, 1, 1, 9, 1, 1, 1, 8, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(864, i5_0 * 9 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 7, 7, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 7, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(7, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 9])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_multiply_add_multiply_add_nn_relu_5"
[16:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 208, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 208, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 208, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 208, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 208, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 208, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 208, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 208, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
[16:04:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 208, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 832, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 7, 7, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 208, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 8, 1, 1, 2, 64, 1, 1, 1, 4, 7, 7, 1, 13, 1, 1, 1, 1, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2)
                    oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(832, i5_0 * 13 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 7, 7, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 13])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 208, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 8, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 4, 7, 7, 1, 13, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(832, i5_0 * 13 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 7, 7, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 13])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 208, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 2, 64, 1, 1, 1, 4, 7, 7, 1, 13, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(832, i5_0 * 13 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 7, 7, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 13])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_multiply_add_multiply_add_nn_relu_6"
[16:04:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 200, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 200, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 200, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 200, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 200, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 200, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 200, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 200, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
[16:04:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 200, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 800, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 7, 7, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 200, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50, 1, 1, 1, 8, 7, 1, 1, 16, 1, 1, 1, 4, 1, 7, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2 * 4 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_3, i4_3])
                    ic = T.axis.reduce(800, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 7, 7, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 8, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[50, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 200, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(50, 1, 1, 1, 8, 7, 1, 1, 16, 1, 1, 1, 4, 1, 7, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 4 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_3, i4_3])
                        ic = T.axis.reduce(800, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 7, 7, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 8, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[50, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 200, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 50, 1, 1, 1, 8, 7, 1, 1, 16, 1, 1, 1, 4, 1, 7, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 4 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_3, i4_3])
                        ic = T.axis.reduce(800, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 7, 7, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 8, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[50, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_multiply_add_multiply_add_nn_relu_7"
[16:04:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 192, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 192, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 192, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 192, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 192, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 192, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 192, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"
[16:04:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 768, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 192, 7, 7, 4], "float32"], ["TENSOR", [32, 192, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 7, 1, 16, 1, 1, 1, 32, 1, 1, 1, 48, 1, 1, 1, 1, 7, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk, oh, ow = T.axis.remap("SSS", [i1_2, i2_3, i3_1])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(768, i5_0 * 48 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 192, 7, 7, 4], "float32"], ["TENSOR", [32, 192, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 32, 1, 1, 1, 48, 1, 1, 1, 1, 7, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk, oh, ow = T.axis.remap("SSS", [i1_2, i2_3, i3_1])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(768, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 192, 7, 7, 4], "float32"], ["TENSOR", [32, 192, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 1, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 16, 1, 1, 1, 32, 1, 1, 1, 48, 1, 1, 1, 1, 7, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk, oh, ow = T.axis.remap("SSS", [i1_2, i2_3, i3_1])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(768, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 192, 7, 7, 4], "float32"], ["TENSOR", [32, 192, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_multiply_add_multiply_add_nn_relu_8"
[16:04:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 184, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 184, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 184, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 184, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 184, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 184, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 184, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 184, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 184, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"
[16:04:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 184, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 736, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 184, 7, 7, 4], "float32"], ["TENSOR", [32, 184, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 184, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 368, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 1, 8, 1, 7, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(736, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 184, 7, 7, 4], "float32"], ["TENSOR", [32, 184, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[368, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 184, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 1, 1, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(368, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 1, 8, 1, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(736, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 184, 7, 7, 4], "float32"], ["TENSOR", [32, 184, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[368, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 184, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 7, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 368, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 1, 8, 1, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(736, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 184, 7, 7, 4], "float32"], ["TENSOR", [32, 184, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[368, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_multiply_add_multiply_add_nn_relu_9"
[16:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 176, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 176, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 176, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 176, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 176, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 176, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 176, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 176, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 176, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu"
[16:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 176, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 704, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 176, 7, 7, 4], "float32"], ["TENSOR", [32, 176, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 176, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 2, 1, 4, 1, 1, 1, 64, 1, 1, 1, 1, 7, 1, 2, 11, 1, 1, 1, 1, 1, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1)
                    oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(704, i5_0 * 11 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 176, 7, 7, 4], "float32"], ["TENSOR", [32, 176, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l5, l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b0)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l5, factors=[v13, v14, v15, v16])
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 4, 1, 1])
l25, l26, l27, l28 = sch.split(loop=l6, factors=[v21, v22, v23, v24])
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l33, l34, l35, l36 = sch.split(loop=l7, factors=[v29, v30, v31, v32])
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l41, l42, l43, l44 = sch.split(loop=l8, factors=[v37, v38, v39, v40])
v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l49, l50, l51, l52 = sch.split(loop=l9, factors=[v45, v46, v47, v48])
v53, v54 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[64, 11])
l55, l56 = sch.split(loop=l10, factors=[v53, v54])
v57, v58 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l59, l60 = sch.split(loop=l11, factors=[v57, v58])
v61, v62 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 1])
l63, l64 = sch.split(loop=l12, factors=[v61, v62])
sch.reorder(l17, l25, l33, l41, l49, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
[16:04:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 176, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 1, 2, 1, 4, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 1, 7, 1, 2, 11, 1, 1, 1, 1, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(704, i5_0 * 11 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 176, 7, 7, 4], "float32"], ["TENSOR", [32, 176, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 7, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + i1_1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max((conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l5, l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b0)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l5, factors=[v13, v14, v15, v16])
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 4, 1, 1])
l25, l26, l27, l28 = sch.split(loop=l6, factors=[v21, v22, v23, v24])
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l33, l34, l35, l36 = sch.split(loop=l7, factors=[v29, v30, v31, v32])
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l41, l42, l43, l44 = sch.split(loop=l8, factors=[v37, v38, v39, v40])
v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l49, l50, l51, l52 = sch.split(loop=l9, factors=[v45, v46, v47, v48])
v53, v54 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[64, 11])
l55, l56 = sch.split(loop=l10, factors=[v53, v54])
v57, v58 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l59, l60 = sch.split(loop=l11, factors=[v57, v58])
v61, v62 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 1])
l63, l64 = sch.split(loop=l12, factors=[v61, v62])
sch.reorder(l17, l25, l33, l41, l49, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52)
b65, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b65, loop=l50, preserve_unit_loops=True)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
[16:04:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 176, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 1, 64, 1, 1, 1, 1, 7, 1, 2, 11, 1, 1, 1, 1, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(704, i5_0 * 11 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 176, 7, 7, 4], "float32"], ["TENSOR", [32, 176, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max((conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l5, l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b0)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l5, factors=[v13, v14, v15, v16])
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 4, 1, 1])
l25, l26, l27, l28 = sch.split(loop=l6, factors=[v21, v22, v23, v24])
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l33, l34, l35, l36 = sch.split(loop=l7, factors=[v29, v30, v31, v32])
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l41, l42, l43, l44 = sch.split(loop=l8, factors=[v37, v38, v39, v40])
v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l49, l50, l51, l52 = sch.split(loop=l9, factors=[v45, v46, v47, v48])
v53, v54 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[64, 11])
l55, l56 = sch.split(loop=l10, factors=[v53, v54])
v57, v58 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l59, l60 = sch.split(loop=l11, factors=[v57, v58])
v61, v62 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 1])
l63, l64 = sch.split(loop=l12, factors=[v61, v62])
sch.reorder(l17, l25, l33, l41, l49, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52)
b65, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b65, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
[16:04:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_multiply_add_multiply_add_nn_relu_10"
[16:04:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 168, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 168, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 168, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 168, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 168, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 168, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 168, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 168, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 168, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 168, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 168, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 168, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 168, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9"
[16:04:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 672, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 7, 7, 4], "float32"], ["TENSOR", [32, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 2, 1, 7, 2, 224, 1, 1, 1, 2, 7, 1, 1, 3, 1, 1, 1, 8, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 8 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_2, i3_1])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(672, i5_0 * 3 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 7, 7, 4], "float32"], ["TENSOR", [32, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[224, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 2, 1, 7, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(224, 1, 1, 1, 2, 7, 1, 1, 3, 1, 1, 1, 8, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_1])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(672, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 7, 7, 4], "float32"], ["TENSOR", [32, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 1, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[224, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 7, 2, 224, 1, 1, 1, 2, 7, 1, 1, 3, 1, 1, 1, 8, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_1])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(672, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 7, 7, 4], "float32"], ["TENSOR", [32, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[224, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_multiply_add_multiply_add_nn_relu_11"
[16:04:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 160, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 160, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 160, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 160, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 160, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 160, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 160, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 160, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 160, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10"
[16:04:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 160, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 640, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 160, 7, 7, 4], "float32"], ["TENSOR", [32, 160, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 160, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 4, 7, 7, 1, 20, 1, 1, 1, 1, 1, 1, 2, 32, 1, 1, 1, 8, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(640, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 160, 7, 7, 4], "float32"], ["TENSOR", [32, 160, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[20, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 160, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 4, 7, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(20, 1, 1, 1, 1, 1, 1, 2, 32, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(640, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 160, 7, 7, 4], "float32"], ["TENSOR", [32, 160, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 1, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[20, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 160, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 7, 7, 1, 20, 1, 1, 1, 1, 1, 1, 2, 32, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(640, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 160, 7, 7, 4], "float32"], ["TENSOR", [32, 160, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[20, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_multiply_add_multiply_add_nn_relu_12"
[16:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 152, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 152, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 152, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 152, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 152, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 152, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 152, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 152, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 152, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11"
[16:04:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 152, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 608, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 152, 7, 7, 4], "float32"], ["TENSOR", [32, 152, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 152, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 7, 1, 1, 1, 1, 1, 7, 1, 152, 1, 1, 1, 4, 1, 1, 4, 4, 1, 1, 1, 2, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_0, i3_1, i4_2])
                    ic = T.axis.reduce(608, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 152, 7, 7, 4], "float32"], ["TENSOR", [32, 152, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[152, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 152, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 7, 1, 1, 1, 1, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(152, 1, 1, 1, 4, 1, 1, 4, 4, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_0, i3_1, i4_2])
                        ic = T.axis.reduce(608, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 152, 7, 7, 4], "float32"], ["TENSOR", [32, 152, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[152, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 152, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 7, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 152, 1, 1, 1, 4, 1, 1, 4, 4, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_0, i3_1, i4_2])
                        ic = T.axis.reduce(608, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 152, 7, 7, 4], "float32"], ["TENSOR", [32, 152, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 7, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[152, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_multiply_add_multiply_add_nn_relu_13"
[16:04:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 144, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 144, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 144, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 144, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #27: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12"
[16:04:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 576, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [32, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 1, 4, 7, 1, 2, 192, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 4, 1, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(576, i5_0 * 3 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [32, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[192, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 4, 7, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(192, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 4, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(576, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [32, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(7, ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[192, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 7, 1, 2, 192, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 4, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(576, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [32, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[192, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #28: "fused_multiply_add_nn_relu"
[16:04:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 136, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 136, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 136, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 136, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 136, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:04:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #29: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13"
[16:04:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 136, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 544, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 136, 7, 7, 4], "float32"], ["TENSOR", [32, 136, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 136, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 1, 2, 7, 1, 1, 16, 1, 1, 1, 8, 1, 1, 2, 34, 1, 1, 1, 2, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 2 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_1, i3_0])
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(544, i5_0 * 34 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 136, 7, 7, 4], "float32"], ["TENSOR", [32, 136, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 34])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 136, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 7, 1, 1, 2, 7, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 8, 1, 1, 2, 34, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_0])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(544, i5_0 * 34 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 136, 7, 7, 4], "float32"], ["TENSOR", [32, 136, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 1, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(7, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 34])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 136, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 1, 1, 16, 1, 1, 1, 8, 1, 1, 2, 34, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_0])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(544, i5_0 * 34 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 136, 7, 7, 4], "float32"], ["TENSOR", [32, 136, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 1, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(7, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 34])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #30: "fused_multiply_add_nn_relu_1"
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 4, 1, 7, 2, 256, 1, 1, 1, 2, 7, 1, 1, 2, 1, 1, 1, 4, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_2 * 4 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_2, i3_1])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 4, 1, 7, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 2, 7, 1, 1, 2, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_1])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 7, 2, 256, 1, 1, 1, 2, 7, 1, 1, 2, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_1])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #32: "fused_nn_contrib_conv2d_NCHWc"
[16:04:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 9, 9, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 9, 9, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 7, 7, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 7, 7, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

[16:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 9, 9, 4], dtype="float32")
            conv2d_NCHWc_global = T.alloc_buffer([1, 8, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 1, 2, 1, 2, 1, 7, 1):
                for i5_0, i6_0 in T.grid(16, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 3, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i5_0 * 2 + ax1)
                            i2 = T.axis.spatial(9, i2_0 + i6_0 + ax2)
                            i3 = T.axis.spatial(9, i3_1 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 1, 1, 1, 8, 1, 3, 1, 1, 1, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_1 * 4 + i1_2)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 7, 7, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 1, 2):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(8, i1_1 * 4 + ax1)
                        v2 = T.axis.spatial(7, i2_0 + ax2)
                        v3 = T.axis.spatial(7, i3_1 + ax3)
                        v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[16:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 9, 9, 4], dtype="float32")
            conv2d_NCHWc_global = T.alloc_buffer([1, 8, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 7, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 2, 1, 7, 1, 16, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 3, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i5_0 * 2 + ax1)
                            i2 = T.axis.spatial(9, i2_0 + i6_0 + ax2)
                            i3 = T.axis.spatial(9, i3_1 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 1, 1, 1, 8, 1, 3, 1, 1, 1, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_1 * 4 + i1_2)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 7, 7, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 7, 2):
                    with T.block("conv2d_NCHWc_global"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(7, i2_0 + ax2)
                        v3 = T.axis.spatial(7, ax3)
                        v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[16:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 2, 1, 2, 1, 7, 1, 16, 3, 1, 1, 4, 1, 1, 1, 8, 1, 3, 1, 1, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_1 * 4 + i1_2)
                    oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                    T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 7, 7, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 8 and 1 <= ow + kw and ow + kw < 8, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[16:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #33: "fused_multiply_add_multiply_add_nn_relu_14"
[16:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 248, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 248, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 248, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 248, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 248, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 248, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 248, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 248, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 248, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #34: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15"
[16:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 248, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 992, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 14, 14, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 248, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 2, 1, 7, 1, 124, 1, 1, 1, 4, 7, 1, 1, 8, 1, 1, 1, 4, 2, 1, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(992, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 14, 14, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[124, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 248, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 1, 2, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(124, 1, 1, 1, 4, 7, 1, 1, 8, 1, 1, 1, 4, 2, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(992, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 14, 14, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 14, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[124, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 248, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 7, 1, 124, 1, 1, 1, 4, 7, 1, 1, 8, 1, 1, 1, 4, 2, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(992, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 14, 14, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 14, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[124, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #35: "fused_multiply_add_multiply_add_nn_relu_15"
[16:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #36: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16"
[16:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 960, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:04:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:04:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 2, 1, 1, 1, 1, 7, 7, 2, 60, 1, 1, 1, 4, 1, 1, 1, 16, 1, 1, 1, 1, 1, 2, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2)
                    oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                    ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(960, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[60, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:04:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 2, 1, 1, 1, 1, 7, 7, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(60, 1, 1, 1, 4, 1, 1, 1, 16, 1, 1, 1, 1, 1, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(960, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 2, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + i2_1 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[60, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:04:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 2, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 7, 2, 60, 1, 1, 1, 4, 1, 1, 1, 16, 1, 1, 1, 1, 1, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(960, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 14, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[60, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #37: "fused_multiply_add_multiply_add_nn_relu_16"
[16:05:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 232, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 232, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 232, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 232, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 232, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 232, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 232, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 232, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 232, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17"
[16:05:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 232, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 928, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 14, 14, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 232, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 2, 1, 1, 7, 1, 1, 58, 1, 1, 1, 1, 1, 1, 2, 16, 1, 1, 1, 16, 2, 14, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_3)
                    oh = T.axis.spatial(14, i2_1 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(928, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 14, 14, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[58, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 232, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 2, 1, 1, 7, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(58, 1, 1, 1, 1, 1, 1, 2, 16, 1, 1, 1, 16, 2, 14, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(928, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 14, 14, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 2, 14, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[58, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 232, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 1, 58, 1, 1, 1, 1, 1, 1, 2, 16, 1, 1, 1, 16, 2, 14, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(928, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 14, 14, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 14, 14, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[58, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #39: "fused_multiply_add_nn_relu_2"
[16:05:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 224, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 224, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 224, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 224, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 224, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 224, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 224, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #40: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18"
[16:05:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 224, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 896, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 14, 14, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 224, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 2, 7, 1, 14, 1, 1, 1, 32, 1, 1, 2, 64, 1, 1, 1, 1, 7, 2, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2)
                    oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                    ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(896, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 14, 14, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[14, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 224, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 1, 2, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(14, 1, 1, 1, 32, 1, 1, 2, 64, 1, 1, 1, 1, 7, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(896, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 14, 14, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 2, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(14, i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[14, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 224, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 7, 1, 14, 1, 1, 1, 32, 1, 1, 2, 64, 1, 1, 1, 1, 7, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(896, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 14, 14, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 14, 14, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[14, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #41: "fused_multiply_add_multiply_add_nn_relu_17"
[16:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 216, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 216, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 216, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 216, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 216, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 216, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 216, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 216, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 216, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #42: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19"
[16:05:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 216, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 864, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 14, 14, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 216, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 7, 4, 1, 2, 2, 1, 1, 288, 1, 1, 1, 2, 7, 1, 1, 3, 1, 1, 1, 4, 1, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                    ow = T.axis.spatial(14, i3_0 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(864, i5_0 * 3 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 14, 14, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[288, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 216, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 7, 4, 1, 2, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(288, 1, 1, 1, 2, 7, 1, 1, 3, 1, 1, 1, 4, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(864, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 14, 14, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 2, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[288, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 216, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 7, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 1, 1, 288, 1, 1, 1, 2, 7, 1, 1, 3, 1, 1, 1, 4, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(864, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 14, 14, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 14, 2, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[288, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #43: "fused_multiply_add_nn_relu_3"
[16:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 208, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 208, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 208, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 208, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 208, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 208, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 208, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #44: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20"
[16:05:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 208, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 832, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 14, 14, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 208, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 2, 1, 2, 14, 1, 2, 52, 1, 1, 1, 4, 1, 14, 1, 16, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 4 + i1_2)
                    oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(832, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 14, 14, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[52, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 208, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 2, 1, 2, 14, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(52, 1, 1, 1, 4, 1, 14, 1, 16, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 4 + i1_2)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(832, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 14, 14, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 14, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[52, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 208, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 14, 1, 2, 52, 1, 1, 1, 4, 1, 14, 1, 16, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 4 + i1_2)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(832, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 14, 14, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 14, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[52, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #45: "fused_multiply_add_multiply_add_nn_relu_18"
[16:05:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 200, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 200, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 200, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 200, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 200, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 200, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 200, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 200, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 200, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #46: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21"
[16:05:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 200, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 800, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 14, 14, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 200, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 1, 1, 2, 7, 14, 4, 200, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                    ow, oc_block = T.axis.remap("SS", [i3_1, i4_1])
                    ic = T.axis.reduce(800, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 14, 14, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[200, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 200, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 1, 1, 1, 2, 7, 14, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(200, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                        ow, oc_block = T.axis.remap("SS", [i3_1, i4_1])
                        ic = T.axis.reduce(800, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 14, 14, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 1, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + i2_1 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[200, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 200, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 2, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 14, 4, 200, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                        ow, oc_block = T.axis.remap("SS", [i3_1, i4_1])
                        ic = T.axis.reduce(800, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 14, 14, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 14, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[200, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #47: "fused_multiply_add_nn_relu_4"
[16:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 192, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 192, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 192, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 192, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #48: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22"
[16:05:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 768, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 192, 14, 14, 4], "float32"], ["TENSOR", [32, 192, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 1, 2, 1, 1, 2, 1, 7, 1, 32, 1, 1, 1, 1, 14, 1, 2, 24, 1, 1, 1, 1, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_1)
                    oh = T.axis.spatial(14, i2_2)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(768, i5_0 * 24 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 192, 14, 14, 4], "float32"], ["TENSOR", [32, 192, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 24])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 1, 2, 1, 1, 2, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 1, 14, 1, 2, 24, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(14, i2_2)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(768, i5_0 * 24 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 192, 14, 14, 4], "float32"], ["TENSOR", [32, 192, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 14, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 2 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 24])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 16, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 7, 1, 32, 1, 1, 1, 1, 14, 1, 2, 24, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(14, i2_2)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(768, i5_0 * 24 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 192, 14, 14, 4], "float32"], ["TENSOR", [32, 192, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 7, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 24])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #49: "fused_multiply_add_multiply_add_nn_relu_19"
[16:05:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 184, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 184, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 184, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 184, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 184, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 184, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 184, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 184, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 184, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 184, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #50: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23"
[16:05:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 184, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 736, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 184, 14, 14, 4], "float32"], ["TENSOR", [32, 184, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 184, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 1, 1, 1, 1, 7, 1, 92, 1, 1, 1, 4, 7, 2, 2, 8, 1, 1, 1, 1, 2, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2)
                    oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(736, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 184, 14, 14, 4], "float32"], ["TENSOR", [32, 184, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[92, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 184, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 1, 1, 1, 1, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(92, 1, 1, 1, 4, 7, 2, 2, 8, 1, 1, 1, 1, 2, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(736, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 184, 14, 14, 4], "float32"], ["TENSOR", [32, 184, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 2, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[92, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 184, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 92, 1, 1, 1, 4, 7, 2, 2, 8, 1, 1, 1, 1, 2, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(736, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 184, 14, 14, 4], "float32"], ["TENSOR", [32, 184, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 14, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[92, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #51: "fused_multiply_add_multiply_add_nn_relu_20"
[16:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 176, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 176, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 176, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 176, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 176, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 176, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 176, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 176, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 176, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 176, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #52: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24"
[16:05:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 176, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 704, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 176, 14, 14, 4], "float32"], ["TENSOR", [32, 176, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 176, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 88, 1, 1, 1, 1, 7, 7, 2, 8, 1, 1, 1, 16, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_3)
                    oh = T.axis.spatial(14, i2_0 * 7 + i2_2)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(704, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 176, 14, 14, 4], "float32"], ["TENSOR", [32, 176, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[88, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 176, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 2, 2, 1, 2, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(88, 1, 1, 1, 1, 7, 7, 2, 8, 1, 1, 1, 16, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(704, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 176, 14, 14, 4], "float32"], ["TENSOR", [32, 176, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[88, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 176, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 2, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 88, 1, 1, 1, 1, 7, 7, 2, 8, 1, 1, 1, 16, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(704, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 176, 14, 14, 4], "float32"], ["TENSOR", [32, 176, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[88, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #53: "fused_multiply_add_nn_relu_5"
[16:05:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 168, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 168, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #54: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25"
[16:05:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 672, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [32, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 14, 1, 2, 1, 8, 1, 1, 1, 84, 1, 1, 1, 2, 1, 2, 1, 8, 1, 1, 1, 1, 1, 7, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(14, i2_0)
                    ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(672, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [32, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[84, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 14, 1, 2, 1, 8, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(84, 1, 1, 1, 2, 1, 2, 1, 8, 1, 1, 1, 1, 1, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(14, i2_0)
                        ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(672, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [32, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 14, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[84, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 14, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 1, 84, 1, 1, 1, 2, 1, 2, 1, 8, 1, 1, 1, 1, 1, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(14, i2_0)
                        ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(672, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [32, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 1, 14, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[84, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #55: "fused_multiply_add_multiply_add_nn_relu_21"
[16:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 160, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 160, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 160, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 160, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 160, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 160, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 160, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 160, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 160, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 160, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #56: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26"
[16:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 160, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 640, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 160, 14, 14, 4], "float32"], ["TENSOR", [32, 160, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 160, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 2, 2, 1, 32, 1, 1, 1, 40, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 2, 7, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1)
                    oh = T.axis.spatial(14, i2_0 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(640, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 160, 14, 14, 4], "float32"], ["TENSOR", [32, 160, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 32, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[40, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 160, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 2, 2, 1, 32, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(40, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 2, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(640, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 160, 14, 14, 4], "float32"], ["TENSOR", [32, 160, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 2, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 32, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[40, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 160, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 7, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 32, 1, 1, 1, 40, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 2, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(640, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 160, 14, 14, 4], "float32"], ["TENSOR", [32, 160, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 2, 7, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(14, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 32, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[40, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #57: "fused_multiply_add_multiply_add_nn_relu_22"
[16:05:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 152, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 152, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 152, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 152, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 152, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 152, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 152, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 152, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 152, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 152, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #58: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27"
[16:05:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 152, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 608, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 152, 14, 14, 4], "float32"], ["TENSOR", [32, 152, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 152, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 304, 1, 1, 1, 8, 1, 7, 4, 2, 1, 1, 1, 1, 7, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2)
                    oh = T.axis.spatial(14, i2_0 * 7 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(608, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 152, 14, 14, 4], "float32"], ["TENSOR", [32, 152, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[304, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 152, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 2, 1, 1, 2, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(304, 1, 1, 1, 8, 1, 7, 4, 2, 1, 1, 1, 1, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(608, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 152, 14, 14, 4], "float32"], ["TENSOR", [32, 152, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[304, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 152, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 304, 1, 1, 1, 8, 1, 7, 4, 2, 1, 1, 1, 1, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(608, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 152, 14, 14, 4], "float32"], ["TENSOR", [32, 152, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[304, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #59: "fused_multiply_add_nn_relu_6"
[16:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 144, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 144, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #60: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28"
[16:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 576, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [32, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 1, 7, 1, 1, 1, 2, 2, 1, 16, 1, 1, 1, 1, 7, 1, 4, 36, 1, 1, 1, 2, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_3)
                    oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                    ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(576, i5_0 * 36 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [32, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 36])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 1, 7, 1, 1, 1, 2, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 7, 1, 4, 36, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(576, i5_0 * 36 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [32, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 36])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 16, 1, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 1, 16, 1, 1, 1, 1, 7, 1, 4, 36, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(576, i5_0 * 36 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [32, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 2, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 36])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #61: "fused_multiply_add_multiply_add_nn_relu_23"
[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 136, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 136, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 136, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 136, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 136, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 136, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 136, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 136, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 136, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 136, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 136, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #62: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29"
[16:05:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 136, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 544, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 136, 14, 14, 4], "float32"], ["TENSOR", [32, 136, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 136, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 4, 1, 1, 2, 2, 1, 32, 1, 1, 1, 4, 1, 1, 1, 17, 1, 1, 1, 2, 7, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                    ow = T.axis.spatial(14, i3_1 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(544, i5_0 * 17 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 136, 14, 14, 4], "float32"], ["TENSOR", [32, 136, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 17])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 136, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 4, 1, 1, 2, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 4, 1, 1, 1, 17, 1, 1, 1, 2, 7, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(544, i5_0 * 17 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 136, 14, 14, 4], "float32"], ["TENSOR", [32, 136, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 7, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 17])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 136, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 1, 32, 1, 1, 1, 4, 1, 1, 1, 17, 1, 1, 1, 2, 7, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(544, i5_0 * 17 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 136, 14, 14, 4], "float32"], ["TENSOR", [32, 136, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 14, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 17])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #63: "fused_multiply_add_nn_relu_7"
[16:05:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #64: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30"
[16:05:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 4, 1, 2, 7, 7, 1, 8, 1, 1, 1, 1, 1, 1, 1, 64, 1, 1, 1, 16, 1, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_3)
                    oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                    ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(512, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 1, 4, 1, 2, 7, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 1, 1, 1, 64, 1, 1, 1, 16, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(512, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 1, 2, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + i2_1 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 2, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 7, 1, 8, 1, 1, 1, 1, 1, 1, 1, 64, 1, 1, 1, 16, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(512, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [32, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 14, 1):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #65: "fused_multiply_add_nn_relu_8"
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 120, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 120, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 120, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 120, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 120, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 120, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 120, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 120, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 120, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 120, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 120, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 120, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #66: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31"
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 120, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 480, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 120, 14, 14, 4], "float32"], ["TENSOR", [32, 120, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 120, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 60, 1, 1, 1, 4, 1, 1, 1, 8, 1, 1, 1, 2, 14, 14, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_3, i4_3])
                    ic = T.axis.reduce(480, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 120, 14, 14, 4], "float32"], ["TENSOR", [32, 120, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[60, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 120, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 2, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(60, 1, 1, 1, 4, 1, 1, 1, 8, 1, 1, 1, 2, 14, 14, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_3, i4_3])
                        ic = T.axis.reduce(480, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 120, 14, 14, 4], "float32"], ["TENSOR", [32, 120, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 14, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[60, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 120, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 60, 1, 1, 1, 4, 1, 1, 1, 8, 1, 1, 1, 2, 14, 14, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_3, i4_3])
                        ic = T.axis.reduce(480, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 120, 14, 14, 4], "float32"], ["TENSOR", [32, 120, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 14, 14, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[60, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #67: "fused_multiply_add_multiply_add_nn_relu_24"
[16:05:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 112, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 112, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 112, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 112, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 112, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 112, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 112, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 112, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 112, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #68: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32"
[16:05:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 448, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 14, 14, 4], "float32"], ["TENSOR", [32, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 2, 1, 1, 2, 1, 1, 56, 1, 1, 1, 2, 7, 1, 1, 8, 1, 1, 1, 2, 1, 14, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                    ow = T.axis.spatial(14, i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(448, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 14, 14, 4], "float32"], ["TENSOR", [32, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[56, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 1, 2, 1, 1, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(56, 1, 1, 1, 2, 7, 1, 1, 8, 1, 1, 1, 2, 1, 14, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(448, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 14, 14, 4], "float32"], ["TENSOR", [32, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 14, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[56, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 1, 56, 1, 1, 1, 2, 7, 1, 1, 8, 1, 1, 1, 2, 1, 14, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(448, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 14, 14, 4], "float32"], ["TENSOR", [32, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 14, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[56, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #69: "fused_multiply_add_nn_relu_9"
[16:05:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 104, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 104, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 104, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 104, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 104, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 104, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 104, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 104, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 104, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 104, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 104, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 104, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #70: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33"
[16:05:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 104, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 416, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 104, 14, 14, 4], "float32"], ["TENSOR", [32, 104, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 104, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 1, 1, 4, 1, 7, 1, 8, 1, 1, 1, 1, 2, 1, 4, 52, 1, 1, 1, 1, 7, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1)
                    oh = T.axis.spatial(14, i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(416, i5_0 * 52 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 104, 14, 14, 4], "float32"], ["TENSOR", [32, 104, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 52])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 104, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 1, 1, 1, 4, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 2, 1, 4, 52, 1, 1, 1, 1, 7, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1)
                        oh = T.axis.spatial(14, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(416, i5_0 * 52 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 104, 14, 14, 4], "float32"], ["TENSOR", [32, 104, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 14, 2, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 52])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 104, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 7, 1, 8, 1, 1, 1, 1, 2, 1, 4, 52, 1, 1, 1, 1, 7, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1)
                        oh = T.axis.spatial(14, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(416, i5_0 * 52 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 104, 14, 14, 4], "float32"], ["TENSOR", [32, 104, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 14, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 52])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #71: "fused_multiply_add_multiply_add_nn_relu_25"
[16:05:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 96, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 96, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 96, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 96, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34"
[16:05:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 384, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [32, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 2, 7, 14, 2, 96, 1, 1, 1, 16, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2)
                    oh = T.axis.spatial(14, i2_1 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_1)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(384, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [32, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 2, 7, 14, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(96, 1, 1, 1, 16, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(384, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [32, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 2, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 14, 2, 96, 1, 1, 1, 16, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(384, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [32, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 14, 14, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #73: "fused_multiply_add_nn_relu_10"
[16:05:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 88, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 88, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 88, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 88, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 88, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 88, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 88, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 88, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 88, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 88, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 88, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 88, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #74: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35"
[16:05:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 88, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 352, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 88, 14, 14, 4], "float32"], ["TENSOR", [32, 88, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 88, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 7, 4, 1, 1, 1, 2, 1, 11, 1, 1, 1, 4, 7, 1, 1, 32, 1, 1, 1, 2, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(352, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 88, 14, 14, 4], "float32"], ["TENSOR", [32, 88, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[11, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 88, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 7, 4, 1, 1, 1, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(11, 1, 1, 1, 4, 7, 1, 1, 32, 1, 1, 1, 2, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(352, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 88, 14, 14, 4], "float32"], ["TENSOR", [32, 88, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[11, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 88, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 7, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 11, 1, 1, 1, 4, 7, 1, 1, 32, 1, 1, 1, 2, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(352, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 88, 14, 14, 4], "float32"], ["TENSOR", [32, 88, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 2, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[11, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #75: "fused_multiply_add_nn_relu_11"
[16:05:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 80, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 80, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 80, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 80, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 80, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 80, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 80, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 80, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 80, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 80, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #76: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36"
[16:05:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 320, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 14, 14, 4], "float32"], ["TENSOR", [32, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 1, 4, 1, 8, 1, 14, 1, 160, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(14, i2_0 * 2 + i2_2)
                    ow, oc_block = T.axis.remap("SS", [i3_1, i4_0])
                    ic = T.axis.reduce(320, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 14, 14, 4], "float32"], ["TENSOR", [32, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[160, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 7, 1, 4, 1, 8, 1, 14, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(160, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_2)
                        ow, oc_block = T.axis.remap("SS", [i3_1, i4_0])
                        ic = T.axis.reduce(320, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 14, 14, 4], "float32"], ["TENSOR", [32, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[160, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 7, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 14, 1, 160, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_2)
                        ow, oc_block = T.axis.remap("SS", [i3_1, i4_0])
                        ic = T.axis.reduce(320, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 14, 14, 4], "float32"], ["TENSOR", [32, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 2, 14, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[160, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #77: "fused_multiply_add_nn_relu_12"
[16:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 72, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 72, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 72, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 72, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 72, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 72, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 72, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 72, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 72, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 72, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 72, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 72, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #78: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37"
[16:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 72, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 288, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 72, 14, 14, 4], "float32"], ["TENSOR", [32, 72, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 72, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 14, 1, 4, 1, 1, 1, 7, 1, 12, 1, 1, 1, 4, 1, 2, 1, 24, 1, 1, 1, 8, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                    oh = T.axis.spatial(14, i2_0)
                    ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(288, i5_0 * 24 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 72, 14, 14, 4], "float32"], ["TENSOR", [32, 72, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[12, 24])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 72, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 14, 1, 4, 1, 1, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 4, 1, 2, 1, 24, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(14, i2_0)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(288, i5_0 * 24 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 72, 14, 14, 4], "float32"], ["TENSOR", [32, 72, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 2, 1):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(14, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[12, 24])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 72, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 14, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 12, 1, 1, 1, 4, 1, 2, 1, 24, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(14, i2_0)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(288, i5_0 * 24 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 72, 14, 14, 4], "float32"], ["TENSOR", [32, 72, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 14, 1):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(14, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[12, 24])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #79: "fused_multiply_add_nn_relu_13"
[16:05:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #80: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38"
[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 14, 14, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 2, 2, 1, 1, 1, 1, 2, 8, 1, 1, 1, 2, 7, 7, 1, 32, 1, 1, 1, 4, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 2, 2, 1, 1, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 2, 7, 7, 1, 32, 1, 1, 1, 4, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 7, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 8, 1, 1, 1, 2, 7, 7, 1, 32, 1, 1, 1, 4, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #81: "fused_nn_contrib_conv2d_NCHWc_1"
[16:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 16, 16, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 16, 16, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 14, 14, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 14, 14, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

[16:05:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 16, 16, 4], dtype="float32")
            conv2d_NCHWc_global = T.alloc_buffer([1, 8, 14, 14, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 16, 16, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 1, 1, 1, 2, 1, 2, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 3, 1, 2, 7, 2, 1, 4, 3, 1, 1, 2, 2, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1_1)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 14, 14, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 14, 1):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(8, i1_1_1 * 4 + ax1)
                        v2, v3 = T.axis.remap("SS", [ax2, ax3])
                        v4 = T.axis.spatial(4, i4_0 * 2 + i4_1_1 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[16:05:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 16, 16, 4], dtype="float32")
            conv2d_NCHWc_global = T.alloc_buffer([1, 8, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1 in T.grid(1, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 16, 16, 4):
                        with T.block("data_pad"):
                            i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 32, 1, 3, 1, 2, 7, 2, 1, 4, 3, 1, 1, 2, 2, 7, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 14, 14, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 14, 2):
                    with T.block("conv2d_NCHWc_global"):
                        v0, v1, v2, v3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[16:05:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 16, 16, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0 in T.grid(1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 32, 1, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 16, 14, 4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i5_0 + ax1)
                        i2 = T.axis.spatial(16, ax2)
                        i3 = T.axis.spatial(16, i7_0 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 2, 1, 4, 3, 1, 1, 2, 2, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 14, 14, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[16:05:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #82: "fused_multiply_add_multiply_add_nn_relu_26"
[16:05:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 120, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 120, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 120, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 120, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 120, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 120, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 120, 28, 28, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 120, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 120, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 120, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 120, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 120, 28, 28, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 120, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 120, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 120, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 120, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 120, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 120, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 120, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 120, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #83: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39"
[16:05:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 120, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 480, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 120, 28, 28, 4], "float32"], ["TENSOR", [32, 120, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 120, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 7, 1, 1, 1, 1, 2, 1, 1, 160, 1, 1, 1, 8, 1, 7, 4, 3, 1, 1, 1, 1, 2, 4, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2)
                    oh = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + i2_3)
                    ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(480, i5_0 * 3 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 120, 28, 28, 4], "float32"], ["TENSOR", [32, 120, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[160, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 120, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 7, 1, 1, 1, 1, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(160, 1, 1, 1, 8, 1, 7, 4, 3, 1, 1, 1, 1, 2, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2)
                        oh = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(480, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 120, 28, 28, 4], "float32"], ["TENSOR", [32, 120, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 28, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[160, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 120, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 7, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 1, 160, 1, 1, 1, 8, 1, 7, 4, 3, 1, 1, 1, 1, 2, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2)
                        oh = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(480, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 120, 28, 28, 4], "float32"], ["TENSOR", [32, 120, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 28, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 4 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[160, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #84: "fused_multiply_add_multiply_add_nn_relu_27"
[16:05:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 112, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 112, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 112, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 112, 28, 28, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 112, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 112, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 28, 28, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 112, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 112, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 112, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #85: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40"
[16:05:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 448, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 28, 28, 4], "float32"], ["TENSOR", [32, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 2, 1, 1, 1, 4, 1, 28, 1, 32, 1, 1, 1, 2, 2, 1, 4, 14, 1, 1, 1, 1, 7, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(28, i2_0 * 14 + i2_2 * 7 + i2_3)
                    ow, oc_block = T.axis.remap("SS", [i3_1, i4_2])
                    ic = T.axis.reduce(448, i5_0 * 14 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 28, 28, 4], "float32"], ["TENSOR", [32, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 14])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 1, 1, 1, 4, 1, 28, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 2, 2, 1, 4, 14, 1, 1, 1, 1, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_2 * 7 + i2_3)
                        ow, oc_block = T.axis.remap("SS", [i3_1, i4_2])
                        ic = T.axis.reduce(448, i5_0 * 14 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 28, 28, 4], "float32"], ["TENSOR", [32, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 14])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 2, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 28, 1, 32, 1, 1, 1, 2, 2, 1, 4, 14, 1, 1, 1, 1, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_2 * 7 + i2_3)
                        ow, oc_block = T.axis.remap("SS", [i3_1, i4_2])
                        ic = T.axis.reduce(448, i5_0 * 14 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 28, 28, 4], "float32"], ["TENSOR", [32, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 28, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 14])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #86: "fused_multiply_add_nn_relu_14"
[16:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 104, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 104, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 104, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 104, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 104, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 104, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 104, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 104, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 104, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 104, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 104, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 104, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #87: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41"
[16:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 104, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 416, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 104, 28, 28, 4], "float32"], ["TENSOR", [32, 104, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 104, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 4, 1, 2, 1, 1, 1, 4, 2, 208, 1, 1, 1, 4, 7, 1, 1, 2, 1, 1, 1, 2, 1, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_0 * 7 + i2_2)
                    ow = T.axis.spatial(28, i3_1 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(416, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 104, 28, 28, 4], "float32"], ["TENSOR", [32, 104, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[208, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 104, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 4, 1, 2, 1, 1, 1, 4, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(208, 1, 1, 1, 4, 7, 1, 1, 2, 1, 1, 1, 2, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(416, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 104, 28, 28, 4], "float32"], ["TENSOR", [32, 104, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 7, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_1 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[208, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 104, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 4, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 4, 2, 208, 1, 1, 1, 4, 7, 1, 1, 2, 1, 1, 1, 2, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(416, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 104, 28, 28, 4], "float32"], ["TENSOR", [32, 104, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 28, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(28, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[208, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #88: "fused_multiply_add_nn_relu_15"
[16:05:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 96, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 96, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #89: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42"
[16:05:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 384, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [32, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 7, 1, 2, 1, 2, 2, 7, 2, 6, 1, 1, 1, 2, 1, 2, 1, 64, 1, 1, 1, 2, 2, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + i2_3)
                    ow = T.axis.spatial(28, i3_1 * 4 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(384, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [32, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 7, 1, 2, 1, 2, 2, 7, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(6, 1, 1, 1, 2, 1, 2, 1, 64, 1, 1, 1, 2, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(384, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [32, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 4, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 7, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 7, 2, 6, 1, 1, 1, 2, 1, 2, 1, 64, 1, 1, 1, 2, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(384, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [32, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 28, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 4 + ax2)
                        ax3_1 = T.axis.spatial(28, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #90: "fused_multiply_add_nn_relu_16"
[16:05:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 88, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 88, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 88, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 88, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 88, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 88, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 88, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 88, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 88, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 88, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 88, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 88, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #91: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43"
[16:05:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 88, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 352, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 88, 28, 28, 4], "float32"], ["TENSOR", [32, 88, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 88, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 8, 2, 2, 1, 8, 1, 1, 1, 2, 14, 7, 1, 44, 1, 1, 1, 2, 1, 1, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_1 * 14 + i2_2)
                    ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(352, i5_0 * 44 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 88, 28, 28, 4], "float32"], ["TENSOR", [32, 88, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 44])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 88, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 1, 8, 2, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 2, 14, 7, 1, 44, 1, 1, 1, 2, 1, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(352, i5_0 * 44 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 88, 28, 28, 4], "float32"], ["TENSOR", [32, 88, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 7, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_1 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 14 + i3_1 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 44])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 88, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 2, 2, 1, 8, 1, 1, 1, 2, 14, 7, 1, 44, 1, 1, 1, 2, 1, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(352, i5_0 * 44 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 88, 28, 28, 4], "float32"], ["TENSOR", [32, 88, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 28, 14, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(28, i3_0 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 44])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #92: "fused_multiply_add_nn_relu_17"
[16:05:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 80, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 80, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 80, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 80, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 80, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 80, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 80, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 80, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 80, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 80, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #93: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44"
[16:05:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 320, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 28, 28, 4], "float32"], ["TENSOR", [32, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 2, 1, 1, 1, 7, 1, 1, 40, 1, 1, 1, 8, 2, 7, 1, 8, 1, 1, 1, 2, 1, 2, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_0 * 14 + i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(28, i3_0 * 14 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(320, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 28, 28, 4], "float32"], ["TENSOR", [32, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[40, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 2, 1, 1, 1, 7, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(40, 1, 1, 1, 8, 2, 7, 1, 8, 1, 1, 1, 2, 1, 2, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(320, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 28, 28, 4], "float32"], ["TENSOR", [32, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 2, 14, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[40, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 1, 40, 1, 1, 1, 8, 2, 7, 1, 8, 1, 1, 1, 2, 1, 2, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(320, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 28, 28, 4], "float32"], ["TENSOR", [32, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 14, 14, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[40, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #94: "fused_multiply_add_multiply_add_nn_relu_28"
[16:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 72, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 72, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 72, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 72, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 72, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 72, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 72, 28, 28, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 72, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 72, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 72, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 72, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 72, 28, 28, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 72, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 72, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 72, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 72, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 72, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 72, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 72, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 72, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #95: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45"
[16:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 72, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 288, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 72, 28, 28, 4], "float32"], ["TENSOR", [32, 72, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 72, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 6, 1, 1, 1, 16, 2, 14, 4, 48, 1, 1, 1, 1, 7, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2)
                    oh = T.axis.spatial(28, i2_0 * 14 + i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(28, i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(288, i5_0 * 48 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 72, 28, 28, 4], "float32"], ["TENSOR", [32, 72, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 72, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 1, 1, 1, 2, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(6, 1, 1, 1, 16, 2, 14, 4, 48, 1, 1, 1, 1, 7, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(288, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 72, 28, 28, 4], "float32"], ["TENSOR", [32, 72, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 14, 28, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 72, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 2, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 6, 1, 1, 1, 16, 2, 14, 4, 48, 1, 1, 1, 1, 7, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(288, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 72, 28, 28, 4], "float32"], ["TENSOR", [32, 72, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 14, 28, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #96: "fused_multiply_add_multiply_add_nn_relu_29"
[16:05:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #97: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46"
[16:05:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 2, 7, 1, 1, 4, 1, 1, 4, 64, 1, 1, 1, 2, 7, 2, 1, 4, 1, 1, 1, 1, 2, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(28, i2_0 * 14 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(28, i3_0 * 4 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1)
                    ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 7, 1, 1, 4, 1, 1, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 2, 7, 2, 1, 4, 1, 1, 1, 1, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 4, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 2, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 4, 64, 1, 1, 1, 2, 7, 2, 1, 4, 1, 1, 1, 1, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(256, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 4, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #98: "fused_multiply_add_nn_relu_18"
[16:05:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 56, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 56, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 56, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 56, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 56, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 56, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 56, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 56, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 56, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 56, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 56, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 56, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #99: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47"
[16:05:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 56, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 224, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 56, 28, 28, 4], "float32"], ["TENSOR", [32, 56, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 56, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 14, 2, 1, 2, 2, 2, 1, 7, 1, 1, 1, 8, 2, 1, 2, 32, 1, 1, 1, 2, 7, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(28, i3_0 * 2 + i3_1)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(224, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 56, 28, 28, 4], "float32"], ["TENSOR", [32, 56, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[7, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 56, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 14, 2, 1, 2, 2, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(7, 1, 1, 1, 8, 2, 1, 2, 32, 1, 1, 1, 2, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(224, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 56, 28, 28, 4], "float32"], ["TENSOR", [32, 56, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 14, 1, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_1 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 2 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[7, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 56, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 14, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 2, 1, 7, 1, 1, 1, 8, 2, 1, 2, 32, 1, 1, 1, 2, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(224, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 56, 28, 28, 4], "float32"], ["TENSOR", [32, 56, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 28, 2, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(28, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[7, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #100: "fused_multiply_add_multiply_add_nn_relu_30"
[16:05:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:05:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #101: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48"
[16:05:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 192, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [32, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 4, 1, 32, 7, 1, 1, 32, 1, 1, 1, 1, 4, 4, 1, 6, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1)
                    oh = T.axis.spatial(28, i2_1 * 4 + i2_2)
                    ow = T.axis.spatial(28, i3_0 * 4 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(192, i5_0 * 6 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [32, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 32, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 7, 4, 1, 32, 7, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 1, 4, 4, 1, 6, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1)
                        oh = T.axis.spatial(28, i2_1 * 4 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(192, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [32, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 4, 4, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_1 * 4 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 32, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 7, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 32, 7, 1, 1, 32, 1, 1, 1, 1, 4, 4, 1, 6, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1)
                        oh = T.axis.spatial(28, i2_1 * 4 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(192, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [32, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 28, 4, 1):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(28, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 32, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #102: "fused_multiply_add_nn_relu_19"
[16:05:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 40, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 40, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 40, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 40, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 40, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #103: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49"
[16:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 160, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 28, 28, 4], "float32"], ["TENSOR", [32, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:05:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 4, 1, 1, 4, 14, 7, 1, 16, 1, 1, 1, 1, 2, 1, 2, 10, 1, 1, 1, 8, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3)
                    oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(28, i3_0 * 7 + i3_1)
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(160, i5_0 * 10 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 28, 28, 4], "float32"], ["TENSOR", [32, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 10])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:05:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 4, 1, 1, 4, 14, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 2, 1, 2, 10, 1, 1, 1, 8, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 7 + i3_1)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(160, i5_0 * 10 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 28, 28, 4], "float32"], ["TENSOR", [32, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 7 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 10])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 4, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 14, 7, 1, 16, 1, 1, 1, 1, 2, 1, 2, 10, 1, 1, 1, 8, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 7 + i3_1)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(160, i5_0 * 10 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 28, 28, 4], "float32"], ["TENSOR", [32, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 28, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(28, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 10])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:05:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #104: "fused_multiply_add_nn_relu_20"
[16:05:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:05:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:05:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:05:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #105: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50"
[16:05:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 2, 4, 4, 1, 2, 14, 7, 1, 64, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_1)
                    oh = T.axis.spatial(28, i2_0 * 14 + i2_1)
                    ow = T.axis.spatial(28, i3_0 * 7 + i3_1)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 2, 4, 4, 1, 2, 14, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_1)
                        ow = T.axis.spatial(28, i3_0 * 7 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 2 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + i2_1 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 7 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 16, 2, 4, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 14, 7, 1, 64, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_1)
                        ow = T.axis.spatial(28, i3_0 * 7 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 7, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #106: "fused_nn_contrib_conv2d_NCHWc_2"
[16:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 30, 30, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 28, 28, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

[16:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            conv2d_NCHWc_global = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 14, 2, 1, 2, 1, 1, 1):
                for i5_0, i6_0, i7_0 in T.grid(2, 3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 28, 2, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i5_0 * 16 + ax1)
                            i2 = T.axis.spatial(30, i6_0 + ax2)
                            i3 = T.axis.spatial(30, i3_0 * 2 + i7_0 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 1, 1, 64, 1, 1, 1, 1, 4, 2, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_2)
                            oh = T.axis.spatial(28, i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(28, i3_0 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 28, 2, 2):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + ax1)
                        v2 = T.axis.spatial(28, ax2)
                        v3 = T.axis.spatial(28, i3_0 * 2 + ax3)
                        v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[16:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            conv2d_NCHWc_global = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 14, 2):
                for i0_1, i1_1 in T.grid(1, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 30, 4, 4):
                        with T.block("data_pad"):
                            i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                            i3 = T.axis.spatial(30, i3_0 * 2 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 3, 3, 1, 2, 7, 1, 1, 64, 1, 1, 1, 1, 4, 2, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_2)
                            oh = T.axis.spatial(28, i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(28, i3_0 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 2, 2):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(8, i1_0 * 4 + ax1)
                        v2 = T.axis.spatial(28, ax2)
                        v3 = T.axis.spatial(28, i3_0 * 2 + ax3)
                        v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[16:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0 in T.grid(1, 2, 1, 14, 2, 1, 2, 1, 1, 1, 2, 3, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 28, 2, 4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i5_0 * 16 + ax1)
                        i2 = T.axis.spatial(30, i6_0 + ax2)
                        i3 = T.axis.spatial(30, i3_0 * 2 + i7_0 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 1, 1, 64, 1, 1, 1, 1, 4, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_2 * 4 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[16:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #107: "fused_multiply_add_multiply_add_nn_relu_31"
[16:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 56, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 56, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 56, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 56, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 56, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 56, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 56, 56, 56, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 56, 56, 56, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 56, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 56, 56, 56, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 56, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 56, 56, 56, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 56, 56, 56, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 56, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 56, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 56, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 56, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 56, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 56, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 56, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #108: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51"
[16:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 56, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 56, 56, 4, 224, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 56, 56, 56, 4], "float32"], ["TENSOR", [32, 56, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 56, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 2, 1, 1, 1, 2, 2, 1, 112, 1, 1, 1, 2, 4, 2, 4, 2, 1, 1, 1, 2, 7, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(56, i2_1 * 28 + i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(224, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 56, 56, 56, 4], "float32"], ["TENSOR", [32, 56, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[112, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 56, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 2, 1, 1, 1, 2, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(112, 1, 1, 1, 2, 4, 2, 4, 2, 1, 1, 1, 2, 7, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 28 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(224, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 56, 56, 56, 4], "float32"], ["TENSOR", [32, 56, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 14, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_1 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[112, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 56, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 1, 112, 1, 1, 1, 2, 4, 2, 4, 2, 1, 1, 1, 2, 7, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 28 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(224, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 56, 56, 56, 4], "float32"], ["TENSOR", [32, 56, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 56, 28, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(56, ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 28 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[112, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #109: "fused_multiply_add_multiply_add_nn_relu_32"
[16:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 48, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #110: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52"
[16:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 56, 56, 4, 192, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [32, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 2, 2, 2, 1, 4, 4, 2, 1, 6, 1, 1, 1, 2, 1, 2, 2, 32, 1, 1, 1, 1, 7, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 7 + i2_3)
                    ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(192, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [32, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 2, 2, 1, 4, 4, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(6, 1, 1, 1, 2, 1, 2, 2, 32, 1, 1, 1, 1, 7, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(192, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [32, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 14, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 2, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 4, 2, 1, 6, 1, 1, 1, 2, 1, 2, 2, 32, 1, 1, 1, 1, 7, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(192, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [32, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 28, 28, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 28 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #111: "fused_multiply_add_multiply_add_nn_relu_33"
[16:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 40, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 40, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 40, 56, 56, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 40, 56, 56, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 40, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 56, 56, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 56, 56, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 56, 56, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 40, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 40, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:06:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #112: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53"
[16:06:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 56, 56, 4, 160, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 56, 56, 4], "float32"], ["TENSOR", [32, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 14, 2, 1, 2, 1, 4, 1, 80, 1, 1, 1, 4, 4, 1, 2, 2, 1, 1, 1, 4, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(56, i2_0 * 8 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(56, i3_0 * 4 + i3_1)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(160, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 56, 56, 4], "float32"], ["TENSOR", [32, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 4, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 4, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[80, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 14, 2, 1, 2, 1, 4, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(80, 1, 1, 1, 4, 4, 1, 2, 2, 1, 1, 1, 4, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 8 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 4 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(160, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 56, 56, 4], "float32"], ["TENSOR", [32, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 8, 1, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 8 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 4 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 4, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 4, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[80, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 7, 14, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 4, 1, 80, 1, 1, 1, 4, 4, 1, 2, 2, 1, 1, 1, 4, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 8 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 4 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(160, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 56, 56, 4], "float32"], ["TENSOR", [32, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 8, 4, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(56, i2_0 * 8 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 4, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 4, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[80, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #113: "fused_multiply_add_nn_relu_21"
[16:06:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:06:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #114: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54"
[16:06:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 56, 56, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 8, 1, 1, 1, 32, 1, 1, 1, 2, 28, 2, 2, 4, 1, 1, 1, 2, 2, 28, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(56, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(56, i3_2 * 28 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 28, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 8, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 2, 28, 2, 2, 4, 1, 1, 1, 2, 2, 28, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_2 * 28 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 56, 56, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 28, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 1, 32, 1, 1, 1, 2, 28, 2, 2, 4, 1, 1, 1, 2, 2, 28, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_2 * 28 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 56, 56, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 28, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #115: "fused_multiply_add_multiply_add_nn_relu_34"
[16:06:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[16:06:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #116: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55"
[16:06:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 56, 56, 4, 96, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [32, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 2, 1, 2, 28, 2, 1, 12, 1, 1, 1, 2, 2, 4, 2, 8, 1, 1, 1, 1, 1, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(56, i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(56, i3_1 * 28 + i3_2 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(96, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [32, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 4, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[12, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 1, 2, 1, 2, 28, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 2, 2, 4, 2, 8, 1, 1, 1, 1, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(56, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(56, i3_1 * 28 + i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(96, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [32, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 28, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_1 * 28 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 4, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[12, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 28, 2, 1, 12, 1, 1, 1, 2, 2, 4, 2, 8, 1, 1, 1, 1, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(56, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(56, i3_1 * 28 + i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(96, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [32, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 56, 56, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 4, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[12, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #117: "fused_multiply_add_nn_relu_22"
[16:06:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:06:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #118: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56"
[16:06:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 56, 56, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 1, 2, 1, 4, 2, 7, 1, 2, 1, 1, 1, 1, 1, 2, 2, 32, 1, 1, 1, 4, 14, 4, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_3)
                    oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 14 + i2_3)
                    ow = T.axis.spatial(56, i3_1 * 8 + i3_2 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 1, 2, 1, 4, 2, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 1, 2, 2, 32, 1, 1, 1, 4, 14, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 14 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 8 + i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 8, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + i2_1 * 14 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_1 * 8 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 2, 7, 1, 2, 1, 1, 1, 1, 1, 2, 2, 32, 1, 1, 1, 4, 14, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 14 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 8 + i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 28, 56, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[16:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #119: "fused_nn_contrib_conv2d_NCHWc_3"
[16:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 58, 58, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 56, 56, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

[16:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
            conv2d_NCHWc_global = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 14, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 58, 6, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(58, i3_0 * 4 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 2, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 2, 1, 1, 1, 2, 3, 3, 1, 1, 8, 2, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                            oh = T.axis.spatial(56, i2_1 * 8 + i2_3)
                            ow = T.axis.spatial(56, i3_0 * 4 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 8, 2, 2):
                        with T.block("conv2d_NCHWc_global"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                            v2 = T.axis.spatial(56, i2_1 * 8 + ax2)
                            v3 = T.axis.spatial(56, i3_0 * 4 + i3_1 * 2 + ax3)
                            v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                            T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                            conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 8])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[16:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
            conv2d_NCHWc_global = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0 in T.grid(1, 4, 1, 14):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 58, 6, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(58, i3_0 * 4 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_0 in T.serial(2):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 2, 1, 64, 1, 1, 1, 2, 1, 1, 1, 2, 3, 3, 1, 1, 8, 2, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                            oh = T.axis.spatial(56, i2_1 * 8 + i2_3)
                            ow = T.axis.spatial(56, i3_0 * 4 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 56, 4, 2):
                        with T.block("conv2d_NCHWc_global"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                            v2 = T.axis.spatial(56, ax2)
                            v3 = T.axis.spatial(56, i3_0 * 4 + ax3)
                            v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                            T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                            conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 8])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[16:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 32, 3, 3, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 14, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 58, 6, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(58, i3_0 * 4 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 2, 1, 64, 1, 1, 1, 2, 1, 1, 1, 2, 3, 3, 1, 1, 8, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(56, i2_1 * 8 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 4 + i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [8, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 8])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[16:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #120: "fused_layout_transform"
[16:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[16:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #121: "fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1"
[16:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 230, 230, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 230, 230, 3):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(3 <= i2_1 and i2_1 < 227 and 3 <= i3_1 and i3_1 < 227, placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 112, 112, 4, 3, 7, 7):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 230, 230, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 229, 229, 3):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(230, ax2)
                        i3 = T.axis.spatial(230, ax3)
                        i4 = T.axis.spatial(3, ax4)
                        T.reads(placeholder[i0, i1, i2 - 3, i3 - 3, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(3 <= i2 and i2 < 227 and 3 <= i3 and i3 < 227, placeholder[i0, i1, i2 - 3, i3 - 3, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 16, 1, 1, 1, 1, 7, 1, 8, 1, 2, 2, 3, 7, 1, 1, 1, 7, 56, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_2)
                        oh = T.axis.spatial(112, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(112, i3_2 * 56 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_0])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l18, l19, l20, l21 = sch.split(loop=l6, factors=[v14, v15, v16, v17])
v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 8, 1])
l26, l27, l28, l29 = sch.split(loop=l7, factors=[v22, v23, v24, v25])
v30, v31, v32, v33 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 16, 1, 7])
l34, l35, l36, l37 = sch.split(loop=l8, factors=[v30, v31, v32, v33])
v38, v39, v40, v41 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 2, 56])
l42, l43, l44, l45 = sch.split(loop=l9, factors=[v38, v39, v40, v41])
v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l50, l51, l52, l53 = sch.split(loop=l10, factors=[v46, v47, v48, v49])
v54, v55 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l56, l57 = sch.split(loop=l11, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 7])
l60, l61 = sch.split(loop=l12, factors=[v58, v59])
v62, v63 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[7, 1])
l64, l65 = sch.split(loop=l13, factors=[v62, v63])
sch.reorder(l18, l26, l34, l42, l50, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52, l57, l61, l65, l21, l29, l37, l45, l53)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
l67 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l67, preserve_unit_loops=True)
[16:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 230, 230, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 230, 230, 3):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(3 <= i2_1 and i2_1 < 227 and 3 <= i3_1 and i3_1 < 227, placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 1, 1, 1, 2, 1, 2, 16, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 8, 1, 2, 2, 3, 7, 1, 1, 1, 7, 56, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1_1 * 8 + i1_2)
                        oh = T.axis.spatial(112, i2_1_1 * 7 + i2_3)
                        ow = T.axis.spatial(112, i3_2 * 56 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_0])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 112, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(112, i2_1_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(112, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max((conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l18, l19, l20, l21 = sch.split(loop=l6, factors=[v14, v15, v16, v17])
v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 8, 1])
l26, l27, l28, l29 = sch.split(loop=l7, factors=[v22, v23, v24, v25])
v30, v31, v32, v33 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 16, 1, 7])
l34, l35, l36, l37 = sch.split(loop=l8, factors=[v30, v31, v32, v33])
v38, v39, v40, v41 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 2, 56])
l42, l43, l44, l45 = sch.split(loop=l9, factors=[v38, v39, v40, v41])
v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l50, l51, l52, l53 = sch.split(loop=l10, factors=[v46, v47, v48, v49])
v54, v55 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l56, l57 = sch.split(loop=l11, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 7])
l60, l61 = sch.split(loop=l12, factors=[v58, v59])
v62, v63 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[7, 1])
l64, l65 = sch.split(loop=l13, factors=[v62, v63])
sch.reorder(l18, l26, l34, l42, l50, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52, l57, l61, l65, l21, l29, l37, l45, l53)
b66, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b66, loop=l51, preserve_unit_loops=True)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v67 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v67)
l68 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l68, preserve_unit_loops=True)
[16:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 7, 7, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 230, 230, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 229, 229, 3):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(230, ax2)
                        i3 = T.axis.spatial(230, ax3)
                        i4 = T.axis.spatial(3, ax4)
                        T.reads(placeholder[i0, i1, i2 - 3, i3 - 3, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(3 <= i2 and i2 < 227 and 3 <= i3 and i3 < 227, placeholder[i0, i1, i2 - 3, i3 - 3, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 16, 1, 1, 1, 1, 7, 1, 8, 1, 2, 2, 3, 7, 1, 1, 1, 7, 56, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_2)
                        oh = T.axis.spatial(112, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(112, i3_2 * 56 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_0])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [16, 1, 7, 7, 3, 4], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 112, 112, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max((conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l18, l19, l20, l21 = sch.split(loop=l6, factors=[v14, v15, v16, v17])
v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 8, 1])
l26, l27, l28, l29 = sch.split(loop=l7, factors=[v22, v23, v24, v25])
v30, v31, v32, v33 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 16, 1, 7])
l34, l35, l36, l37 = sch.split(loop=l8, factors=[v30, v31, v32, v33])
v38, v39, v40, v41 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 2, 56])
l42, l43, l44, l45 = sch.split(loop=l9, factors=[v38, v39, v40, v41])
v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l50, l51, l52, l53 = sch.split(loop=l10, factors=[v46, v47, v48, v49])
v54, v55 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l56, l57 = sch.split(loop=l11, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 7])
l60, l61 = sch.split(loop=l12, factors=[v58, v59])
v62, v63 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[7, 1])
l64, l65 = sch.split(loop=l13, factors=[v62, v63])
sch.reorder(l18, l26, l34, l42, l50, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52, l57, l61, l65, l21, l29, l37, l45, l53)
b66, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b66, loop=l50, preserve_unit_loops=True)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v67 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v67)
l68 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l68, preserve_unit_loops=True)
[16:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #122: "fused_nn_max_pool2d"
[16:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], tensor: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 114, 114, 4):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1, ax4])
                T.writes(pad_temp[ax0, ax1, ax2, ax3, ax4])
                pad_temp[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(1 <= ax2 and ax2 < 113 and 1 <= ax3 and ax3 < 113, placeholder[ax0, ax1, ax2 - 1, ax3 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 56, 56, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], tensor: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 16, 56, 56, 4, 9], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 16, 56, 56, 4, 9, 1):
                with T.block("tensor_rf"):
                    vi5_i6_fused_0 = T.axis.spatial(9, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_0 // 3 - 1, ax3 * 2 + vi5_i6_fused_0 % 3 - 1, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.if_then_else(1 <= ax2 * 2 + vi5_i6_fused_0 // 3 and ax2 * 2 + vi5_i6_fused_0 // 3 < 113 and 1 <= ax3 * 2 + vi5_i6_fused_0 % 3 and ax3 * 2 + vi5_i6_fused_0 % 3 < 113, placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_0 // 3 - 1, ax3 * 2 + vi5_i6_fused_0 % 3 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 16, 56, 56, 4, 9, 1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(9, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[9, 1])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], tensor: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 16, 56, 56, 4, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 16, 56, 56, 4, 9, 1):
                with T.block("tensor_rf"):
                    vi5_i6_fused_1 = T.axis.spatial(1, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    rv0 = T.axis.reduce(3, i5_i6_fused_0 // 3)
                    rv1 = T.axis.reduce(3, i5_i6_fused_0 % 3)
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 113 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 113, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32"))
            for i0, i1, i2, i3, i4, i5_i6_fused_1 in T.grid(1, 16, 56, 56, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(1, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[9, 1])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], tensor: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 16, 56):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 3, 113, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1 + ax1)
                        ax2_1 = T.axis.spatial(114, i2 * 2 + ax2)
                        ax3_1 = T.axis.spatial(114, ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(1 <= ax2_1 and ax2_1 < 113 and 1 <= ax3_1 and ax3_1 < 113, placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i3, i4, i5, i6 in T.grid(56, 4, 3, 3):
                    with T.block("tensor"):
                        ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                        T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        with T.init():
                            tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #123: "fused_concatenate"
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_concat: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 16, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(16 <= ax1, placeholder_1[ax0, ax1 - 16, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_concat: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 16, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(16 <= ax1, placeholder_1[ax0, ax1 - 16, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #124: "fused_concatenate_1"
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_concat: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 24, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(24 <= ax1, placeholder_1[ax0, ax1 - 24, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_concat: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 24, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(24 <= ax1, placeholder_1[ax0, ax1 - 24, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #125: "fused_concatenate_2"
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_concat: T.Buffer[(1, 40, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 56, 56, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(32 <= ax1, placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_concat: T.Buffer[(1, 40, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 40, 56, 56, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(32 <= ax1, placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #126: "fused_concatenate_3"
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_concat: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 40, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(40 <= ax1, placeholder_1[ax0, ax1 - 40, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_concat: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 40, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(40 <= ax1, placeholder_1[ax0, ax1 - 40, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #127: "fused_concatenate_4"
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_concat: T.Buffer[(1, 56, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 56, 56, 56, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(48 <= ax1, placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_concat: T.Buffer[(1, 56, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 56, 56, 56, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(48 <= ax1, placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #128: "fused_concatenate_multiply_add_multiply_add_nn_relu"
[16:06:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_5: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 64, 56, 56, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 64, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 56, 56, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 64, 56, 56, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 56, 56, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 56, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(56 <= ax1, placeholder_1[ax0, ax1 - 56, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 56, 56, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_concat[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = T_concat[ax0, ax1, ax2, ax3, ax4] * placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 56, 56, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 56, 56, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_5[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_5[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 56, 56, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_5: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 64, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_concat = T.alloc_buffer([1, 64, 56, 56, 4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 64, 56):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 56, 4):
                    with T.block("T_concat"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1 + ax1)
                        ax2_1 = T.axis.spatial(56, i2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder_1[ax0_1, ax1_1 - 56, ax2_1, ax3_1, ax4_1], placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_concat[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_concat[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(56 <= ax1_1, placeholder_1[ax0_1, ax1_1 - 56, ax2_1, ax3_1, ax4_1], placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                for i3, i4 in T.grid(56, 4):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_concat[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4], placeholder_5[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((T_concat[ax0, ax1, ax2, ax3, ax4] * placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, 0, 0, ax4]) * placeholder_4[ax0, ax1, 0, 0, ax4] + placeholder_5[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_concat", func_name="main")
b1 = sch.get_block(name="T_multiply", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply_1", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v6 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v6)
l7 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[16:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #129: "fused_nn_contrib_conv2d_NCHWc_4"
[16:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 56, 56, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 56, 56, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc_global = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 2, 1, 2, 8, 28, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 4, 7, 2, 1, 8, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(56, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 56, 56, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 2, 2):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + ax1)
                        v2 = T.axis.spatial(56, i2_1 * 7 + ax2)
                        v3 = T.axis.spatial(56, i3_1 * 2 + ax3)
                        v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 2, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc_global = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 8, 28, 1, 32, 1, 1, 1, 4, 7, 2, 1, 8, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(56, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 56, 56, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 56, 56, 2):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        v2, v3 = T.axis.remap("SS", [ax2, ax3])
                        v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 2, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 2, 1, 2, 8, 28, 1, 32, 1, 1, 1, 4, 7, 2, 1, 8, 1, 1, 1, 2, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(56, i2_1 * 7 + i2_2)
                    ow = T.axis.spatial(56, i3_1 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 56, 56, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 2, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #130: "fused_nn_avg_pool2d"
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], tensor: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 28, 28, 4, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 55) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 55) + 1 - ax3 * 2), 1), "float32")
    

[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], tensor: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 28, 28, 4, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 55) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 55) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #131: "fused_concatenate_5"
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 40, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(32 <= ax1, placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 40, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 40, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(32 <= ax1, placeholder_1[ax0, ax1 - 32, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #132: "fused_concatenate_6"
[16:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 40, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(40 <= ax1, placeholder_1[ax0, ax1 - 40, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 40, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(40 <= ax1, placeholder_1[ax0, ax1 - 40, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #133: "fused_concatenate_7"
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 56, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 56, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(48 <= ax1, placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 56, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 56, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(48 <= ax1, placeholder_1[ax0, ax1 - 48, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #134: "fused_concatenate_8"
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 56, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(56 <= ax1, placeholder_1[ax0, ax1 - 56, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 56, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(56 <= ax1, placeholder_1[ax0, ax1 - 56, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #135: "fused_concatenate_9"
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 72, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 72, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 72, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 72, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #136: "fused_concatenate_10"
[16:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 80, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 80, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 72, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(72 <= ax1, placeholder_1[ax0, ax1 - 72, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 80, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 80, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 72, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(72 <= ax1, placeholder_1[ax0, ax1 - 72, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #137: "fused_concatenate_11"
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 88, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 88, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 80, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(80 <= ax1, placeholder_1[ax0, ax1 - 80, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 88, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 88, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 80, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(80 <= ax1, placeholder_1[ax0, ax1 - 80, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #138: "fused_concatenate_12"
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 88, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(88 <= ax1, placeholder_1[ax0, ax1 - 88, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 88, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(88 <= ax1, placeholder_1[ax0, ax1 - 88, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #139: "fused_concatenate_13"
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 104, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 104, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 96, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(96 <= ax1, placeholder_1[ax0, ax1 - 96, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 104, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 104, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 96, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(96 <= ax1, placeholder_1[ax0, ax1 - 96, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #140: "fused_concatenate_14"
[16:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 112, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 104, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(104 <= ax1, placeholder_1[ax0, ax1 - 104, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 112, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 112, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 104, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(104 <= ax1, placeholder_1[ax0, ax1 - 104, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #141: "fused_concatenate_15"
[16:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 120, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 120, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 112, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(112 <= ax1, placeholder_1[ax0, ax1 - 112, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_concat: T.Buffer[(1, 120, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 120, 28, 28, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 112, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(112 <= ax1, placeholder_1[ax0, ax1 - 112, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #142: "fused_concatenate_multiply_add_multiply_add_nn_relu_1"
[16:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_5: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 128, 28, 28, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 128, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 128, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 128, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 28, 28, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 120, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(120 <= ax1, placeholder_1[ax0, ax1 - 120, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 28, 28, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_concat[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = T_concat[ax0, ax1, ax2, ax3, ax4] * placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 28, 28, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_5[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_5[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 28, 28, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_5: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 128, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_concat = T.alloc_buffer([1, 128, 28, 28, 4], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 4):
                    with T.block("T_concat"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1 = T.axis.spatial(28, i2 + ax2)
                        ax3_1 = T.axis.spatial(28, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(placeholder_1[ax0_1, ax1_1 - 120, ax2_1, ax3_1, ax4_1], placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_concat[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_concat[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(120 <= ax1_1, placeholder_1[ax0_1, ax1_1 - 120, ax2_1, ax3_1, ax4_1], placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                for i4 in T.serial(4):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_concat[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4], placeholder_5[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((T_concat[ax0, ax1, ax2, ax3, ax4] * placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, 0, 0, ax4]) * placeholder_4[ax0, ax1, 0, 0, ax4] + placeholder_5[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_concat", func_name="main")
b1 = sch.get_block(name="T_multiply", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply_1", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v6 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v6)
l7 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #143: "fused_nn_contrib_conv2d_NCHWc_5"
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 28, 28, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 28, 28, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc_global = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 28, 2, 1, 64, 4, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(512, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_1)
                        oh = T.axis.spatial(28, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_0)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 28, 28, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 7, 1, 2):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(64, i1_1 + ax1)
                        v2 = T.axis.spatial(28, i2_1 * 7 + ax2)
                        v3 = T.axis.spatial(28, i3_0 + ax3)
                        v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 64, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[28, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[512, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc_global = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 28, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 64, 4, 1, 1, 512, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_1)
                        oh = T.axis.spatial(28, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_0)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 28, 28, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 28, 1, 2):
                    with T.block("conv2d_NCHWc_global"):
                        v0, v1, v2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        v3 = T.axis.spatial(28, i3_0 + ax3)
                        v4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 64, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[28, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[512, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 28, 2, 1, 64, 4, 1, 1, 512, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_1)
                    oh = T.axis.spatial(28, i2_1 * 7 + i2_2)
                    ow = T.axis.spatial(28, i3_0)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(512, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 28, 28, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 64, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[28, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[512, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #144: "fused_nn_avg_pool2d_1"
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], tensor: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 14, 14, 4, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 27) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 27) + 1 - ax3 * 2), 1), "float32")
    

[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], tensor: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 1, 2, 2):
                    with T.block("tensor"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1 + ax1)
                        ax2_1 = T.axis.spatial(14, i2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        rv0, rv1 = T.axis.remap("RR", [ax5, ax6])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 2 + rv0, ax3_1 * 2 + rv1, ax4_1])
                        T.writes(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder[ax0_1, ax1_1, ax2_1 * 2 + rv0, ax3_1 * 2 + rv1, ax4_1]
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 27) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 27) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #145: "fused_concatenate_16"
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 72, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 72, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 72, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 72, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #146: "fused_concatenate_17"
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 80, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 80, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 72, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(72 <= ax1, placeholder_1[ax0, ax1 - 72, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 72, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 80, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 80, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 72, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(72 <= ax1, placeholder_1[ax0, ax1 - 72, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #147: "fused_concatenate_18"
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 88, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 88, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 80, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(80 <= ax1, placeholder_1[ax0, ax1 - 80, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 88, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 88, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 80, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(80 <= ax1, placeholder_1[ax0, ax1 - 80, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #148: "fused_concatenate_19"
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 88, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(88 <= ax1, placeholder_1[ax0, ax1 - 88, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 88, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 88, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(88 <= ax1, placeholder_1[ax0, ax1 - 88, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #149: "fused_concatenate_20"
[16:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 104, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 104, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 96, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(96 <= ax1, placeholder_1[ax0, ax1 - 96, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 104, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 104, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 96, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(96 <= ax1, placeholder_1[ax0, ax1 - 96, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #150: "fused_concatenate_21"
[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 112, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 104, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(104 <= ax1, placeholder_1[ax0, ax1 - 104, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 104, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 112, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 112, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 104, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(104 <= ax1, placeholder_1[ax0, ax1 - 104, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #151: "fused_concatenate_22"
[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 120, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 120, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 112, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(112 <= ax1, placeholder_1[ax0, ax1 - 112, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 120, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 120, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 112, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(112 <= ax1, placeholder_1[ax0, ax1 - 112, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #152: "fused_concatenate_23"
[16:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 120, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(120 <= ax1, placeholder_1[ax0, ax1 - 120, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 120, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 120, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(120 <= ax1, placeholder_1[ax0, ax1 - 120, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #153: "fused_concatenate_24"
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 136, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 136, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 136, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #154: "fused_concatenate_25"
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 136, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(136 <= ax1, placeholder_1[ax0, ax1 - 136, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 136, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(136 <= ax1, placeholder_1[ax0, ax1 - 136, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #155: "fused_concatenate_26"
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 152, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 144, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(144 <= ax1, placeholder_1[ax0, ax1 - 144, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 152, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 152, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 144, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(144 <= ax1, placeholder_1[ax0, ax1 - 144, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #156: "fused_concatenate_27"
[16:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 160, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 152, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(152 <= ax1, placeholder_1[ax0, ax1 - 152, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 160, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 160, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 152, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(152 <= ax1, placeholder_1[ax0, ax1 - 152, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #157: "fused_concatenate_28"
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 160, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(160 <= ax1, placeholder_1[ax0, ax1 - 160, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 160, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(160 <= ax1, placeholder_1[ax0, ax1 - 160, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #158: "fused_concatenate_29"
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 176, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 168, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(168 <= ax1, placeholder_1[ax0, ax1 - 168, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 176, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 176, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 168, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(168 <= ax1, placeholder_1[ax0, ax1 - 168, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #159: "fused_concatenate_30"
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 184, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 176, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(176 <= ax1, placeholder_1[ax0, ax1 - 176, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 184, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 184, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 176, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(176 <= ax1, placeholder_1[ax0, ax1 - 176, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #160: "fused_concatenate_31"
[16:06:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 192, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 184, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(184 <= ax1, placeholder_1[ax0, ax1 - 184, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 192, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 192, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 184, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(184 <= ax1, placeholder_1[ax0, ax1 - 184, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #161: "fused_concatenate_32"
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 200, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 192, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(192 <= ax1, placeholder_1[ax0, ax1 - 192, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 200, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 200, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 192, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(192 <= ax1, placeholder_1[ax0, ax1 - 192, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #162: "fused_concatenate_33"
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 208, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 200, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(200 <= ax1, placeholder_1[ax0, ax1 - 200, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 208, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 208, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 200, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(200 <= ax1, placeholder_1[ax0, ax1 - 200, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #163: "fused_concatenate_34"
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 216, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 208, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(208 <= ax1, placeholder_1[ax0, ax1 - 208, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 216, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 216, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 208, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(208 <= ax1, placeholder_1[ax0, ax1 - 208, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #164: "fused_concatenate_35"
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 224, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 216, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(216 <= ax1, placeholder_1[ax0, ax1 - 216, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 224, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 224, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 216, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(216 <= ax1, placeholder_1[ax0, ax1 - 216, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #165: "fused_concatenate_36"
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 232, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 224, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(224 <= ax1, placeholder_1[ax0, ax1 - 224, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 232, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 232, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 224, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(224 <= ax1, placeholder_1[ax0, ax1 - 224, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #166: "fused_concatenate_37"
[16:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 232, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(232 <= ax1, placeholder_1[ax0, ax1 - 232, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 232, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(232 <= ax1, placeholder_1[ax0, ax1 - 232, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #167: "fused_concatenate_38"
[16:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 248, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 240, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(240 <= ax1, placeholder_1[ax0, ax1 - 240, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], T_concat: T.Buffer[(1, 248, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 248, 14, 14, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 240, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(240 <= ax1, placeholder_1[ax0, ax1 - 240, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #168: "fused_concatenate_multiply_add_nn_relu"
[16:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 256, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 256, 14, 14, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 256, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 14, 14, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 248, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(248 <= ax1, placeholder_1[ax0, ax1 - 248, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 14, 14, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_concat[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = T_concat[ax0, ax1, ax2, ax3, ax4] * placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 14, 14, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 256, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_concat = T.alloc_buffer([1, 256, 14, 14, 4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 256, 14):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 14, 4):
                    with T.block("T_concat"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 + ax1)
                        ax2_1 = T.axis.spatial(14, i2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder_1[ax0_1, ax1_1 - 248, ax2_1, ax3_1, ax4_1], placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_concat[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_concat[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(248 <= ax1_1, placeholder_1[ax0_1, ax1_1 - 248, ax2_1, ax3_1, ax4_1], placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                for i3, i4 in T.grid(14, 4):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_concat[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_concat[ax0, ax1, ax2, ax3, ax4] * placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_concat", func_name="main")
b1 = sch.get_block(name="T_multiply", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l5, preserve_unit_loops=True)
[16:06:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #169: "fused_nn_contrib_conv2d_NCHWc_6"
[16:06:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 14, 14, 4, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 14, 14, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

[16:06:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc_global = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 2, 1, 1, 1, 16, 7, 2, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 1, 1, 7, 1, 16, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 16 + i1_1)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                        ow = T.axis.spatial(14, i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 14, 14, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 7, 2):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(128, i1_0 * 16 + i1_1 + ax1)
                        v2 = T.axis.spatial(14, i2_0 * 7 + i2_1 + ax2)
                        v3 = T.axis.spatial(14, i3_1 * 7 + ax3)
                        v4 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 16, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc_global = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 2, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 7, 2, 2, 64, 1, 1, 1, 1, 1, 7, 1, 16, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 16 + i1_1)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                        ow = T.axis.spatial(14, i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 14, 14, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 14, 4):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(128, i1_0 * 16 + ax1)
                        v2 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        v3, v4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 16, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 2, 1, 1, 1, 16, 7, 2, 2, 64, 1, 1, 1, 1, 1, 7, 1, 16, 1, 1, 1, 1, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 16 + i1_1)
                    oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                    ow = T.axis.spatial(14, i3_1 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(1024, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 14, 14, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 16, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[16:06:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #170: "fused_nn_avg_pool2d_2"
[16:06:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 4, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 13) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 13) + 1 - ax3 * 2), 1), "float32")
    

[16:06:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], tensor: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 4, 2, 2):
                    with T.block("tensor"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1 = T.axis.spatial(7, i2 + ax2)
                        ax3_1 = T.axis.spatial(7, i3 + ax3)
                        ax4_1, rv0, rv1 = T.axis.remap("SRR", [ax4, ax5, ax6])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 2 + rv0, ax3_1 * 2 + rv1, ax4_1])
                        T.writes(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder[ax0_1, ax1_1, ax2_1 * 2 + rv0, ax3_1 * 2 + rv1, ax4_1]
                for i4 in T.serial(4):
                    with T.block("tensor_1"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 13) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 13) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[16:06:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #171: "fused_concatenate_39"
[16:06:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 136, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #172: "fused_concatenate_40"
[16:06:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 136, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(136 <= ax1, placeholder_1[ax0, ax1 - 136, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 136, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(136 <= ax1, placeholder_1[ax0, ax1 - 136, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #173: "fused_concatenate_41"
[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 152, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 152, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 144, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(144 <= ax1, placeholder_1[ax0, ax1 - 144, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 152, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 152, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 144, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(144 <= ax1, placeholder_1[ax0, ax1 - 144, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #174: "fused_concatenate_42"
[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 160, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 160, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 152, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(152 <= ax1, placeholder_1[ax0, ax1 - 152, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 152, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 160, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 160, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 152, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(152 <= ax1, placeholder_1[ax0, ax1 - 152, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #175: "fused_concatenate_43"
[16:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 168, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 160, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(160 <= ax1, placeholder_1[ax0, ax1 - 160, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 168, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 168, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 160, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(160 <= ax1, placeholder_1[ax0, ax1 - 160, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #176: "fused_concatenate_44"
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 176, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 176, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 168, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(168 <= ax1, placeholder_1[ax0, ax1 - 168, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 176, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 176, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 168, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(168 <= ax1, placeholder_1[ax0, ax1 - 168, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #177: "fused_concatenate_45"
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 184, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 184, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 176, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(176 <= ax1, placeholder_1[ax0, ax1 - 176, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 176, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 184, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 184, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 176, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(176 <= ax1, placeholder_1[ax0, ax1 - 176, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #178: "fused_concatenate_46"
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 192, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 184, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(184 <= ax1, placeholder_1[ax0, ax1 - 184, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 184, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 192, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 184, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(184 <= ax1, placeholder_1[ax0, ax1 - 184, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #179: "fused_concatenate_47"
[16:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 200, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 192, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(192 <= ax1, placeholder_1[ax0, ax1 - 192, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 200, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 192, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(192 <= ax1, placeholder_1[ax0, ax1 - 192, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #180: "fused_concatenate_48"
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 208, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 200, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(200 <= ax1, placeholder_1[ax0, ax1 - 200, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 208, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 200, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(200 <= ax1, placeholder_1[ax0, ax1 - 200, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #181: "fused_concatenate_49"
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 216, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 208, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(208 <= ax1, placeholder_1[ax0, ax1 - 208, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 216, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 208, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(208 <= ax1, placeholder_1[ax0, ax1 - 208, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #182: "fused_concatenate_50"
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 224, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 216, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(216 <= ax1, placeholder_1[ax0, ax1 - 216, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 224, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 216, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(216 <= ax1, placeholder_1[ax0, ax1 - 216, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #183: "fused_concatenate_51"
[16:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 232, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 224, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(224 <= ax1, placeholder_1[ax0, ax1 - 224, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 232, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 224, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(224 <= ax1, placeholder_1[ax0, ax1 - 224, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #184: "fused_concatenate_52"
[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 232, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(232 <= ax1, placeholder_1[ax0, ax1 - 232, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 232, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(232 <= ax1, placeholder_1[ax0, ax1 - 232, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #185: "fused_concatenate_53"
[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 248, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 240, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(240 <= ax1, placeholder_1[ax0, ax1 - 240, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], T_concat: T.Buffer[(1, 248, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 240, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(240 <= ax1, placeholder_1[ax0, ax1 - 240, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #186: "fused_concatenate_multiply_add_multiply_add_nn_relu_2"
[16:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_5: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 248, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(248 <= ax1, placeholder_1[ax0, ax1 - 248, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_concat[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = T_concat[ax0, ax1, ax2, ax3, ax4] * placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_5[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_5[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:06:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 8, 7, 7, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_5: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 256, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_concat = T.alloc_buffer([1, 256, 7, 7, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 7, 7, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_concat"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 + ax1)
                        ax2_1 = T.axis.spatial(7, i2 + ax2)
                        ax3_1 = T.axis.spatial(7, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        T.reads(placeholder_1[ax0_1, ax1_1 - 248, ax2_1, ax3_1, ax4_1], placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_concat[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_concat[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(248 <= ax1_1, placeholder_1[ax0_1, ax1_1 - 248, ax2_1, ax3_1, ax4_1], placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(T_concat[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4], placeholder_5[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((T_concat[ax0, ax1, ax2, ax3, ax4] * placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, 0, 0, ax4]) * placeholder_4[ax0, ax1, 0, 0, ax4] + placeholder_5[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="T_concat", func_name="main")
b1 = sch.get_block(name="T_multiply", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply_1", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v6 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v6)
l7 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[16:06:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #187: "fused_nn_global_avg_pool2d"
[16:06:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 1, 1, 4, 7, 7):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1, ax4])
                T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 1, 1, 4):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] * T.float32(0.020408163265306121)
    

[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 256, 1, 1, 4, 49], dtype="float32")
            for i0, i1 in T.grid(1, 256):
                for ax0 in T.serial(49):
                    for ax0_1, ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 1, 1, 4):
                        with T.block("tensor_rf"):
                            vi5_i6_fused_0 = T.axis.spatial(49, ax0 + ax0_1)
                            ax0_2 = T.axis.spatial(1, ax1)
                            ax1_1 = T.axis.spatial(256, i1 + ax2)
                            ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax3, ax4, ax5])
                            T.reads(placeholder[ax0_2, ax1_1, ax2_1 * 7 + vi5_i6_fused_0 // 7, ax3_1 * 7 + vi5_i6_fused_0 % 7, ax4_1])
                            T.writes(tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0])
                            tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = placeholder[ax0_2, ax1_1, ax2_1 * 7 + vi5_i6_fused_0 // 7, ax3_1 * 7 + vi5_i6_fused_0 % 7, ax4_1]
                    for ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 1, 4):
                        with T.block("tensor"):
                            vi5_i6_fused_0, ax0_3 = T.axis.remap("RS", [ax0, ax1])
                            ax1_2 = T.axis.spatial(256, i1 + ax2)
                            ax2_2, ax3_2, ax4_2 = T.axis.remap("SSS", [ax3, ax4, ax5])
                            T.reads(tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0])
                            T.writes(tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2])
                            with T.init():
                                tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = T.float32(0)
                            tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] + tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0]
                for i2, i3, i4 in T.grid(1, 1, 4):
                    with T.block("tensor_1"):
                        ax0_4, ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                        T.writes(tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                        tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] = tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[49, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=2)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 256, 1, 1, 4, 1], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7 in T.grid(1, 1, 1, 1, 1, 1, 7, 7):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_1, ax0_1 = T.axis.remap("SS", [ax0, ax1])
                        ax1_1 = T.axis.spatial(256, i1 + ax2)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax3, ax4])
                        ax4_1 = T.axis.spatial(4, i4 + ax5)
                        rv0, rv1 = T.axis.remap("RR", [ax6, ax7])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 7 + rv0, ax3_1 * 7 + rv1, ax4_1])
                        T.writes(tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1])
                        with T.init():
                            tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] = T.float32(0)
                        tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] = tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] + placeholder[ax0_1, ax1_1, ax2_1 * 7 + rv0, ax3_1 * 7 + rv1, ax4_1]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 1, 1, 1):
                    with T.block("tensor"):
                        vi5_i6_fused_1, ax0_2 = T.axis.remap("RS", [ax0, ax1])
                        ax1_2 = T.axis.spatial(256, i1 + ax2)
                        ax2_2, ax3_2 = T.axis.remap("SS", [ax3, ax4])
                        ax4_2 = T.axis.spatial(4, i4 + ax5)
                        T.reads(tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_1])
                        T.writes(tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2])
                        with T.init():
                            tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = T.float32(0)
                        tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] + tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_1]
                with T.block("tensor_1"):
                    ax0_3, ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                    T.writes(tensor[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                    tensor[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] = tensor_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[49, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=4)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 256, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 256, 1, 1, 4], dtype="float32")
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 1, 1, 4, 7, 7):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1, ax4])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1, ax4]
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 1, 1, 4):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #188: "fused_nn_contrib_conv2d_NCHWc_add"
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_1: T.Buffer[(250, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 250, 1, 1, 4, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 1, 1, 4], "float32"], ["TENSOR", [250, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 250, 1, 1, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, ax2, ax3, ax4]
    

[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_1: T.Buffer[(250, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 10, 1, 1, 1, 1, 1, 1, 1, 2, 256, 1, 1, 1, 25, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(250, i1_0 * 25 + i1_2)
                    oh = T.axis.spatial(1, 0)
                    ow = T.axis.spatial(1, 0)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(1024, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 1, 1, 4], "float32"], ["TENSOR", [250, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 250, 1, 1, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[10, 1, 25, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_1: T.Buffer[(250, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 10, 1, 1, 1, 1, 1, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 25, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(250, i1_0 * 25 + i1_2)
                        oh = T.axis.spatial(1, 0)
                        ow = T.axis.spatial(1, 0)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 1, 1, 4], "float32"], ["TENSOR", [250, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 25, 1, 1, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(250, i1_0 * 25 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[10, 1, 25, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 1, 1, 4), "float32"], placeholder_1: T.Buffer[(250, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 10, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 256, 1, 1, 1, 25, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(250, i1_0 * 25 + i1_2)
                        oh = T.axis.spatial(1, 0)
                        ow = T.axis.spatial(1, 0)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 1, 1, 4], "float32"], ["TENSOR", [250, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 25, 1, 1, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(250, i1_0 * 25 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[10, 1, 25, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #189: "fused_layout_transform_1"
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_layout_trans: T.Buffer[(1, 1000, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 1000, 1, 1):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 1000 and ax2 < 1 and ax3 < 1, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_layout_trans: T.Buffer[(1, 1000, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 1000, 1, 1):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 1000 and ax2 < 1 and ax3 < 1, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |            N/A |          N/A |                   N/A |      0 |            
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |            N/A |          N/A |                   N/A |      0 |            
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |            N/A |          N/A |                   N/A |      0 |            
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |            N/A |          N/A |                   N/A |      0 |            
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |            N/A |          N/A |                   N/A |      0 |            
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[16:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_multiply_add_multiply_add_nn_relu"
[16:06:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:06:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:06:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a44d6548)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169fd1b358)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169e4acf68)]: 0 failure(s)
[16:06:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:06:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a44d6548)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169fd1b358)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169e4acf68)]: 0 failure(s)
[16:06:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a44d6548)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169fd1b358)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169e4acf68)]: 0 failure(s)
[16:06:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a44d6548)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169fd1b358)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169e4acf68)]: 0 failure(s)
[16:07:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a44d6548)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169fd1b358)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169e4acf68)]: 0 failure(s)
[16:07:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.7957  0.7453  0.4334  0.1635
[16:07:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[16:07:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[16:07:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[16:07:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[16:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[16:07:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:07:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:07:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00c74a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169defcdf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a441f828)]: 0 failure(s)
[16:07:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:07:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00c74a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169defcdf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a441f828)]: 0 failure(s)
[16:07:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00c74a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169defcdf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a441f828)]: 0 failure(s)
[16:07:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00c74a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169defcdf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a441f828)]: 0 failure(s)
[16:08:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00c74a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169defcdf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a441f828)]: 0 failure(s)
[16:08:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9995  0.9993  0.9992  0.9992  0.9989  0.9989  0.9988  0.9988  0.9988  0.9985  0.9985  0.9985  0.9984  0.9983  0.9982
[17 : 32]:	0.9982  0.9980  0.9973  0.9970  0.9968  0.9968  0.9967  0.9967  0.9967  0.9966  0.9964  0.9963  0.9962  0.9961  0.9960  0.9959
[16:08:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:08:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:08:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:08:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:08:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_multiply_add_multiply_add_nn_relu_1"
[16:08:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:08:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:09:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a014b668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169db222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169fe622c8)]: 0 failure(s)
[16:09:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:09:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a014b668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169db222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169fe622c8)]: 0 failure(s)
[16:09:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a014b668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169db222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169fe622c8)]: 0 failure(s)
[16:09:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a014b668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169db222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169fe622c8)]: 0 failure(s)
[16:09:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a014b668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169db222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169fe622c8)]: 0 failure(s)
[16:09:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.8921  0.8781  0.6849  0.4122
[16:09:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[16:09:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[16:09:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[16:09:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[16:09:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[16:09:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:09:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:10:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0165628)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a2fae1f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a08f87a8)]: 0 failure(s)
[16:10:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:10:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0165628)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a2fae1f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a08f87a8)]: 0 failure(s)
[16:10:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0165628)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a2fae1f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a08f87a8)]: 0 failure(s)
[16:10:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0165628)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a2fae1f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a08f87a8)]: 0 failure(s)
[16:11:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0165628)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a2fae1f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a08f87a8)]: 0 failure(s)
[16:11:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9997  0.9996  0.9994  0.9992  0.9991  0.9989  0.9988  0.9987  0.9987  0.9987  0.9986  0.9986  0.9982  0.9981  0.9977
[17 : 32]:	0.9976  0.9976  0.9976  0.9975  0.9975  0.9974  0.9973  0.9972  0.9970  0.9969  0.9969  0.9969  0.9967  0.9967  0.9965  0.9964
[16:11:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:11:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:11:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:11:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:11:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_multiply_add_multiply_add_nn_relu_2"
[16:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:11:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169e6d1e88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169f011c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a08f8c78)]: 0 failure(s)
[16:11:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:11:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169e6d1e88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169f011c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a08f8c78)]: 0 failure(s)
[16:11:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169e6d1e88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169f011c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a08f8c78)]: 0 failure(s)
[16:11:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169e6d1e88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169f011c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a08f8c78)]: 0 failure(s)
[16:12:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169e6d1e88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169f011c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a08f8c78)]: 0 failure(s)
[16:12:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.7439  0.6975  0.4558  0.2644
[16:12:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[16:12:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[16:12:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[16:12:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[16:12:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
[16:12:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:12:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:13:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a4364668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169dc072d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169ff3f5a8)]: 0 failure(s)
[16:13:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:13:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a4364668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169dc072d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169ff3f5a8)]: 0 failure(s)
[16:13:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a4364668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169dc072d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169ff3f5a8)]: 0 failure(s)
[16:13:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a4364668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169dc072d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169ff3f5a8)]: 0 failure(s)
[16:14:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a4364668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169dc072d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169ff3f5a8)]: 0 failure(s)
[16:14:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9997  0.9997  0.9996  0.9996  0.9995  0.9994  0.9993  0.9992  0.9990  0.9990  0.9989  0.9989  0.9983
[17 : 32]:	0.9979  0.9979  0.9978  0.9978  0.9977  0.9976  0.9976  0.9975  0.9973  0.9973  0.9972  0.9971  0.9970  0.9970  0.9969  0.9969
[16:14:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:14:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:14:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_multiply_add_multiply_add_nn_relu_3"
[16:14:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:14:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:14:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169fd00328)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a00f1e48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169ded2d88)]: 0 failure(s)
[16:14:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:14:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169fd00328)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a00f1e48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169ded2d88)]: 0 failure(s)
[16:14:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169fd00328)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a00f1e48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169ded2d88)]: 0 failure(s)
[16:15:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169fd00328)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a00f1e48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169ded2d88)]: 0 failure(s)
[16:15:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169fd00328)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a00f1e48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169ded2d88)]: 0 failure(s)
[16:15:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.8282  0.4592  0.3960  0.3267
[16:15:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[16:15:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[16:15:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[16:15:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[16:15:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
[16:15:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:15:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:15:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00fba38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169ff456b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a3c74078)]: 0 failure(s)
[16:15:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:15:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00fba38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169ff456b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a3c74078)]: 0 failure(s)
[16:16:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00fba38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169ff456b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a3c74078)]: 0 failure(s)
[16:16:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00fba38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169ff456b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a3c74078)]: 0 failure(s)
[16:16:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00fba38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169ff456b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a3c74078)]: 0 failure(s)
[16:16:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9997  0.9995  0.9995  0.9994  0.9993  0.9992  0.9992  0.9991  0.9987  0.9987  0.9986  0.9984  0.9983  0.9979
[17 : 32]:	0.9979  0.9979  0.9974  0.9973  0.9971  0.9970  0.9970  0.9969  0.9967  0.9967  0.9966  0.9966  0.9964  0.9964  0.9962  0.9961
[16:16:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:16:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:16:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:16:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:16:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_multiply_add_multiply_add_nn_relu_4"
[16:16:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:16:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:17:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff4c168)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169debbdc8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a0075b68)]: 0 failure(s)
[16:17:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:17:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff4c168)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169debbdc8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a0075b68)]: 0 failure(s)
[16:17:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff4c168)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169debbdc8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a0075b68)]: 0 failure(s)
[16:17:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff4c168)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169debbdc8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a0075b68)]: 0 failure(s)
[16:17:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff4c168)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169debbdc8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a0075b68)]: 0 failure(s)
[16:17:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.9594  0.6886  0.2151  0.1439
[16:17:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[16:17:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[16:17:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[16:17:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[16:17:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
[16:17:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:17:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:18:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff43c38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169ff54dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169dc01968)]: 0 failure(s)
[16:18:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:18:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff43c38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169ff54dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169dc01968)]: 0 failure(s)
[16:18:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff43c38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169ff54dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169dc01968)]: 0 failure(s)
[16:19:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff43c38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169ff54dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169dc01968)]: 0 failure(s)
[16:19:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff43c38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169ff54dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169dc01968)]: 0 failure(s)
[16:19:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9997  0.9996  0.9994  0.9993  0.9991  0.9988  0.9986  0.9985  0.9984  0.9984  0.9984  0.9981  0.9981  0.9980  0.9977
[17 : 32]:	0.9977  0.9976  0.9973  0.9973  0.9972  0.9972  0.9971  0.9970  0.9968  0.9967  0.9966  0.9966  0.9965  0.9964  0.9964  0.9961
[16:19:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:19:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:19:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:19:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:20:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_multiply_add_multiply_add_nn_relu_5"
[16:20:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:20:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:20:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff407b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169e0f3418)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a00c1fe8)]: 0 failure(s)
[16:20:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:20:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff407b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169e0f3418)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a00c1fe8)]: 0 failure(s)
[16:20:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff407b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169e0f3418)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a00c1fe8)]: 0 failure(s)
[16:20:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff407b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169e0f3418)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a00c1fe8)]: 0 failure(s)
[16:20:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56169ff407b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169e0f3418)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x5616a00c1fe8)]: 0 failure(s)
[16:21:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.8685  0.7118  0.4407  0.1704
[16:21:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[16:21:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[16:21:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[16:21:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[16:21:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
[16:21:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:21:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:21:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00e0e38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169dec8c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169f01e198)]: 0 failure(s)
[16:21:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:21:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00e0e38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169dec8c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169f01e198)]: 0 failure(s)
[16:21:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00e0e38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169dec8c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169f01e198)]: 0 failure(s)
[16:22:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00e0e38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169dec8c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169f01e198)]: 0 failure(s)
[16:22:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a00e0e38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x56169dec8c48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169f01e198)]: 0 failure(s)
[16:22:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9998  0.9997  0.9997  0.9994  0.9993  0.9993  0.9993  0.9992  0.9989  0.9989  0.9988  0.9987  0.9987  0.9986
[17 : 32]:	0.9986  0.9984  0.9982  0.9981  0.9981  0.9980  0.9979  0.9976  0.9976  0.9976  0.9975  0.9974  0.9973  0.9973  0.9968  0.9967
[16:22:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:22:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:22:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:22:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:23:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_multiply_add_multiply_add_nn_relu_6"
[16:23:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:23:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:23:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0b348a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a0fcfb48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169f0c16d8)]: 0 failure(s)
[16:23:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:23:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0b348a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a0fcfb48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169f0c16d8)]: 0 failure(s)
[16:23:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0b348a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a0fcfb48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169f0c16d8)]: 0 failure(s)
[16:23:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0b348a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a0fcfb48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169f0c16d8)]: 0 failure(s)
[16:23:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0b348a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a0fcfb48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169f0c16d8)]: 0 failure(s)
[16:23:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.9824  0.6468  0.4628  0.3737
[16:23:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[16:23:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[16:23:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[16:23:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[16:23:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
[16:23:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:23:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:24:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0670778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a03359e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169fe02278)]: 0 failure(s)
[16:24:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:24:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0670778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a03359e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169fe02278)]: 0 failure(s)
[16:24:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0670778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a03359e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169fe02278)]: 0 failure(s)
[16:24:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0670778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a03359e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169fe02278)]: 0 failure(s)
[16:24:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0670778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a03359e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169fe02278)]: 0 failure(s)
[16:25:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9996  0.9995  0.9995  0.9994  0.9993  0.9992  0.9991  0.9990  0.9990  0.9990  0.9990  0.9987  0.9986  0.9985  0.9983
[17 : 32]:	0.9981  0.9980  0.9980  0.9980  0.9979  0.9973  0.9972  0.9971  0.9971  0.9970  0.9969  0.9968  0.9968  0.9967  0.9966  0.9965
[16:25:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:25:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:25:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:25:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:25:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_multiply_add_multiply_add_nn_relu_7"
[16:25:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:25:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:25:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0327788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a08fa418)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169e7d8878)]: 0 failure(s)
[16:25:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:25:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0327788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a08fa418)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169e7d8878)]: 0 failure(s)
[16:25:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0327788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a08fa418)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169e7d8878)]: 0 failure(s)
[16:25:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0327788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a08fa418)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169e7d8878)]: 0 failure(s)
[16:25:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5616a0327788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x5616a08fa418)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x56169e7d8878)]: 0 failure(s)
[16:25:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.3599  0.3073  0.1722  0.1405
[16:25:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[16:25:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[16:25:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[16:25:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[16:25:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_multiply_add_nn_relu"] Trial #0: GFLOPs: 5.0795. Time: 0.0478 ms. Best GFLOPs: 5.0795
[16:25:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_multiply_add_nn_relu"] Trial #1: GFLOPs: 7.5514. Time: 0.0322 ms. Best GFLOPs: 7.5514
[16:25:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_multiply_add_nn_relu"] Trial #2: GFLOPs: 5.3527. Time: 0.0454 ms. Best GFLOPs: 7.5514
[16:25:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_multiply_add_nn_relu"] Trial #3: GFLOPs: 6.6775. Time: 0.0364 ms. Best GFLOPs: 7.5514
[16:25:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_multiply_add_multiply_add_nn_relu"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |            
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |            N/A |          N/A |                   N/A |      0 |            
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |            N/A |          N/A |                   N/A |      0 |            
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |            N/A |          N/A |                   N/A |      0 |            
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |            N/A |          N/A |                   N/A |      0 |            
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 4
Total latency (us): 32.1849

[16:25:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #0 has finished. Remaining task(s): 189
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #0: GFLOPs: 7.9603. Time: 1.5648 ms. Best GFLOPs: 7.9603
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #1: GFLOPs: 10.7690. Time: 1.1567 ms. Best GFLOPs: 10.7690
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #2: GFLOPs: 10.2278. Time: 1.2179 ms. Best GFLOPs: 10.7690
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #3: GFLOPs: 5.8233. Time: 2.1390 ms. Best GFLOPs: 10.7690
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #4: GFLOPs: 9.2545. Time: 1.3460 ms. Best GFLOPs: 10.7690
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #5: GFLOPs: 5.8380. Time: 2.1336 ms. Best GFLOPs: 10.7690
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #6: GFLOPs: 8.6087. Time: 1.4469 ms. Best GFLOPs: 10.7690
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #7: GFLOPs: 19.1244. Time: 0.6513 ms. Best GFLOPs: 19.1244
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #8: GFLOPs: 21.9488. Time: 0.5675 ms. Best GFLOPs: 21.9488
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #9: GFLOPs: 0.5663. Time: 21.9962 ms. Best GFLOPs: 21.9488
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #10: GFLOPs: 23.4605. Time: 0.5309 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #11: GFLOPs: 1.1143. Time: 11.1783 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #12: GFLOPs: 2.3613. Time: 5.2752 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #13: GFLOPs: 0.9805. Time: 12.7044 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #14: GFLOPs: 2.4586. Time: 5.0663 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #15: GFLOPs: 2.6528. Time: 4.6955 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #16: GFLOPs: 1.2710. Time: 9.8003 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #17: GFLOPs: 0.9854. Time: 12.6407 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #18: GFLOPs: 3.6799. Time: 3.3849 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #19: GFLOPs: 8.9236. Time: 1.3959 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #20: GFLOPs: 4.3254. Time: 2.8798 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #21: GFLOPs: 5.1436. Time: 2.4217 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #22: GFLOPs: 5.1676. Time: 2.4104 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #23: GFLOPs: 5.2629. Time: 2.3668 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #24: GFLOPs: 1.9482. Time: 6.3938 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #25: GFLOPs: 10.2248. Time: 1.2182 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #26: GFLOPs: 1.4851. Time: 8.3876 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #27: GFLOPs: 2.3012. Time: 5.4130 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #28: GFLOPs: 1.3841. Time: 8.9995 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #29: GFLOPs: 1.7367. Time: 7.1721 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #30: GFLOPs: 6.8771. Time: 1.8113 ms. Best GFLOPs: 23.4605
[16:25:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #31: GFLOPs: 7.3044. Time: 1.7053 ms. Best GFLOPs: 23.4605
[16:25:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |            
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |            N/A |          N/A |                   N/A |      0 |            
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |            N/A |          N/A |                   N/A |      0 |            
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |            N/A |          N/A |                   N/A |      0 |            
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 36
Total latency (us): 563.127

[16:25:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #1 has finished. Remaining task(s): 188
[16:25:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_multiply_add_multiply_add_nn_relu_1"] Trial #0: GFLOPs: 0.1215. Time: 1.9361 ms. Best GFLOPs: 0.1215
[16:25:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_multiply_add_multiply_add_nn_relu_1"] Trial #1: GFLOPs: 8.3621. Time: 0.0281 ms. Best GFLOPs: 8.3621
[16:25:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_multiply_add_multiply_add_nn_relu_1"] Trial #2: GFLOPs: 0.0493. Time: 4.7690 ms. Best GFLOPs: 8.3621
[16:25:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_multiply_add_multiply_add_nn_relu_1"] Trial #3: GFLOPs: 0.0504. Time: 4.6663 ms. Best GFLOPs: 8.3621
[16:26:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_multiply_add_multiply_add_nn_relu_1"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |            
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |            N/A |          N/A |                   N/A |      0 |            
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |            N/A |          N/A |                   N/A |      0 |            
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 40
Total latency (us): 591.254

[16:26:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #2 has finished. Remaining task(s): 187
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #0: GFLOPs: 21.8357. Time: 0.5521 ms. Best GFLOPs: 21.8357
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #1: GFLOPs: 18.0145. Time: 0.6692 ms. Best GFLOPs: 21.8357
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #2: GFLOPs: 3.6190. Time: 3.3310 ms. Best GFLOPs: 21.8357
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #3: GFLOPs: 4.3499. Time: 2.7713 ms. Best GFLOPs: 21.8357
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #4: GFLOPs: 7.2595. Time: 1.6605 ms. Best GFLOPs: 21.8357
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #5: GFLOPs: 31.8695. Time: 0.3783 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #6: GFLOPs: 0.8284. Time: 14.5516 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #7: GFLOPs: 17.3964. Time: 0.6929 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #8: GFLOPs: 0.7037. Time: 17.1313 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #9: GFLOPs: 4.7105. Time: 2.5591 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #10: GFLOPs: 8.0017. Time: 1.5065 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #11: GFLOPs: 9.7877. Time: 1.2316 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #12: GFLOPs: 11.7374. Time: 1.0270 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #13: GFLOPs: 0.4888. Time: 24.6636 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #14: GFLOPs: 4.0176. Time: 3.0005 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #15: GFLOPs: 8.9827. Time: 1.3420 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #16: GFLOPs: 3.5623. Time: 3.3840 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #17: GFLOPs: 2.2019. Time: 5.4747 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #18: GFLOPs: 16.6469. Time: 0.7241 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #19: GFLOPs: 1.7083. Time: 7.0567 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #20: GFLOPs: 13.2323. Time: 0.9110 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #21: GFLOPs: 4.9616. Time: 2.4296 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #22: GFLOPs: 3.8241. Time: 3.1523 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #23: GFLOPs: 1.6487. Time: 7.3117 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #24: GFLOPs: 10.4653. Time: 1.1519 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #25: GFLOPs: 17.6331. Time: 0.6836 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #26: GFLOPs: 17.1523. Time: 0.7028 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #27: GFLOPs: 7.0366. Time: 1.7132 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #28: GFLOPs: 1.5589. Time: 7.7331 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #29: GFLOPs: 5.7518. Time: 2.0958 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #30: GFLOPs: 10.8359. Time: 1.1125 ms. Best GFLOPs: 31.8695
[16:26:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #31: GFLOPs: 14.8411. Time: 0.8123 ms. Best GFLOPs: 31.8695
[16:26:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |            
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |            N/A |          N/A |                   N/A |      0 |            
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 72
Total latency (us): 969.509

[16:26:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #3 has finished. Remaining task(s): 186
[16:26:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_multiply_add_multiply_add_nn_relu_2"] Trial #0: GFLOPs: 0.0826. Time: 2.7526 ms. Best GFLOPs: 0.0826
[16:26:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_multiply_add_multiply_add_nn_relu_2"] Trial #1: GFLOPs: 0.2832. Time: 0.8028 ms. Best GFLOPs: 0.2832
[16:26:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_multiply_add_multiply_add_nn_relu_2"] Trial #2: GFLOPs: 7.9704. Time: 0.0285 ms. Best GFLOPs: 7.9704
[16:26:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_multiply_add_multiply_add_nn_relu_2"] Trial #3: GFLOPs: 9.2335. Time: 0.0246 ms. Best GFLOPs: 9.2335
[16:26:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_multiply_add_multiply_add_nn_relu_2"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |            
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 76
Total latency (us): 994.132

[16:26:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #4 has finished. Remaining task(s): 185
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #0: GFLOPs: 3.5553. Time: 3.2777 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #1: GFLOPs: 2.5119. Time: 4.6393 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #2: GFLOPs: 1.6594. Time: 7.0225 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #3: GFLOPs: 1.5852. Time: 7.3513 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #4: GFLOPs: 1.3914. Time: 8.3751 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #5: GFLOPs: 3.2821. Time: 3.5506 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #6: GFLOPs: 2.2446. Time: 5.1917 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #7: GFLOPs: 1.1017. Time: 10.5777 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #8: GFLOPs: 1.1139. Time: 10.4614 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #9: GFLOPs: 1.7070. Time: 6.8266 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #10: GFLOPs: 1.1447. Time: 10.1799 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #11: GFLOPs: 2.4818. Time: 4.6955 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #12: GFLOPs: 1.6132. Time: 7.2236 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #13: GFLOPs: 3.1333. Time: 3.7192 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #14: GFLOPs: 2.3956. Time: 4.8644 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #15: GFLOPs: 2.0679. Time: 5.6355 ms. Best GFLOPs: 3.5553
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #16: GFLOPs: 3.7292. Time: 3.1249 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #17: GFLOPs: 0.5359. Time: 21.7461 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #18: GFLOPs: 1.0752. Time: 10.8388 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #19: GFLOPs: 0.7069. Time: 16.4851 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #20: GFLOPs: 1.9432. Time: 5.9971 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #21: GFLOPs: 0.9280. Time: 12.5569 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #22: GFLOPs: 0.8621. Time: 13.5171 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #23: GFLOPs: 1.0406. Time: 11.1985 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #24: GFLOPs: 1.2626. Time: 9.2298 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #25: GFLOPs: 1.3295. Time: 8.7653 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #26: GFLOPs: 2.4148. Time: 4.8259 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #27: GFLOPs: 2.2884. Time: 5.0923 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #28: GFLOPs: 0.9712. Time: 11.9987 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #29: GFLOPs: 1.9226. Time: 6.0611 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #30: GFLOPs: 1.7216. Time: 6.7688 ms. Best GFLOPs: 3.7292
[16:26:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #31: GFLOPs: 4.7100. Time: 2.4742 ms. Best GFLOPs: 4.7100
[16:26:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |          Y 
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |         4.7100 |    2474.1573 |             2474.1573 |     32 |            
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 108
Total latency (us): 3468.29

[16:26:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #5 has finished. Remaining task(s): 184
[16:26:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_multiply_add_multiply_add_nn_relu_3"] Trial #0: GFLOPs: 1.2056. Time: 0.1821 ms. Best GFLOPs: 1.2056
[16:26:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_multiply_add_multiply_add_nn_relu_3"] Trial #1: GFLOPs: 0.3294. Time: 0.6664 ms. Best GFLOPs: 1.2056
[16:26:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_multiply_add_multiply_add_nn_relu_3"] Trial #2: GFLOPs: 7.3869. Time: 0.0297 ms. Best GFLOPs: 7.3869
[16:26:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_multiply_add_multiply_add_nn_relu_3"] Trial #3: GFLOPs: 11.3142. Time: 0.0194 ms. Best GFLOPs: 11.3142
[16:26:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_multiply_add_multiply_add_nn_relu_3"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |          Y 
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |         4.7100 |    2474.1573 |             2474.1573 |     32 |          Y 
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        11.3142 |      19.4022 |               19.4022 |      4 |            
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 112
Total latency (us): 3487.69

[16:26:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #6 has finished. Remaining task(s): 183
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #0: GFLOPs: 21.0996. Time: 0.5333 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #1: GFLOPs: 2.5171. Time: 4.4702 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #2: GFLOPs: 2.4649. Time: 4.5648 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #3: GFLOPs: 1.4066. Time: 7.9993 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #4: GFLOPs: 0.3755. Time: 29.9619 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #5: GFLOPs: 0.9486. Time: 11.8616 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #6: GFLOPs: 1.2643. Time: 8.8997 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #7: GFLOPs: 1.1594. Time: 9.7053 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #8: GFLOPs: 1.3721. Time: 8.2005 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #9: GFLOPs: 2.4871. Time: 4.5241 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #10: GFLOPs: 3.0071. Time: 3.7418 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #11: GFLOPs: 2.1464. Time: 5.2422 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #12: GFLOPs: 0.8238. Time: 13.6583 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #13: GFLOPs: 0.9600. Time: 11.7209 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #14: GFLOPs: 2.2105. Time: 5.0902 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #15: GFLOPs: 2.3569. Time: 4.7741 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #16: GFLOPs: 1.3017. Time: 8.6441 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #17: GFLOPs: 1.5233. Time: 7.3864 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #18: GFLOPs: 1.7487. Time: 6.4344 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #19: GFLOPs: 1.4070. Time: 7.9969 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #20: GFLOPs: 4.8333. Time: 2.3280 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #21: GFLOPs: 0.7790. Time: 14.4435 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #22: GFLOPs: 3.6061. Time: 3.1203 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #23: GFLOPs: 2.4109. Time: 4.6671 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #24: GFLOPs: 2.6438. Time: 4.2559 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #25: GFLOPs: 1.9533. Time: 5.7605 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #26: GFLOPs: 1.4016. Time: 8.0278 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #27: GFLOPs: 2.2739. Time: 4.9483 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #28: GFLOPs: 1.4864. Time: 7.5700 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #29: GFLOPs: 1.5972. Time: 7.0448 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #30: GFLOPs: 1.7449. Time: 6.4484 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #31: GFLOPs: 2.0921. Time: 5.3784 ms. Best GFLOPs: 21.0996
[16:26:06] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |          Y 
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |         4.7100 |    2474.1573 |             2474.1573 |     32 |          Y 
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        11.3142 |      19.4022 |               19.4022 |      4 |          Y 
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |        21.0996 |     533.2780 |              533.2780 |     32 |            
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 144
Total latency (us): 4020.97

[16:26:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #7 has finished. Remaining task(s): 182
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_multiply_add_multiply_add_nn_relu_4"] Trial #0: GFLOPs: 0.1469. Time: 1.4408 ms. Best GFLOPs: 0.1469
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_multiply_add_multiply_add_nn_relu_4"] Trial #1: GFLOPs: 0.0502. Time: 4.2176 ms. Best GFLOPs: 0.1469
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_multiply_add_multiply_add_nn_relu_4"] Trial #2: GFLOPs: 0.0759. Time: 2.7879 ms. Best GFLOPs: 0.1469
[16:26:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_multiply_add_multiply_add_nn_relu_4"] Trial #3: GFLOPs: 0.1654. Time: 1.2794 ms. Best GFLOPs: 0.1654
[16:26:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_multiply_add_multiply_add_nn_relu_4"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |          Y 
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |         4.7100 |    2474.1573 |             2474.1573 |     32 |          Y 
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        11.3142 |      19.4022 |               19.4022 |      4 |          Y 
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |        21.0996 |     533.2780 |              533.2780 |     32 |          Y 
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |         0.1654 |    1279.4411 |             1279.4411 |      4 |            
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 148
Total latency (us): 5300.41

[16:26:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #8 has finished. Remaining task(s): 181
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #0: GFLOPs: 2.2303. Time: 4.8651 ms. Best GFLOPs: 2.2303
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #1: GFLOPs: 0.9602. Time: 11.2998 ms. Best GFLOPs: 2.2303
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #2: GFLOPs: 2.0500. Time: 5.2930 ms. Best GFLOPs: 2.2303
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #3: GFLOPs: 1.6317. Time: 6.6499 ms. Best GFLOPs: 2.2303
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #4: GFLOPs: 1.0199. Time: 10.6392 ms. Best GFLOPs: 2.2303
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #5: GFLOPs: 1.2739. Time: 8.5173 ms. Best GFLOPs: 2.2303
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #6: GFLOPs: 1.2713. Time: 8.5347 ms. Best GFLOPs: 2.2303
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #7: GFLOPs: 2.1006. Time: 5.1655 ms. Best GFLOPs: 2.2303
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #8: GFLOPs: 2.6436. Time: 4.1045 ms. Best GFLOPs: 2.6436
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #9: GFLOPs: 2.2012. Time: 4.9293 ms. Best GFLOPs: 2.6436
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #10: GFLOPs: 2.2456. Time: 4.8319 ms. Best GFLOPs: 2.6436
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #11: GFLOPs: 1.8872. Time: 5.7496 ms. Best GFLOPs: 2.6436
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #12: GFLOPs: 1.7355. Time: 6.2520 ms. Best GFLOPs: 2.6436
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #13: GFLOPs: 3.5658. Time: 3.0430 ms. Best GFLOPs: 3.5658
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #14: GFLOPs: 1.8171. Time: 5.9713 ms. Best GFLOPs: 3.5658
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #15: GFLOPs: 2.4041. Time: 4.5133 ms. Best GFLOPs: 3.5658
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #16: GFLOPs: 1.6238. Time: 6.6821 ms. Best GFLOPs: 3.5658
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #17: GFLOPs: 2.2324. Time: 4.8605 ms. Best GFLOPs: 3.5658
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #18: GFLOPs: 2.4321. Time: 4.4613 ms. Best GFLOPs: 3.5658
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #19: GFLOPs: 1.1919. Time: 9.1036 ms. Best GFLOPs: 3.5658
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #20: GFLOPs: 6.5232. Time: 1.6634 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #21: GFLOPs: 1.9730. Time: 5.4995 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #22: GFLOPs: 1.1777. Time: 9.2137 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #23: GFLOPs: 3.3270. Time: 3.2614 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #24: GFLOPs: 2.3591. Time: 4.5995 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #25: GFLOPs: 1.1936. Time: 9.0905 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #26: GFLOPs: 3.7949. Time: 2.8592 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #27: GFLOPs: 1.7086. Time: 6.3507 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #28: GFLOPs: 0.8896. Time: 12.1977 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #29: GFLOPs: 1.8504. Time: 5.8640 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #30: GFLOPs: 1.3317. Time: 8.1477 ms. Best GFLOPs: 6.5232
[16:26:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #31: GFLOPs: 1.7610. Time: 6.1616 ms. Best GFLOPs: 6.5232
[16:26:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |          Y 
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |         4.7100 |    2474.1573 |             2474.1573 |     32 |          Y 
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        11.3142 |      19.4022 |               19.4022 |      4 |          Y 
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |        21.0996 |     533.2780 |              533.2780 |     32 |          Y 
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |         0.1654 |    1279.4411 |             1279.4411 |      4 |          Y 
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |         6.5232 |    1663.3680 |             1663.3680 |     32 |            
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 180
Total latency (us): 6963.78

[16:26:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #9 has finished. Remaining task(s): 180
[16:26:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_multiply_add_multiply_add_nn_relu_5"] Trial #0: GFLOPs: 0.2685. Time: 0.7591 ms. Best GFLOPs: 0.2685
[16:26:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_multiply_add_multiply_add_nn_relu_5"] Trial #1: GFLOPs: 1.7102. Time: 0.1192 ms. Best GFLOPs: 1.7102
[16:26:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_multiply_add_multiply_add_nn_relu_5"] Trial #2: GFLOPs: 7.9347. Time: 0.0257 ms. Best GFLOPs: 7.9347
[16:26:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_multiply_add_multiply_add_nn_relu_5"] Trial #3: GFLOPs: 9.5712. Time: 0.0213 ms. Best GFLOPs: 9.5712
[16:26:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_multiply_add_multiply_add_nn_relu_5"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |          Y 
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |         4.7100 |    2474.1573 |             2474.1573 |     32 |          Y 
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        11.3142 |      19.4022 |               19.4022 |      4 |          Y 
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |        21.0996 |     533.2780 |              533.2780 |     32 |          Y 
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |         0.1654 |    1279.4411 |             1279.4411 |      4 |          Y 
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |         6.5232 |    1663.3680 |             1663.3680 |     32 |          Y 
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |         9.5712 |      21.2973 |               21.2973 |      4 |            
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 184
Total latency (us): 6985.08

[16:26:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #10 has finished. Remaining task(s): 179
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #0: GFLOPs: 1.9007. Time: 5.4975 ms. Best GFLOPs: 1.9007
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #1: GFLOPs: 0.6773. Time: 15.4269 ms. Best GFLOPs: 1.9007
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #2: GFLOPs: 1.4863. Time: 7.0302 ms. Best GFLOPs: 1.9007
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #3: GFLOPs: 3.2341. Time: 3.2310 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #4: GFLOPs: 1.7673. Time: 5.9124 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #5: GFLOPs: 1.4608. Time: 7.1531 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #6: GFLOPs: 1.4915. Time: 7.0056 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #7: GFLOPs: 1.3686. Time: 7.6347 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #8: GFLOPs: 1.8465. Time: 5.6589 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #9: GFLOPs: 1.8779. Time: 5.5643 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #10: GFLOPs: 2.7736. Time: 3.7674 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #11: GFLOPs: 2.2910. Time: 4.5609 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #12: GFLOPs: 2.4736. Time: 4.2242 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #13: GFLOPs: 2.3975. Time: 4.3584 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #14: GFLOPs: 2.8331. Time: 3.6883 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #15: GFLOPs: 1.3693. Time: 7.6311 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #16: GFLOPs: 2.4382. Time: 4.2857 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #17: GFLOPs: 2.5333. Time: 4.1248 ms. Best GFLOPs: 3.2341
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #18: GFLOPs: 4.3739. Time: 2.3890 ms. Best GFLOPs: 4.3739
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #19: GFLOPs: 22.3704. Time: 0.4671 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #20: GFLOPs: 11.6055. Time: 0.9004 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #21: GFLOPs: 7.5937. Time: 1.3760 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #22: GFLOPs: 19.9458. Time: 0.5239 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #23: GFLOPs: 8.0946. Time: 1.2909 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #24: GFLOPs: 1.8662. Time: 5.5991 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #25: GFLOPs: 0.8872. Time: 11.7778 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #26: GFLOPs: 2.0419. Time: 5.1173 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #27: GFLOPs: 1.3773. Time: 7.5866 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #28: GFLOPs: 1.9793. Time: 5.2792 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #29: GFLOPs: 0.5443. Time: 19.1986 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #30: GFLOPs: 0.8452. Time: 12.3622 ms. Best GFLOPs: 22.3704
[16:26:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #31: GFLOPs: 1.0263. Time: 10.1815 ms. Best GFLOPs: 22.3704
[16:26:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |          Y 
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |         4.7100 |    2474.1573 |             2474.1573 |     32 |          Y 
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        11.3142 |      19.4022 |               19.4022 |      4 |          Y 
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |        21.0996 |     533.2780 |              533.2780 |     32 |          Y 
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |         0.1654 |    1279.4411 |             1279.4411 |      4 |          Y 
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |         6.5232 |    1663.3680 |             1663.3680 |     32 |          Y 
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |         9.5712 |      21.2973 |               21.2973 |      4 |          Y 
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |        22.3704 |     467.0982 |              467.0982 |     32 |            
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 216
Total latency (us): 7452.17

[16:26:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #11 has finished. Remaining task(s): 178
[16:26:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_multiply_add_multiply_add_nn_relu_6"] Trial #0: GFLOPs: 4.2684. Time: 0.0459 ms. Best GFLOPs: 4.2684
[16:26:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_multiply_add_multiply_add_nn_relu_6"] Trial #1: GFLOPs: 8.1335. Time: 0.0241 ms. Best GFLOPs: 8.1335
[16:26:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_multiply_add_multiply_add_nn_relu_6"] Trial #2: GFLOPs: 6.2509. Time: 0.0314 ms. Best GFLOPs: 8.1335
[16:26:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_multiply_add_multiply_add_nn_relu_6"] Trial #3: GFLOPs: 5.5324. Time: 0.0354 ms. Best GFLOPs: 8.1335
[16:26:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_multiply_add_multiply_add_nn_relu_6"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |          Y 
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |         4.7100 |    2474.1573 |             2474.1573 |     32 |          Y 
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        11.3142 |      19.4022 |               19.4022 |      4 |          Y 
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |        21.0996 |     533.2780 |              533.2780 |     32 |          Y 
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |         0.1654 |    1279.4411 |             1279.4411 |      4 |          Y 
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |         6.5232 |    1663.3680 |             1663.3680 |     32 |          Y 
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |         9.5712 |      21.2973 |               21.2973 |      4 |          Y 
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |        22.3704 |     467.0982 |              467.0982 |     32 |          Y 
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |         8.1335 |      24.0977 |               24.0977 |      4 |            
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 220
Total latency (us): 7476.27

[16:26:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #12 has finished. Remaining task(s): 177
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #0: GFLOPs: 42.5385. Time: 0.2362 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #1: GFLOPs: 5.2069. Time: 1.9297 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #2: GFLOPs: 5.7003. Time: 1.7627 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #3: GFLOPs: 16.4344. Time: 0.6114 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #4: GFLOPs: 18.2460. Time: 0.5507 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #5: GFLOPs: 13.0463. Time: 0.7702 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #6: GFLOPs: 0.4439. Time: 22.6372 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #7: GFLOPs: 21.8377. Time: 0.4601 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #8: GFLOPs: 16.2860. Time: 0.6170 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #9: GFLOPs: 11.7361. Time: 0.8561 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #10: GFLOPs: 30.5682. Time: 0.3287 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #11: GFLOPs: 11.2603. Time: 0.8923 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #12: GFLOPs: 3.5198. Time: 2.8546 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #13: GFLOPs: 37.3401. Time: 0.2691 ms. Best GFLOPs: 42.5385
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #14: GFLOPs: 44.5827. Time: 0.2254 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #15: GFLOPs: 26.8326. Time: 0.3745 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #16: GFLOPs: 1.0130. Time: 9.9188 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #17: GFLOPs: 5.6275. Time: 1.7855 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #18: GFLOPs: 30.7819. Time: 0.3264 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #19: GFLOPs: 27.5545. Time: 0.3646 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #20: GFLOPs: 12.6316. Time: 0.7954 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #21: GFLOPs: 10.5051. Time: 0.9565 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #22: GFLOPs: 22.6326. Time: 0.4440 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #23: GFLOPs: 8.8851. Time: 1.1309 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #24: GFLOPs: 6.8441. Time: 1.4681 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #25: GFLOPs: 9.1495. Time: 1.0982 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #26: GFLOPs: 4.9773. Time: 2.0187 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #27: GFLOPs: 8.8564. Time: 1.1345 ms. Best GFLOPs: 44.5827
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #28: GFLOPs: 61.7470. Time: 0.1627 ms. Best GFLOPs: 61.7470
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #29: GFLOPs: 4.4999. Time: 2.2329 ms. Best GFLOPs: 61.7470
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #30: GFLOPs: 10.5409. Time: 0.9532 ms. Best GFLOPs: 61.7470
[16:26:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #31: GFLOPs: 10.9780. Time: 0.9153 ms. Best GFLOPs: 61.7470
[16:26:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |          Y 
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |         4.7100 |    2474.1573 |             2474.1573 |     32 |          Y 
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        11.3142 |      19.4022 |               19.4022 |      4 |          Y 
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |        21.0996 |     533.2780 |              533.2780 |     32 |          Y 
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |         0.1654 |    1279.4411 |             1279.4411 |      4 |          Y 
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |         6.5232 |    1663.3680 |             1663.3680 |     32 |          Y 
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |         9.5712 |      21.2973 |               21.2973 |      4 |          Y 
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |        22.3704 |     467.0982 |              467.0982 |     32 |          Y 
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |         8.1335 |      24.0977 |               24.0977 |      4 |          Y 
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |        61.7470 |     162.7245 |              162.7245 |     32 |            
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 252
Total latency (us): 7639

[16:26:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #13 has finished. Remaining task(s): 176
[16:26:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_multiply_add_multiply_add_nn_relu_7"] Trial #0: GFLOPs: 7.7399. Time: 0.0243 ms. Best GFLOPs: 7.7399
[16:26:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_multiply_add_multiply_add_nn_relu_7"] Trial #1: GFLOPs: 12.7997. Time: 0.0147 ms. Best GFLOPs: 12.7997
[16:26:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_multiply_add_multiply_add_nn_relu_7"] Trial #2: GFLOPs: 7.5574. Time: 0.0249 ms. Best GFLOPs: 12.7997
[16:26:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_multiply_add_multiply_add_nn_relu_7"] Trial #3: GFLOPs: 8.4728. Time: 0.0222 ms. Best GFLOPs: 12.7997
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_multiply_add_multiply_add_nn_relu_7"
  ID |                                                     Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |                  fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |         7.5514 |      32.1849 |               32.1849 |      4 |          Y 
   1 |                fused_nn_contrib_conv2d_NCHWc_add_nn_relu |  12456192 |      1 |        23.4605 |     530.9423 |              530.9423 |     32 |          Y 
   2 |                fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |         8.3621 |      28.1269 |               28.1269 |      4 |          Y 
   3 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 |  12054784 |      1 |        31.8695 |     378.2547 |              378.2547 |     32 |          Y 
   4 |                fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |         9.2335 |      24.6233 |               24.6233 |      4 |          Y 
   5 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 |  11653376 |      1 |         4.7100 |    2474.1573 |             2474.1573 |     32 |          Y 
   6 |                fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        11.3142 |      19.4022 |               19.4022 |      4 |          Y 
   7 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 |  11251968 |      1 |        21.0996 |     533.2780 |              533.2780 |     32 |          Y 
   8 |                fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |         0.1654 |    1279.4411 |             1279.4411 |      4 |          Y 
   9 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  10850560 |      1 |         6.5232 |    1663.3680 |             1663.3680 |     32 |          Y 
  10 |                fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |         9.5712 |      21.2973 |               21.2973 |      4 |          Y 
  11 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 |  10449152 |      1 |        22.3704 |     467.0982 |              467.0982 |     32 |          Y 
  12 |                fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |         8.1335 |      24.0977 |               24.0977 |      4 |          Y 
  13 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 |  10047744 |      1 |        61.7470 |     162.7245 |              162.7245 |     32 |          Y 
  14 |                fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |        12.7997 |      14.7003 |               14.7003 |      4 |            
  15 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |                fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |                fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |   fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |               fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |              fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |               fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |               fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |               fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                               fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                             fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                            fused_nn_contrib_conv2d_NCHWc |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |               fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |               fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |               fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                             fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |               fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                             fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |               fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                             fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |               fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |               fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                             fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |               fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |               fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                             fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |               fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                             fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                             fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |               fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                             fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |               fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                            fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                            fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                            fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                            fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                          fused_nn_contrib_conv2d_NCHWc_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |               fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |               fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                            fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                            fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                            fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                            fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |               fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |               fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                            fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |               fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                            fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                            fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                          fused_nn_contrib_conv2d_NCHWc_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |               fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |               fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |               fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                            fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |               fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                            fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                          fused_nn_contrib_conv2d_NCHWc_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |                                   fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 | fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                      fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                        fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                      fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                      fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                      fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |                                      fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |      fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                          fused_nn_contrib_conv2d_NCHWc_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                      fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                      fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                      fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                      fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                      fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                      fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                     fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                     fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                     fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                     fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                     fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 |                                     fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |    fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                          fused_nn_contrib_conv2d_NCHWc_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                    fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                     fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                     fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                     fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                     fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                     fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                     fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                     fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                     fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                     fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                     fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                     fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                     fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                     fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                     fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                     fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                     fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                     fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                     fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                     fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                     fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                     fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                     fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                                     fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                   fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                          fused_nn_contrib_conv2d_NCHWc_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                    fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                     fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                     fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                     fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                     fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                     fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                     fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                     fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                     fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                     fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                     fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                     fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                     fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                     fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                     fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 |                                     fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |    fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                               fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 188 |                        fused_nn_contrib_conv2d_NCHWc_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 189 |                                 fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 256
Total latency (us): 7653.7

[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #14 has finished. Remaining task(s): 175
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #15 has finished. Remaining task(s): 174
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #16 has finished. Remaining task(s): 173
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #17 has finished. Remaining task(s): 172
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #18 has finished. Remaining task(s): 171
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #19 has finished. Remaining task(s): 170
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #20 has finished. Remaining task(s): 169
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #21 has finished. Remaining task(s): 168
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #22 has finished. Remaining task(s): 167
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #23 has finished. Remaining task(s): 166
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #24 has finished. Remaining task(s): 165
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #25 has finished. Remaining task(s): 164
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #26 has finished. Remaining task(s): 163
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #27 has finished. Remaining task(s): 162
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #28 has finished. Remaining task(s): 161
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #29 has finished. Remaining task(s): 160
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #30 has finished. Remaining task(s): 159
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #31 has finished. Remaining task(s): 158
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #32 has finished. Remaining task(s): 157
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #33 has finished. Remaining task(s): 156
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #34 has finished. Remaining task(s): 155
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #35 has finished. Remaining task(s): 154
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #36 has finished. Remaining task(s): 153
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #37 has finished. Remaining task(s): 152
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #38 has finished. Remaining task(s): 151
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #39 has finished. Remaining task(s): 150
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #40 has finished. Remaining task(s): 149
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #41 has finished. Remaining task(s): 148
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #42 has finished. Remaining task(s): 147
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #43 has finished. Remaining task(s): 146
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #44 has finished. Remaining task(s): 145
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #45 has finished. Remaining task(s): 144
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #46 has finished. Remaining task(s): 143
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #47 has finished. Remaining task(s): 142
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #48 has finished. Remaining task(s): 141
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #49 has finished. Remaining task(s): 140
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #50 has finished. Remaining task(s): 139
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #51 has finished. Remaining task(s): 138
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #52 has finished. Remaining task(s): 137
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #53 has finished. Remaining task(s): 136
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #54 has finished. Remaining task(s): 135
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #55 has finished. Remaining task(s): 134
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #56 has finished. Remaining task(s): 133
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #57 has finished. Remaining task(s): 132
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #58 has finished. Remaining task(s): 131
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #59 has finished. Remaining task(s): 130
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #60 has finished. Remaining task(s): 129
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #61 has finished. Remaining task(s): 128
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #62 has finished. Remaining task(s): 127
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #63 has finished. Remaining task(s): 126
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #64 has finished. Remaining task(s): 125
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #65 has finished. Remaining task(s): 124
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #66 has finished. Remaining task(s): 123
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #67 has finished. Remaining task(s): 122
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #68 has finished. Remaining task(s): 121
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #69 has finished. Remaining task(s): 120
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #70 has finished. Remaining task(s): 119
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #71 has finished. Remaining task(s): 118
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #72 has finished. Remaining task(s): 117
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #73 has finished. Remaining task(s): 116
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #74 has finished. Remaining task(s): 115
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #75 has finished. Remaining task(s): 114
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #76 has finished. Remaining task(s): 113
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #77 has finished. Remaining task(s): 112
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #78 has finished. Remaining task(s): 111
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #79 has finished. Remaining task(s): 110
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #80 has finished. Remaining task(s): 109
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #81 has finished. Remaining task(s): 108
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #82 has finished. Remaining task(s): 107
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #83 has finished. Remaining task(s): 106
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #84 has finished. Remaining task(s): 105
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #85 has finished. Remaining task(s): 104
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #86 has finished. Remaining task(s): 103
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #87 has finished. Remaining task(s): 102
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #88 has finished. Remaining task(s): 101
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #89 has finished. Remaining task(s): 100
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #90 has finished. Remaining task(s): 99
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #91 has finished. Remaining task(s): 98
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #92 has finished. Remaining task(s): 97
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #93 has finished. Remaining task(s): 96
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #94 has finished. Remaining task(s): 95
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #95 has finished. Remaining task(s): 94
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #96 has finished. Remaining task(s): 93
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #97 has finished. Remaining task(s): 92
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #98 has finished. Remaining task(s): 91
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #99 has finished. Remaining task(s): 90
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #100 has finished. Remaining task(s): 89
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #101 has finished. Remaining task(s): 88
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #102 has finished. Remaining task(s): 87
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #103 has finished. Remaining task(s): 86
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #104 has finished. Remaining task(s): 85
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #105 has finished. Remaining task(s): 84
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #106 has finished. Remaining task(s): 83
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #107 has finished. Remaining task(s): 82
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #108 has finished. Remaining task(s): 81
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #109 has finished. Remaining task(s): 80
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #110 has finished. Remaining task(s): 79
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #111 has finished. Remaining task(s): 78
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #112 has finished. Remaining task(s): 77
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #113 has finished. Remaining task(s): 76
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #114 has finished. Remaining task(s): 75
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #115 has finished. Remaining task(s): 74
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #116 has finished. Remaining task(s): 73
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #117 has finished. Remaining task(s): 72
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #118 has finished. Remaining task(s): 71
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #119 has finished. Remaining task(s): 70
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #120 has finished. Remaining task(s): 69
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #121 has finished. Remaining task(s): 68
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #122 has finished. Remaining task(s): 67
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #123 has finished. Remaining task(s): 66
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #124 has finished. Remaining task(s): 65
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #125 has finished. Remaining task(s): 64
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #126 has finished. Remaining task(s): 63
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #127 has finished. Remaining task(s): 62
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #128 has finished. Remaining task(s): 61
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #129 has finished. Remaining task(s): 60
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #130 has finished. Remaining task(s): 59
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #131 has finished. Remaining task(s): 58
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #132 has finished. Remaining task(s): 57
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #133 has finished. Remaining task(s): 56
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #134 has finished. Remaining task(s): 55
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #135 has finished. Remaining task(s): 54
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #136 has finished. Remaining task(s): 53
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #137 has finished. Remaining task(s): 52
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #138 has finished. Remaining task(s): 51
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #139 has finished. Remaining task(s): 50
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #140 has finished. Remaining task(s): 49
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #141 has finished. Remaining task(s): 48
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #142 has finished. Remaining task(s): 47
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #143 has finished. Remaining task(s): 46
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #144 has finished. Remaining task(s): 45
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #145 has finished. Remaining task(s): 44
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #146 has finished. Remaining task(s): 43
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #147 has finished. Remaining task(s): 42
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #148 has finished. Remaining task(s): 41
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #149 has finished. Remaining task(s): 40
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #150 has finished. Remaining task(s): 39
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #151 has finished. Remaining task(s): 38
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #152 has finished. Remaining task(s): 37
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #153 has finished. Remaining task(s): 36
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #154 has finished. Remaining task(s): 35
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #155 has finished. Remaining task(s): 34
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #156 has finished. Remaining task(s): 33
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #157 has finished. Remaining task(s): 32
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #158 has finished. Remaining task(s): 31
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #159 has finished. Remaining task(s): 30
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #160 has finished. Remaining task(s): 29
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #161 has finished. Remaining task(s): 28
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #162 has finished. Remaining task(s): 27
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #163 has finished. Remaining task(s): 26
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #164 has finished. Remaining task(s): 25
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #165 has finished. Remaining task(s): 24
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #166 has finished. Remaining task(s): 23
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #167 has finished. Remaining task(s): 22
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #168 has finished. Remaining task(s): 21
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #169 has finished. Remaining task(s): 20
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #170 has finished. Remaining task(s): 19
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #171 has finished. Remaining task(s): 18
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #172 has finished. Remaining task(s): 17
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #173 has finished. Remaining task(s): 16
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #174 has finished. Remaining task(s): 15
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #175 has finished. Remaining task(s): 14
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #176 has finished. Remaining task(s): 13
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #177 has finished. Remaining task(s): 12
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #178 has finished. Remaining task(s): 11
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #179 has finished. Remaining task(s): 10
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #180 has finished. Remaining task(s): 9
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #181 has finished. Remaining task(s): 8
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #182 has finished. Remaining task(s): 7
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #183 has finished. Remaining task(s): 6
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #184 has finished. Remaining task(s): 5
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #185 has finished. Remaining task(s): 4
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #186 has finished. Remaining task(s): 3
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #187 has finished. Remaining task(s): 2
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #188 has finished. Remaining task(s): 1
[16:26:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #189 has finished. Remaining task(s): 0
[16:29:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_layout_transform
[16:29:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu
[16:29:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_max_pool2d
[16:29:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_layout_transform
[16:29:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu
[16:29:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc
[16:29:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_layout_transform_concatenate
[16:29:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_layout_transform
[16:29:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1
[16:29:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_concatenate
[16:29:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_layout_transform_1
[16:29:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2
[16:29:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_concatenate_1
[16:29:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_layout_transform_1
[16:29:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3
[16:29:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_concatenate_2
[16:29:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_layout_transform_2
[16:29:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4
[16:29:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_concatenate_3
[16:29:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_layout_transform_3
[16:29:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5
[16:29:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_concatenate_multiply_add_multiply_add_nn_relu_layout_tran_4cfb7ee92b06409c_
[16:29:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_1
[16:29:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_avg_pool2d
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_2
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_1
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_1
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_2
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_2
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_9
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_3
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_1
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_10
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_4
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_2
[16:29:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_11
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_5
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_3
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_12
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_6
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_4
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_13
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_7
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_5
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_8
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_6
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_15
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_9
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_3
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_16
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_10
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_4
[16:29:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_17
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_multiply_add_multiply_add_nn_relu
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_3
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_avg_pool2d_1
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_7
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_18
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_4
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_11
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_8
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_19
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_12
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_9
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_20
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_13
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_10
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_21
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_14
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_5
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_22
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_15
[16:29:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_11
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_23
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_16
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_6
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_24
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_17
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_12
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_25
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_18
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_13
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_26
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_19
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_7
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_27
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_20
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_14
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_28
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_21
[16:29:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_8
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_29
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_22
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_9
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_30
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_23
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_15
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_31
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_24
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_10
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_32
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_25
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_11
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_33
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_26
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_16
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_34
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_27
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_12
[16:29:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_35
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_28
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_17
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_36
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_29
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_13
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_37
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_30
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_18
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_38
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_31
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_14
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_39
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_32
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_15
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_40
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_33
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_16
[16:29:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_41
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_multiply_add_nn_relu
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_5
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_avg_pool2d_2
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_19
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_42
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_6
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_34
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_20
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_43
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_35
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_17
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_44
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_36
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_18
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_45
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_37
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_19
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_46
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_38
[16:29:30] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_20
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_47
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_39
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_21
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_multiply_add_nn_relu_1
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_40
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_22
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_48
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_41
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 192, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 192, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 192, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 192, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 192, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 192, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_23(placeholder: T.Buffer[(1, 192, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 192, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 192, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 192, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(192, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b3)", "sch.compute_inline(block=b2)", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.unroll_explicit\")", "b7, = sch.get_child_blocks(b6)", "l8, l9, l10, l11, l12 = sch.get_loops(block=b7)", "l13 = sch.fuse(l8, l9)", "sch.parallel(loop=l13)", "l14 = sch.fuse(l11, l12)", "sch.vectorize(loop=l14)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_49
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_42
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 200, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 200, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 200, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 200, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 200, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 200, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 200, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_24(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 200, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 200, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 200, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(200, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b3)", "sch.compute_inline(block=b2)", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.unroll_explicit\")", "b7, = sch.get_child_blocks(b6)", "l8, l9, l10, l11, l12 = sch.get_loops(block=b7)", "l13 = sch.fuse(l8, l9)", "sch.parallel(loop=l13)", "l14 = sch.fuse(l11, l12)", "sch.vectorize(loop=l14)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 200, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 800, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 7, 7, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_50(placeholder: T.Buffer[(1, 200, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 200, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                for i2_3_init in T.serial(7):
                    for i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16)
                            oh = T.axis.spatial(7, i2_3_init)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16)
                            oc_block = T.axis.spatial(4, i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 7, 7, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(100, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 7):
                    for i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16)
                            oh = T.axis.spatial(7, i2_3)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 16)
                            oc_block = T.axis.spatial(4, i3_3_i4_3_fused)
                            ic = T.axis.reduce(800, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 200, 7, 7, 4], "float32"], ["TENSOR", [32, 200, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(28):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(32, i0_i1_i2_fused // 7)
                    ax2 = T.axis.spatial(7, i0_i1_i2_fused % 7)
                    ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 1, 1])", "l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])", "l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])", "l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])", "l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[100, 8])", "l53, l54 = sch.split(loop=l8, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l57, l58 = sch.split(loop=l9, factors=[v55, v56])", "v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])", "l61, l62 = sch.split(loop=l10, factors=[v59, v60])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v63)", "sch.enter_postproc()", "b64 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.unroll_explicit\")", "b65, b66 = sch.get_child_blocks(b64)", "l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)", "l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73)", "sch.parallel(loop=l93)", "l94 = sch.fuse(l91, l92)", "sch.vectorize(loop=l94)", "sch.annotate(block_or_loop=l93, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l93, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l95, l96, l97, l98, l99 = sch.get_loops(block=b66)", "l100 = sch.fuse(l95, l96, l97)", "sch.parallel(loop=l100)", "l101 = sch.fuse(l98, l99)", "sch.vectorize(loop=l101)", "sch.annotate(block_or_loop=l100, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l100, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b102 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b102)", "b122 = sch.decompose_reduction(block=b102, loop=l107)"]
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_43
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 208, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 208, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 208, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 208, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 208, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 208, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 208, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_25(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 208, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 208, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 208, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(208, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:31] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b3)", "sch.compute_inline(block=b2)", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.unroll_explicit\")", "b7, = sch.get_child_blocks(b6)", "l8, l9, l10, l11, l12 = sch.get_loops(block=b7)", "l13 = sch.fuse(l8, l9)", "sch.parallel(loop=l13)", "l14 = sch.fuse(l11, l12)", "sch.vectorize(loop=l14)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 208, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 832, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 7, 7, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_51(placeholder: T.Buffer[(1, 208, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 208, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(2, 7, 4, 7):
                for i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 8 + i1_2_init * 4 + i1_3_init)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2_init, i3_3_init, i4_3_fused_init])
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 7, 7, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(13, 1, 1, 1, 2, 7, 1, 1, 64, 1, 1, 1, 4, 1, 7):
                for i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 8 + i1_2 * 4 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_3, i4_3_fused])
                        ic = T.axis.reduce(832, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 208, 7, 7, 4], "float32"], ["TENSOR", [32, 208, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 8, 7):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 8 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])", "l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])", "l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])", "l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])", "l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[13, 64])", "l53, l54 = sch.split(loop=l8, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l57, l58 = sch.split(loop=l9, factors=[v55, v56])", "v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])", "l61, l62 = sch.split(loop=l10, factors=[v59, v60])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)", "b63, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v64)", "sch.enter_postproc()", "b65 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.unroll_explicit\")", "b66, b67 = sch.get_child_blocks(b65)", "l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)", "l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)", "sch.parallel(loop=l94)", "l95 = sch.fuse(l93)", "sch.vectorize(loop=l95)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)", "l102 = sch.fuse(l100, l101)", "sch.vectorize(loop=l102)", "sch.annotate(block_or_loop=l96, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l96, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b103 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b103)", "b121 = sch.decompose_reduction(block=b103, loop=l105)"]
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_44
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 216, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 216, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 216, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 216, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 216, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 216, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 216, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_26(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 216, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 216, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 216, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(216, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b3)", "sch.compute_inline(block=b2)", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.unroll_explicit\")", "b7, = sch.get_child_blocks(b6)", "l8, l9, l10, l11, l12 = sch.get_loops(block=b7)", "l13 = sch.fuse(l8, l9)", "sch.parallel(loop=l13)", "l14 = sch.fuse(l11, l12)", "sch.vectorize(loop=l14)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 216, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 864, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 7, 7, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_52(placeholder: T.Buffer[(1, 216, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 216, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(7, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_2_init in T.grid(32, 7):
                for i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk, oh, ow = T.axis.remap("SSS", [i1_2_init, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused, i3_2_init])
                        oc_block = T.axis.spatial(4, i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init // 2 * 2 + i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init % 2)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 7, 7, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2 in T.grid(864, 1, 1, 1, 32, 1, 7):
                for i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk, oh, ow = T.axis.remap("SSS", [i1_2, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused, i3_2])
                        oc_block = T.axis.spatial(4, i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused // 2 * 2 + i4_2_i5_1_i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused % 2)
                        ic = T.axis.reduce(864, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 216, 7, 7, 4], "float32"], ["TENSOR", [32, 216, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1 in T.grid(1, 32):
                for ax2_ax3_ax4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2 = T.axis.remap("SS", [ax1, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused])
                        ax3 = T.axis.spatial(7, ax2_ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax2_ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])", "l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])", "l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])", "l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])", "l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[864, 1])", "l53, l54 = sch.split(loop=l8, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l57, l58 = sch.split(loop=l9, factors=[v55, v56])", "v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])", "l61, l62 = sch.split(loop=l10, factors=[v59, v60])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)", "b63, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v64)", "sch.enter_postproc()", "b65 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.unroll_explicit\")", "b66, b67 = sch.get_child_blocks(b65)", "l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)", "l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)", "sch.parallel(loop=l94)", "l95 = sch.fuse(l85, l86, l87, l88, l89, l90, l91, l92, l93)", "sch.vectorize(loop=l95)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)", "l102 = sch.fuse(l99, l100, l101)", "sch.vectorize(loop=l102)", "sch.annotate(block_or_loop=l96, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l96, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b103 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b103)", "b113 = sch.decompose_reduction(block=b103, loop=l105)"]
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_45
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 224, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 224, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 224, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 224, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 224, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 224, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 224, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_27(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 224, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 224, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 224, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b3)", "sch.compute_inline(block=b2)", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.unroll_explicit\")", "b7, = sch.get_child_blocks(b6)", "l8, l9, l10, l11, l12 = sch.get_loops(block=b7)", "l13 = sch.fuse(l8, l9)", "sch.parallel(loop=l13)", "l14 = sch.fuse(l11, l12)", "sch.vectorize(loop=l14)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 224, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 896, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 7, 7, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_53(placeholder: T.Buffer[(1, 224, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 224, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_3_init, i2_3_init in T.grid(4, 7):
                for i3_3_i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 7 * 4 + i1_3_init)
                        oh = T.axis.spatial(7, i2_3_init)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                        oc_block = T.axis.spatial(4, i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 7, 7, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(32, 1, 1, 1, 1, 1, 1, 1, 28, 1, 1, 1, 4, 7):
                for i3_3_i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 7 * 4 + i1_3)
                        oh = T.axis.spatial(7, i2_3)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                        oc_block = T.axis.spatial(4, i3_3_i4_3_fused)
                        ic = T.axis.reduce(896, i5_0 * 28 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 224, 7, 7, 4], "float32"], ["TENSOR", [32, 224, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 4, 7):
                for ax3_ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 7 * 4 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])", "l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])", "l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])", "l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])", "l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 28])", "l53, l54 = sch.split(loop=l8, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l57, l58 = sch.split(loop=l9, factors=[v55, v56])", "v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])", "l61, l62 = sch.split(loop=l10, factors=[v59, v60])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)", "b63, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v64)", "sch.enter_postproc()", "b65 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.unroll_explicit\")", "b66, b67 = sch.get_child_blocks(b65)", "l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)", "l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)", "sch.parallel(loop=l94)", "l95 = sch.fuse(l92, l93)", "sch.vectorize(loop=l95)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)", "l102 = sch.fuse(l100, l101)", "sch.vectorize(loop=l102)", "sch.annotate(block_or_loop=l96, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l96, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b103 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b103)", "b120 = sch.decompose_reduction(block=b103, loop=l105)"]
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_46
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 232, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 232, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 232, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 232, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 232, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 232, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 232, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_28(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 232, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 232, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 232, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(232, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b3)", "sch.compute_inline(block=b2)", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.unroll_explicit\")", "b7, = sch.get_child_blocks(b6)", "l8, l9, l10, l11, l12 = sch.get_loops(block=b7)", "l13 = sch.fuse(l8, l9)", "sch.parallel(loop=l13)", "l14 = sch.fuse(l11, l12)", "sch.vectorize(loop=l14)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 232, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 928, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 7, 7, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_54(placeholder: T.Buffer[(1, 232, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 232, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0_0 in T.serial(1, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 7, 1, 2):
                    for i1_2_init, i1_3_init, i3_3_init in T.grid(2, 2, 7):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2_init * 2 + i1_3_init)
                                oh, ow = T.axis.remap("SS", [i2_1, i3_3_init])
                                oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 7, 7, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(29, 1, 1, 1, 2, 1, 1, 1, 32, 1, 1, 1, 2, 1, 7):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_2 * 2 + i1_3)
                                oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                                oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                                ic = T.axis.reduce(928, i5_0 * 32 + i5_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 232, 7, 7, 4], "float32"], ["TENSOR", [32, 232, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 32, 7):
                    for ax3_ax4_fused in T.vectorized(28):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                            ax3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 2, 2])", "l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])", "l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])", "l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])", "l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[29, 32])", "l53, l54 = sch.split(loop=l8, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l57, l58 = sch.split(loop=l9, factors=[v55, v56])", "v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])", "l61, l62 = sch.split(loop=l10, factors=[v59, v60])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)", "b63, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v64)", "sch.enter_postproc()", "b65 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.unroll_explicit\")", "b66, b67 = sch.get_child_blocks(b65)", "l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)", "l94 = sch.fuse(l93)", "sch.vectorize(loop=l94)", "sch.annotate(block_or_loop=l68, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l68, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b67)", "l105 = sch.fuse(l103, l104)", "sch.vectorize(loop=l105)", "sch.annotate(block_or_loop=l95, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l95, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b106 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b106)", "b133 = sch.decompose_reduction(block=b106, loop=l117)"]
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_47
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_29(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 240, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(240, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b3)", "sch.compute_inline(block=b2)", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.unroll_explicit\")", "b7, = sch.get_child_blocks(b6)", "l8, l9, l10, l11, l12 = sch.get_loops(block=b7)", "l13 = sch.fuse(l8, l9)", "sch.parallel(loop=l13)", "l14 = sch.fuse(l11, l12)", "sch.vectorize(loop=l14)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 960, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_55(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused in T.parallel(784):
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                for i1_2_init, i4_2_init in T.grid(2, 2):
                    for i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused // 392 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused % 8 * 2 + i1_2_init)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused % 392 // 56)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused % 56 // 8)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(192, 1, 1, 1, 2, 1, 1, 2, 5):
                    for i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused // 392 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused % 8 * 2 + i1_2)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused % 392 // 56)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused % 56 // 8)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i6_1_i7_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(960, i5_0 * 5 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [32, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1 in T.grid(1, 2):
                    for ax2_ax3_ax4_fused in T.vectorized(4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused // 392 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused % 8 * 2 + ax1)
                            ax2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused % 392 // 56)
                            ax3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused_fused % 56 // 8)
                            ax4 = T.axis.spatial(4, ax2_ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 2, 1])", "l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])", "l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])", "l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])", "l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[192, 5])", "l53, l54 = sch.split(loop=l8, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l57, l58 = sch.split(loop=l9, factors=[v55, v56])", "v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])", "l61, l62 = sch.split(loop=l10, factors=[v59, v60])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)", "b63, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v64)", "sch.enter_postproc()", "b65 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.unroll_explicit\")", "b66, b67 = sch.get_child_blocks(b65)", "l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)", "l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74)", "sch.parallel(loop=l94)", "l95 = sch.fuse(l87, l88, l89, l90, l91, l92, l93)", "sch.vectorize(loop=l95)", "l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b67)", "l105 = sch.fuse(l96)", "sch.parallel(loop=l105)", "l106 = sch.fuse(l102, l103, l104)", "sch.vectorize(loop=l106)", "b107 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b107)", "b122 = sch.decompose_reduction(block=b107, loop=l112)"]
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_48
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 248, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 248, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 248, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 248, 7, 7, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 248, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 248, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] * placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_multiply_1[ax0, ax1, ax2, ax3, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 248, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_30(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 248, 1, 1, 4), "float32"], placeholder_4: T.Buffer[(1, 248, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 248, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(248, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max((placeholder[ax0, ax1, ax2, ax3, ax4] * placeholder_1[ax0, ax1, 0, 0, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * placeholder_3[ax0, ax1, 0, 0, ax4] + placeholder_4[ax0, ax1, 0, 0, ax4], T.float32(0))
    

[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b3)", "sch.compute_inline(block=b2)", "sch.compute_inline(block=b1)", "sch.compute_inline(block=b0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b6, ann_key=\"meta_schedule.unroll_explicit\")", "b7, = sch.get_child_blocks(b6)", "l8, l9, l10, l11, l12 = sch.get_loops(block=b7)", "l13 = sch.fuse(l8, l9)", "sch.parallel(loop=l13)", "l14 = sch.fuse(l11, l12)", "sch.vectorize(loop=l14)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l13, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 248, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 7, 7, 4, 992, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 7, 7, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_nn_relu_56(placeholder: T.Buffer[(1, 248, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(32, 248, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 32, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 7, 7, 4], dtype="float32")
        for i0_0 in T.serial(1, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 1, 1):
                    for i3_2_init, i1_3_init, i2_3_init in T.grid(7, 4, 7):
                        for i3_3_i4_3_fused_init in T.vectorized(4):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_3_init)
                                oh, ow, oc_block = T.axis.remap("SSS", [i2_3_init, i3_2_init, i3_3_i4_3_fused_init])
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 7, 7, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(124, 1, 1, 1, 1, 1, 7, 1, 8, 1, 1, 1, 4, 7):
                        for i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_3)
                                oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_2, i3_3_i4_3_fused])
                                ic = T.axis.reduce(992, i5_0 * 8 + i5_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 248, 7, 7, 4], "float32"], ["TENSOR", [32, 248, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 32, 7):
                    for ax3_ax4_fused in T.vectorized(28):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                            ax3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])", "l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])", "l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])", "l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])", "l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[124, 8])", "l53, l54 = sch.split(loop=l8, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l57, l58 = sch.split(loop=l9, factors=[v55, v56])", "v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])", "l61, l62 = sch.split(loop=l10, factors=[v59, v60])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)", "b63, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v64)", "sch.enter_postproc()", "b65 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.unroll_explicit\")", "b66, b67 = sch.get_child_blocks(b65)", "l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)", "l94 = sch.fuse(l92, l93)", "sch.vectorize(loop=l94)", "sch.annotate(block_or_loop=l68, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l68, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b67)", "l105 = sch.fuse(l103, l104)", "sch.vectorize(loop=l105)", "sch.annotate(block_or_loop=l95, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l95, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b106 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b106)", "b132 = sch.decompose_reduction(block=b106, loop=l117)"]
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_multiply_add_multiply_add_nn_relu_1
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_global_avg_pool2d
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add
[16:29:33] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_1
https://storage.cloud.google.com/octoml-aquarium-models/onnx_model_zoo/vision_classification_densenet.onnx
file existed. Skipping downloading.
/home/yj/models/densenet.onnx
Starting to build with relay.
/home/yj/anaconda3/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
