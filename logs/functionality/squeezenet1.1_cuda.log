nohup: ignoring input
[23:17:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_nn_conv2d_add_nn_relu"
[23:17:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 15, 15], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 15, 15):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 13, 13, 64, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:17:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(26, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(780):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 780 // 195)
                                    v2 = T.axis.spatial(15, ax0_ax1_ax2_ax3_fused % 195 // 13)
                                    v3 = T.axis.spatial(15, i6_0 + ax0_ax1_ax2_ax3_fused % 13)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(96):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax0_ax1_ax2_ax3_fused // 12)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 12 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                    yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                    xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                                    rc = T.axis.reduce(64, i4_0 * 4 + i4_1)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                                v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                                v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 2, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:17:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_conv2d_add_nn_relu_1"
[23:17:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 48, 15, 15], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 192, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 48, 15, 15):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 192, 13, 13, 48, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 192, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:17:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(13, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(9360):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(48, ax0_ax1_ax2_ax3_fused % 9360 // 195)
                                    v2 = T.axis.spatial(15, i5_0 + ax0_ax1_ax2_ax3_fused % 195 // 15)
                                    v3 = T.axis.spatial(15, ax0_ax1_ax2_ax3_fused % 15)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(27648):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, ax0_ax1_ax2_ax3_fused // 144)
                                    v1 = T.axis.spatial(48, ax0_ax1_ax2_ax3_fused % 144 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 3, 1, 6, 1, 13, 16, 1, 1, 1, 8, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 48 + i1_3 * 8 + i1_4)
                                    yy, xx = T.axis.remap("SS", [i0_2_i1_2_i2_2_i3_2_fused, i3_3])
                                    rc = T.axis.reduce(48, i4_1 * 16 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 48, 1, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 48 + ax1)
                                v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(13, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 6, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:17:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_conv2d_add_nn_relu_2"
[23:17:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 32, 29, 29], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 27, 27], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 27, 27], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 29, 29):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 28 and 1 <= i3_1 and i3_1 < 28, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 27, 27, 32, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 27, 27):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 27, 27):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:17:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6264):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 6264 // 783)
                                    v2 = T.axis.spatial(29, i5_0 + ax0_ax1_ax2_ax3_fused % 783 // 29)
                                    v3 = T.axis.spatial(29, ax0_ax1_ax2_ax3_fused % 29)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(3072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 24)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 24 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 8, 9, 3, 4, 1, 1, 1, 4, 3, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 9 * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(27, i2_3 * 3 + i2_4)
                                    xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_3)
                                    rc = T.axis.reduce(32, i4_0 * 8 + i4_1 * 4 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 27, 3):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 9 * 32 + ax1)
                                v2 = T.axis.spatial(27, ax2)
                                v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 9, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 9, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:17:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_conv2d_add_nn_relu_3"
[23:17:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 16, 57, 57], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 55, 55], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 55, 55], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 16, 57, 57):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 56 and 1 <= i3_1 and i3_1 < 56, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 55, 55, 16, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 55, 55):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 55, 55):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(275, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(12540):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 12540 // 3135)
                                    v2 = T.axis.spatial(57, i5_0 + ax0_ax1_ax2_ax3_fused % 3135 // 57)
                                    v3 = T.axis.spatial(57, ax0_ax1_ax2_ax3_fused % 57)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(48):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax0_ax1_ax2_ax3_fused // 12)
                                    v1 = T.axis.spatial(16, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 12 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 11, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_4)
                                    yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused // 55 * 11 + i2_4)
                                    xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55)
                                    rc = T.axis.reduce(16, i4_0 * 4 + i4_1)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 11, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax1)
                                v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused // 55 * 11 + ax2)
                                v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 1, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 5, 1, 11])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_conv2d_add_nn_relu_4"
[23:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 111, 111), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 224, 224], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 111, 111], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 111, 111], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 224, 224):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 111, 111, 3, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 111, 111):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 111, 111):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:17:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 111, 111), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 111, 111], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(888, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(3, 3, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(365):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 111 // 37 * 74 + i5_0 + ax0_ax1_ax2_ax3_fused % 365 // 5)
                                    v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 37 * 6 + i6_0 + ax0_ax1_ax2_ax3_fused % 5)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(8):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 111 * 8 + ax0_ax1_ax2_ax3_fused)
                                    v1, v2, v3 = T.axis.remap("SSS", [i4_0, i5_0, i6_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 37, 1, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 111 * 8 + i1_3)
                                    yy = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 111 // 37 * 37 + i2_3)
                                    xx = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 37, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 111 * 8 + ax1)
                                v2 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 111 // 37 * 37 + ax2)
                                v3 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 1, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 1, 37, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[37, 3, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:17:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_max_pool2d"
[23:17:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 111, 111), "float32"], tensor: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 64, 55, 55, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[23:17:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 111, 111), "float32"], tensor: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 64, 55, 55, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:17:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_conv2d_add_nn_relu_5"
[23:17:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 55, 55], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 16, 55, 55], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 55, 55], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 55, 55):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 55, 55, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 16, 55, 55):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 16, 55, 55):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:17:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([16, 64, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(193600):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 3025)
                                    v2 = T.axis.spatial(55, ax0_ax1_ax2_ax3_fused % 3025 // 55)
                                    v3 = T.axis.spatial(55, ax0_ax1_ax2_ax3_fused % 55)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused // 64)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 2, 5, 5, 1, 1, 1, 1, 4, 11, 11):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(55, i2_3 * 11 + i2_4)
                                    xx = T.axis.spatial(55, i3_3 * 11 + i3_4)
                                    rc = T.axis.reduce(64, i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 55, 55):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 5, 11])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 5, 11])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 64, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:17:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_conv2d_add_nn_relu_6"
[23:17:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 55, 55], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 16, 55, 55], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 55, 55], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 55, 55):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 55, 55, 128, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 55, 55], "float32"], ["TENSOR", [16, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 16, 55, 55):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 16, 55, 55):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:17:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 55, 55], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([16, 128, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(193600):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 64 + ax0_ax1_ax2_ax3_fused // 3025)
                                    v2 = T.axis.spatial(55, ax0_ax1_ax2_ax3_fused % 3025 // 55)
                                    v3 = T.axis.spatial(55, ax0_ax1_ax2_ax3_fused % 55)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused // 64)
                                    v1 = T.axis.spatial(128, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 11, 5, 16, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused // 11 * 11 + i2_3)
                                    xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_3)
                                    rc = T.axis.reduce(128, i4_0 * 64 + i4_1 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 55, 55], "float32"], ["TENSOR", [16, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 11, 5):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + ax1)
                                v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused // 11 * 11 + ax2)
                                v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 5, 11, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 11, 5, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:17:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_conv2d_add_nn_relu_7"
[23:17:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 16, 55, 55], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 55, 55], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 55, 55], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 16, 55, 55):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 55, 55, 16, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 55, 55):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 55, 55):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:17:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 16, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(88, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(4400):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused // 275)
                                    v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + ax0_ax1_ax2_ax3_fused % 275 // 55)
                                    v3 = T.axis.spatial(55, ax0_ax1_ax2_ax3_fused % 55)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 8 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 8, 5, 1, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 8 + i1_3)
                                    yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i2_3)
                                    xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused)
                                    rc = T.axis.reduce(16, i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 5, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 8 + ax1)
                                v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + ax2)
                                v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 1, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[11, 1, 1, 5, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:17:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_concatenate"
[23:17:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 55, 55), "float32"], placeholder_1: T.Buffer[(1, 64, 55, 55), "float32"], T_concat: T.Buffer[(1, 128, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 128, 55, 55):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[23:17:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 55, 55), "float32"], placeholder_1: T.Buffer[(1, 64, 55, 55), "float32"], T_concat: T.Buffer[(1, 128, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 128, 55, 55):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:17:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_nn_max_pool2d_1"
[23:17:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 55, 55), "float32"], tensor: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 128, 27, 27, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[23:17:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 55, 55), "float32"], tensor: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 128, 27, 27, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:17:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_conv2d_add_nn_relu_8"
[23:17:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 27, 27], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 27, 27], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 27, 27], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 27, 27):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 27, 27, 128, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 27, 27):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 27, 27):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 128, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(72, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(27, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(11664):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 729)
                                    v2 = T.axis.spatial(27, ax0_ax1_ax2_ax3_fused % 729 // 27)
                                    v3 = T.axis.spatial(27, ax0_ax1_ax2_ax3_fused % 27)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(256):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 3):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 9 * 2 + i1_3)
                                    yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 9 * 3 + i0_2_i1_2_i2_2_i3_2_fused // 9)
                                    xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_4)
                                    rc = T.axis.reduce(128, i4_0 * 16 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 3):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 9 * 2 + ax1)
                                v2 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 9 * 3 + i0_2_i1_2_i2_2_i3_2_fused // 9 + ax2)
                                v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 1, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 9, 3, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 9, 1, 3])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_conv2d_add_nn_relu_9"
[23:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 27, 27], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 27, 27], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 27, 27, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 27, 27):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 27, 27):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:17:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:17:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 27, 27], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(9, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(186624):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused // 729)
                                    v2 = T.axis.spatial(27, ax0_ax1_ax2_ax3_fused % 729 // 27)
                                    v3 = T.axis.spatial(27, ax0_ax1_ax2_ax3_fused % 27)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(4096):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + ax0_ax1_ax2_ax3_fused // 256)
                                    v1 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused % 256)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 1, 3, 9, 4, 1, 1, 1, 4, 3, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                    yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused // 3 * 9 + i2_3 * 3 + i2_4)
                                    xx = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + i3_3)
                                    rc = T.axis.reduce(256, i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 9, 9):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused // 3 * 9 + ax2)
                                v3 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 3, 1, 3, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 3, 1, 9, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 64, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:17:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_conv2d_add_nn_relu_10"
[23:17:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 32, 27, 27], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 27, 27], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 27, 27], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 27, 27):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 27, 27, 32, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 27, 27):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 27, 27):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 32, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1458):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + ax0_ax1_ax2_ax3_fused // 729)
                                    v2 = T.axis.spatial(27, ax0_ax1_ax2_ax3_fused % 729 // 27)
                                    v3 = T.axis.spatial(27, ax0_ax1_ax2_ax3_fused % 27)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax0_ax1_ax2_ax3_fused // 2)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 3, 27, 2, 1, 1, 1, 1, 9, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused)
                                    yy = T.axis.spatial(27, i2_3 * 9 + i2_4)
                                    xx = T.axis.spatial(27, i3_3)
                                    rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 27, 27):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 4, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 3, 9])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 27, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_concatenate_1"
[23:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(1, 128, 27, 27), "float32"], T_concat: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[23:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(1, 128, 27, 27), "float32"], T_concat: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_max_pool2d_2"
[23:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], tensor: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 13, 13, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[23:18:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], tensor: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 13, 13, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:18:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_conv2d_add_nn_relu_11"
[23:18:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], placeholder_1: T.Buffer[(48, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1), "float32"], T_relu: T.Buffer[(1, 48, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 48, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 48, 13, 13, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 13, 13], "float32"], ["TENSOR", [48, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 48, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 48, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:18:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], placeholder_1: T.Buffer[(48, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1), "float32"], T_relu: T.Buffer[(1, 48, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([48, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(5408):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 169)
                                    v2 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 169 // 13)
                                    v3 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1536):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(48, ax0_ax1_ax2_ax3_fused // 32)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 8, 13, 1, 16, 1, 1, 1, 2, 1, 13):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(48, i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_3 * 2 + i1_4)
                                    yy, xx = T.axis.remap("SS", [i2_3, i3_4])
                                    rc = T.axis.reduce(256, i4_0 * 32 + i4_1 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 13, 13], "float32"], ["TENSOR", [48, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 13, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(48, i0_1_i1_1_i2_1_i3_1_fused * 16 + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 1, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:18:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_conv2d_add_nn_relu_12"
[23:18:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(48, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1), "float32"], T_relu: T.Buffer[(1, 48, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 48, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 48, 13, 13, 384, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [48, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 48, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 48, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:18:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(48, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1), "float32"], T_relu: T.Buffer[(1, 48, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 384, 13, 13], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([48, 384, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(6, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1352):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 169)
                                    v2 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 169 // 13)
                                    v3 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(384):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(48, ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(384, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 1, 13, 13):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(48, i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3)
                                    yy, xx = T.axis.remap("SS", [i2_4, i3_4])
                                    rc = T.axis.reduce(384, i4_0 * 8 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [48, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 13, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(48, i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 6, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[48, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:18:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_conv2d_add_nn_relu_13"
[23:18:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 48, 13, 13], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 192, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 48, 13, 13):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 192, 13, 13, 48, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 192, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([192, 48, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(78, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(4056):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(48, i4_0 * 24 + ax0_ax1_ax2_ax3_fused // 169)
                                    v2 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 169 // 13)
                                    v3 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(4608):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, ax0_ax1_ax2_ax3_fused // 24)
                                    v1 = T.axis.spatial(48, i4_0 * 24 + ax0_ax1_ax2_ax3_fused % 24)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 8, 1, 1, 3, 1, 1, 1, 4, 1, 13):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 13 * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                    xx = T.axis.spatial(13, i3_4)
                                    rc = T.axis.reduce(48, i4_0 * 24 + i4_1 * 3 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 13 * 32 + ax1)
                                v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                                v3 = T.axis.spatial(13, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 6, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 8, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:18:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_concatenate_2"
[23:18:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 13, 13), "float32"], placeholder_1: T.Buffer[(1, 192, 13, 13), "float32"], T_concat: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(192 <= ax1, placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[23:18:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 13, 13), "float32"], placeholder_1: T.Buffer[(1, 192, 13, 13), "float32"], T_concat: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(192 <= ax1, placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:18:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_nn_conv2d_add_nn_relu_14"
[23:18:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 13, 13, 384, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:18:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 384, 13, 13], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(192, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(338):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 2 + ax0_ax1_ax2_ax3_fused // 169)
                                    v2 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 169 // 13)
                                    v3 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(64):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 2)
                                    v1 = T.axis.spatial(384, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 2, 13, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(13, i2_4)
                                    xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                    rc = T.axis.reduce(384, i4_0 * 2 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 13, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + ax1)
                                v2 = T.axis.spatial(13, ax2)
                                v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 1, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[192, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:18:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_conv2d_add_nn_relu_15"
[23:18:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], placeholder_1: T.Buffer[(64, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 13, 13, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 13, 13], "float32"], ["TENSOR", [64, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:18:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], placeholder_1: T.Buffer[(64, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(86528):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused // 169)
                                    v2 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 169 // 13)
                                    v3 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(32768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 512)
                                    v1 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused % 512)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                    xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                    rc = T.axis.reduce(512, i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 13, 13], "float32"], ["TENSOR", [64, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                                v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                                v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 64, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:18:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_conv2d_add_nn_relu_16"
[23:18:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 13, 13], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 13, 13):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 13, 13, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 13, 13], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(832):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 13)
                                    v2 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 13)
                                    v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(8192):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + ax0_ax1_ax2_ax3_fused // 64)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 8, 1, 1, 4, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                    xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    rc = T.axis.reduce(64, i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 32 + ax1)
                                v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                                v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 16, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_concatenate_3"
[23:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], placeholder_1: T.Buffer[(1, 256, 13, 13), "float32"], T_concat: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(256 <= ax1, placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[23:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], placeholder_1: T.Buffer[(1, 256, 13, 13), "float32"], T_concat: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(256 <= ax1, placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_nn_conv2d_add_nn_relu_17"
[23:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], placeholder_1: T.Buffer[(1000, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1000, 1, 1), "float32"], T_relu: T.Buffer[(1, 1000, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1000, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 1000, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1000, 13, 13, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 13, 13], "float32"], ["TENSOR", [1000, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1000, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1000, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:18:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], placeholder_1: T.Buffer[(1000, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1000, 1, 1), "float32"], T_relu: T.Buffer[(1, 1000, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 1000, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1000, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1300, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(130, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3328):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused // 13)
                                    v2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    v3 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2560):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused // 13 * 10 + ax0_ax1_ax2_ax3_fused // 256)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 256)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 64, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused // 13 * 10 + i0_1_i1_1_i2_1_i3_1_fused // 13)
                                    yy = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                    rc = T.axis.reduce(512, i4_0 * 256 + i4_1 * 64 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 13, 13], "float32"], ["TENSOR", [1000, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused // 13 * 10 + i0_1_i1_1_i2_1_i3_1_fused // 13 + ax1)
                                v2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax2)
                                v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[100, 10, 1, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:18:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_avg_pool2d"
[23:18:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000, 13, 13), "float32"], tensor: T.Buffer[(1, 1000, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 1000, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 1000, 1, 1, 13, 13):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 13 + rv0, ax3 * 13 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 13 + rv0, ax3 * 13 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 1000, 1, 1):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 13 + T.min(ax2 * 13 + 12, 12) + 1 - ax2 * 13) * (T.min(ax3, 0) * 13 + T.min(ax3 * 13 + 12, 12) + 1 - ax3 * 13), 1), "float32")
    

[23:18:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 2 design space(s) generated
[23:18:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000, 13, 13), "float32"], tensor: T.Buffer[(1, 1000, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            tensor_shared = T.alloc_buffer([1, 1000, 1, 1], dtype="float32", scope="shared")
            for i0, i1, i2, i3_0 in T.grid(1, 1000, 1, 1):
                for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(1, 1, 1, 1, 1):
                    for ax4_ax5_fused_1 in T.thread_binding(512, thread="threadIdx.x"):
                        with T.block("tensor"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(1000, i1)
                            ax2_1 = T.axis.spatial(1, 0)
                            ax3_1 = T.axis.spatial(1, 0)
                            rv0 = T.axis.reduce(13, ax4_ax5_fused_1 // 13)
                            rv1 = T.axis.reduce(13, ax4_ax5_fused_1 % 13)
                            T.where(ax4_ax5_fused_1 < 169)
                            T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 13 + rv0, ax3_1 * 13 + rv1])
                            T.writes(tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1])
                            with T.init():
                                tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = T.float32(0)
                            tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] + placeholder[ax0_1, ax1_1, ax2_1 * 13 + rv0, ax3_1 * 13 + rv1]
                for i3_1 in T.thread_binding(512, thread="threadIdx.x"):
                    with T.block("tensor_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(1000, i1)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        T.where(i3_1 < 1)
                        T.reads(tensor_shared[ax0, ax1, ax2, ax3])
                        T.writes(tensor[ax0, ax1, ax2, ax3])
                        tensor[ax0, ax1, ax2, ax3] = tensor_shared[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 13 + T.min(ax2 * 13 + 12, 12) + 1 - ax2 * 13) * (T.min(ax3, 0) * 13 + T.min(ax3 * 13 + 12, 12) + 1 - ax3 * 13), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
b2, = sch.get_consumers(block=b0)
l3, l4, l5, l6 = sch.get_loops(block=b2)
v7 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l8, l9 = sch.split(loop=l6, factors=[None, v7])
sch.bind(loop=l9, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l8, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l10, l11, l12, l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b0)
l20 = sch.fuse(l18, l19)
l21, l22 = sch.split(loop=l20, factors=[None, v7])
sch.bind(loop=l22, thread_axis="threadIdx.x")
v23 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v23)
[23:18:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000, 13, 13), "float32"], tensor: T.Buffer[(1, 1000, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            tensor_1 = T.alloc_buffer([1, 1000, 1, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 1000, 1, 1, 13, 13):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 13 + rv0, ax3 * 13 + rv1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 13 + rv0, ax3 * 13 + rv1]
            for i0, i1, i2, i3 in T.grid(1, 1000, 1, 1):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 13 + T.min(ax2 * 13 + 12, 12) + 1 - ax2 * 13) * (T.min(ax3, 0) * 13 + T.min(ax3 * 13 + 12, 12) + 1 - ax3 * 13), 1), "float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:18:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_reshape"
[23:18:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[0, ax1 % 1000, 0, 0])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 1000, 0, 0]
    

[23:18:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:18:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, ax1 % 1000, 0, 0])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, ax1 % 1000, 0, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:18:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |            N/A |          N/A |                   N/A |      0 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |            N/A |          N/A |                   N/A |      0 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |            N/A |          N/A |                   N/A |      0 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[23:18:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_conv2d_add_nn_relu"
[23:18:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:18:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:19:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92144d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9224588)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93cc0f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f9af78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c933c568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c82f4328)]: 1853 failure(s)
[23:19:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 195 candidate(s)
[23:21:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92144d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9224588)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93cc0f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f9af78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c933c568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c82f4328)]: 454 failure(s)
[23:24:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92144d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9224588)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93cc0f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f9af78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c933c568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c82f4328)]: 371 failure(s)
[23:26:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92144d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9224588)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93cc0f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f9af78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c933c568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c82f4328)]: 370 failure(s)
[23:28:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92144d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9224588)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93cc0f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f9af78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c933c568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c82f4328)]: 330 failure(s)
[23:28:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9993  0.9988  0.9987  0.9987  0.9982  0.9980  0.9979  0.9978  0.9977  0.9977  0.9968  0.9965  0.9961  0.9958
[17 : 32]:	0.9955  0.9953  0.9952  0.9950  0.9949  0.9948  0.9946  0.9944  0.9943  0.9937  0.9936  0.9935  0.9934  0.9934  0.9932  0.9931
[23:28:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:28:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:28:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:29:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [23:29:53] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x00005627061af554
  69: 0xffffffffffffffff


[23:30:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_nn_relu_1"
[23:30:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:30:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:31:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9236578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f608f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9328628)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c933c128)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c922ad98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9343fc8)]: 1784 failure(s)
[23:31:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 264 candidate(s)
[23:33:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9236578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f608f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9328628)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c933c128)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c922ad98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9343fc8)]: 504 failure(s)
[23:36:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9236578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f608f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9328628)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c933c128)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c922ad98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9343fc8)]: 411 failure(s)
[23:40:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9236578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f608f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9328628)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c933c128)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c922ad98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9343fc8)]: 398 failure(s)
[23:43:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9236578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f608f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9328628)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c933c128)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c922ad98)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9343fc8)]: 345 failure(s)
[23:43:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9997  0.9997  0.9996  0.9995  0.9995  0.9991  0.9990  0.9990  0.9988  0.9988  0.9985  0.9982  0.9981  0.9977
[17 : 32]:	0.9975  0.9974  0.9974  0.9964  0.9964  0.9956  0.9955  0.9953  0.9951  0.9951  0.9950  0.9950  0.9948  0.9946  0.9943  0.9942
[23:43:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:43:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:43:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:44:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:44:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_conv2d_add_nn_relu_2"
[23:44:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:44:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:45:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93dfc78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9281b98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93341d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9282b08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c932e648)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9345658)]: 1735 failure(s)
[23:45:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 313 candidate(s)
[23:48:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93dfc78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9281b98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93341d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9282b08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c932e648)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9345658)]: 307 failure(s)
[23:51:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93dfc78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9281b98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93341d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9282b08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c932e648)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9345658)]: 268 failure(s)
[23:54:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93dfc78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9281b98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93341d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9282b08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c932e648)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9345658)]: 252 failure(s)
[23:58:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93dfc78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9281b98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93341d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9282b08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c932e648)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9345658)]: 281 failure(s)
[23:58:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9997  0.9995  0.9991  0.9991  0.9989  0.9989  0.9988  0.9988  0.9987  0.9987  0.9987  0.9987  0.9986  0.9986  0.9984
[17 : 32]:	0.9976  0.9972  0.9971  0.9970  0.9969  0.9969  0.9966  0.9965  0.9965  0.9965  0.9964  0.9962  0.9959  0.9958  0.9958  0.9957
[23:59:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:59:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:59:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [23:59:59] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x000055693b94a554
  69: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [00:00:07] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x0000563359782554
  69: 0xffffffffffffffff


[00:00:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_conv2d_add_nn_relu_3"
[00:00:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:00:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:01:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9237408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f8e818)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f65c58)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f9a7f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8fd2118)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c92391f8)]: 1762 failure(s)
[00:01:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 286 candidate(s)
[00:04:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9237408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f8e818)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f65c58)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f9a7f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8fd2118)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c92391f8)]: 405 failure(s)
[00:06:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9237408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f8e818)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f65c58)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f9a7f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8fd2118)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c92391f8)]: 298 failure(s)
[00:09:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9237408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f8e818)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f65c58)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f9a7f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8fd2118)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c92391f8)]: 296 failure(s)
[00:12:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9237408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f8e818)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f65c58)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f9a7f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8fd2118)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c92391f8)]: 304 failure(s)
[00:13:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9994  0.9994  0.9993  0.9991  0.9991  0.9991  0.9990  0.9990  0.9990  0.9989  0.9989  0.9989  0.9988  0.9983
[17 : 32]:	0.9982  0.9980  0.9979  0.9977  0.9972  0.9971  0.9971  0.9970  0.9969  0.9964  0.9962  0.9962  0.9962  0.9960  0.9959  0.9955
[00:13:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:13:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:13:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:14:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_conv2d_add_nn_relu_4"
[00:14:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:14:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:15:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9321e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c927d3d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93b83e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92246d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c932e8d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93db668)]: 1849 failure(s)
[00:15:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 199 candidate(s)
[00:16:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9321e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c927d3d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93b83e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92246d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c932e8d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93db668)]: 284 failure(s)
[00:18:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9321e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c927d3d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93b83e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92246d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c932e8d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93db668)]: 250 failure(s)
[00:20:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9321e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c927d3d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93b83e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92246d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c932e8d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93db668)]: 259 failure(s)
[00:21:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9321e18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c927d3d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93b83e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92246d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c932e8d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93db668)]: 244 failure(s)
[00:22:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9998  0.9997  0.9996  0.9994  0.9993  0.9992  0.9987  0.9987  0.9984  0.9984  0.9983  0.9983  0.9978  0.9973
[17 : 32]:	0.9972  0.9970  0.9969  0.9968  0.9965  0.9965  0.9962  0.9961  0.9960  0.9959  0.9958  0.9958  0.9957  0.9955  0.9955  0.9955
[00:22:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:22:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:22:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:23:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [00:23:15] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x00005651e5f7f554
  69: 0xffffffffffffffff


[00:23:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d"
[00:23:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:23:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:23:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[00:23:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:24:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[00:24:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[00:24:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[00:25:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[00:25:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9665  0.9255  0.5969  0.5056  0.2451
[00:25:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[00:25:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[00:25:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[00:25:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[00:25:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_conv2d_add_nn_relu_5"
[00:25:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:25:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:26:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c91f49f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f2458)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c934db18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c923f768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93dd098)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f8a6d8)]: 1859 failure(s)
[00:26:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 189 candidate(s)
[00:27:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c91f49f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f2458)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c934db18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c923f768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93dd098)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f8a6d8)]: 538 failure(s)
[00:29:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c91f49f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f2458)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c934db18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c923f768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93dd098)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f8a6d8)]: 498 failure(s)
[00:31:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c91f49f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f2458)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c934db18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c923f768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93dd098)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f8a6d8)]: 482 failure(s)
[00:32:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c91f49f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f2458)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c934db18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c923f768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93dd098)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f8a6d8)]: 479 failure(s)
[00:33:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9996  0.9993  0.9991  0.9991  0.9990  0.9989  0.9989  0.9989  0.9988  0.9987  0.9984  0.9982  0.9981  0.9980  0.9980  0.9977
[17 : 32]:	0.9976  0.9976  0.9974  0.9974  0.9973  0.9971  0.9971  0.9970  0.9969  0.9969  0.9964  0.9962  0.9962  0.9960  0.9957  0.9956
[00:33:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:33:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:33:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:34:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:34:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_conv2d_add_nn_relu_6"
[00:34:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:34:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:35:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9329828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9216c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f9b8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8fc88b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c928d4c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9209408)]: 1888 failure(s)
[00:35:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 160 candidate(s)
[00:36:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9329828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9216c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f9b8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8fc88b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c928d4c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9209408)]: 566 failure(s)
[00:38:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9329828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9216c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f9b8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8fc88b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c928d4c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9209408)]: 515 failure(s)
[00:40:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9329828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9216c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f9b8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8fc88b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c928d4c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9209408)]: 476 failure(s)
[00:41:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9329828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9216c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f9b8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8fc88b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c928d4c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9209408)]: 484 failure(s)
[00:42:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9996  0.9984  0.9982  0.9982  0.9980  0.9977  0.9973  0.9968  0.9967  0.9967  0.9963  0.9957  0.9957
[17 : 32]:	0.9956  0.9953  0.9952  0.9951  0.9947  0.9940  0.9939  0.9938  0.9938  0.9937  0.9937  0.9935  0.9933  0.9931  0.9926  0.9925
[00:42:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:42:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:42:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:43:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:43:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_conv2d_add_nn_relu_7"
[00:43:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:43:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:44:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c7d512f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f808c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c921a118)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9205708)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef438)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f4ca8)]: 1675 failure(s)
[00:44:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 373 candidate(s)
[00:45:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c7d512f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f808c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c921a118)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9205708)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef438)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f4ca8)]: 366 failure(s)
[00:47:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c7d512f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f808c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c921a118)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9205708)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef438)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f4ca8)]: 366 failure(s)
[00:48:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c7d512f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f808c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c921a118)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9205708)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef438)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f4ca8)]: 300 failure(s)
[00:50:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c7d512f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c8f808c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c921a118)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9205708)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef438)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f4ca8)]: 347 failure(s)
[00:51:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9996  0.9996  0.9994  0.9991  0.9989  0.9988  0.9986  0.9986  0.9983  0.9982  0.9981  0.9977  0.9977  0.9976  0.9976
[17 : 32]:	0.9973  0.9970  0.9967  0.9966  0.9966  0.9965  0.9965  0.9964  0.9964  0.9964  0.9960  0.9959  0.9956  0.9955  0.9954  0.9951
[00:51:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:51:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:51:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:51:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:52:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_concatenate"
[00:52:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:52:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:52:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[00:52:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:53:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[00:53:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[00:53:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[00:54:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[00:54:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9162  0.7624  0.3491  0.1653  0.1349
[00:54:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[00:54:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[00:54:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[00:54:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[00:54:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_max_pool2d_1"
[00:54:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:54:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:54:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[00:54:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:55:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[00:55:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[00:56:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[00:56:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[00:56:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.7750  0.6651  0.6021  0.2749  0.1422
[00:56:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[00:56:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[00:56:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[00:56:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[00:57:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_conv2d_add_nn_relu_8"
[00:57:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:57:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:57:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f69458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91fc678)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9024d28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9277918)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f29db8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c92038e8)]: 1857 failure(s)
[00:57:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 191 candidate(s)
[00:59:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f69458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91fc678)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9024d28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9277918)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f29db8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c92038e8)]: 443 failure(s)
[01:00:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f69458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91fc678)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9024d28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9277918)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f29db8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c92038e8)]: 388 failure(s)
[01:02:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f69458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91fc678)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9024d28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9277918)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f29db8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c92038e8)]: 395 failure(s)
[01:04:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f69458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91fc678)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9024d28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9277918)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f29db8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c92038e8)]: 387 failure(s)
[01:05:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9998  0.9997  0.9991  0.9991  0.9989  0.9987  0.9987  0.9986  0.9983  0.9983  0.9978  0.9976  0.9976  0.9975  0.9973
[17 : 32]:	0.9970  0.9967  0.9963  0.9961  0.9961  0.9961  0.9961  0.9956  0.9953  0.9950  0.9949  0.9948  0.9947  0.9947  0.9942  0.9940
[01:05:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:05:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:05:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_conv2d_add_nn_relu_9"
[01:06:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:06:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:07:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f49b98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9343778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f24d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f60cc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9221108)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f24db8)]: 1898 failure(s)
[01:07:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 150 candidate(s)
[01:09:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f49b98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9343778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f24d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f60cc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9221108)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f24db8)]: 469 failure(s)
[01:10:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f49b98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9343778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f24d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f60cc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9221108)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f24db8)]: 387 failure(s)
[01:12:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f49b98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9343778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f24d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f60cc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9221108)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f24db8)]: 416 failure(s)
[01:14:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f49b98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9343778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f24d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f60cc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9221108)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f24db8)]: 402 failure(s)
[01:14:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9996  0.9995  0.9995  0.9994  0.9992  0.9992  0.9991  0.9991  0.9990  0.9987  0.9987  0.9987  0.9985  0.9984
[17 : 32]:	0.9980  0.9978  0.9978  0.9976  0.9968  0.9968  0.9967  0.9967  0.9961  0.9961  0.9957  0.9955  0.9953  0.9948  0.9947  0.9942
[01:14:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:14:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:14:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:15:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:15:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_conv2d_add_nn_relu_10"
[01:15:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:15:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:16:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f7db28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c916beb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f58e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c82dd668)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92083c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9052b98)]: 1698 failure(s)
[01:16:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 350 candidate(s)
[01:17:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f7db28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c916beb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f58e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c82dd668)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92083c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9052b98)]: 298 failure(s)
[01:19:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f7db28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c916beb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f58e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c82dd668)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92083c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9052b98)]: 253 failure(s)
[01:21:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f7db28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c916beb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f58e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c82dd668)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92083c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9052b98)]: 228 failure(s)
[01:23:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f7db28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c916beb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8f58e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c82dd668)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92083c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9052b98)]: 235 failure(s)
[01:23:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9997  0.9996  0.9995  0.9993  0.9992  0.9978  0.9977  0.9975  0.9973  0.9973  0.9968  0.9968  0.9967  0.9966
[17 : 32]:	0.9965  0.9963  0.9962  0.9962  0.9962  0.9962  0.9960  0.9958  0.9958  0.9956  0.9955  0.9953  0.9951  0.9947  0.9946  0.9945
[01:24:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:24:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:24:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:24:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:25:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_concatenate_1"
[01:25:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:25:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:25:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[01:25:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:25:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[01:26:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[01:26:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[01:27:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[01:27:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.5667  0.4547  0.3397  0.2564  0.1517
[01:27:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:27:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:27:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:27:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:27:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_2"
[01:27:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:27:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:27:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[01:27:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:28:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[01:28:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[01:28:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[01:29:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[01:29:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.4841  0.4116  0.1639  0.0546  0.0353
[01:29:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:29:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:29:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:29:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:29:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_conv2d_add_nn_relu_11"
[01:29:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:29:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:30:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9053be8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92771c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c836c228)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9208068)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c916b788)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91fc308)]: 1832 failure(s)
[01:30:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 216 candidate(s)
[01:31:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9053be8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92771c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c836c228)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9208068)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c916b788)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91fc308)]: 619 failure(s)
[01:33:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9053be8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92771c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c836c228)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9208068)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c916b788)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91fc308)]: 564 failure(s)
[01:35:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9053be8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92771c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c836c228)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9208068)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c916b788)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91fc308)]: 565 failure(s)
[01:36:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9053be8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92771c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c836c228)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9208068)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c916b788)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91fc308)]: 554 failure(s)
[01:37:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9997  0.9991  0.9980  0.9979  0.9976  0.9976  0.9975  0.9974  0.9974  0.9974  0.9972  0.9971  0.9969  0.9968
[17 : 32]:	0.9967  0.9963  0.9960  0.9958  0.9958  0.9955  0.9955  0.9951  0.9950  0.9948  0.9944  0.9941  0.9940  0.9940  0.9940  0.9938
[01:37:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:37:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:37:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:38:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:38:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_conv2d_add_nn_relu_12"
[01:38:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:38:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:39:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c922fd78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c965c598)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93372a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c7257018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82f7f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93e87b8)]: 1849 failure(s)
[01:39:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 199 candidate(s)
[01:40:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c922fd78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c965c598)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93372a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c7257018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82f7f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93e87b8)]: 689 failure(s)
[01:41:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c922fd78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c965c598)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93372a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c7257018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82f7f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93e87b8)]: 590 failure(s)
[01:42:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c922fd78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c965c598)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93372a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c7257018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82f7f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93e87b8)]: 487 failure(s)
[01:44:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c922fd78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c965c598)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93372a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c7257018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82f7f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93e87b8)]: 548 failure(s)
[01:44:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9994  0.9994  0.9992  0.9988  0.9988  0.9988  0.9984  0.9981  0.9978  0.9978  0.9977  0.9974  0.9972  0.9972  0.9972
[17 : 32]:	0.9971  0.9970  0.9968  0.9966  0.9964  0.9964  0.9964  0.9961  0.9956  0.9955  0.9952  0.9952  0.9947  0.9942  0.9941  0.9939
[01:44:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:44:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:44:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:45:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:45:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_conv2d_add_nn_relu_13"
[01:45:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:45:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:46:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c904cc48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93df5b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9216b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f6b708)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9280578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9216b68)]: 1711 failure(s)
[01:46:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 337 candidate(s)
[01:47:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c904cc48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93df5b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9216b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f6b708)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9280578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9216b68)]: 454 failure(s)
[01:48:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c904cc48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93df5b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9216b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f6b708)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9280578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9216b68)]: 438 failure(s)
[01:50:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c904cc48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93df5b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9216b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f6b708)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9280578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9216b68)]: 397 failure(s)
[01:51:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c904cc48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93df5b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9216b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f6b708)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9280578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9216b68)]: 414 failure(s)
[01:52:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9995  0.9992  0.9991  0.9990  0.9988  0.9984  0.9983  0.9983  0.9983  0.9982  0.9980  0.9978  0.9976  0.9976  0.9974  0.9968
[17 : 32]:	0.9968  0.9968  0.9964  0.9964  0.9963  0.9962  0.9962  0.9962  0.9962  0.9961  0.9961  0.9953  0.9953  0.9949  0.9944  0.9943
[01:52:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:52:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:52:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:53:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:53:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_concatenate_2"
[01:53:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:53:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:53:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[01:53:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:54:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[01:54:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[01:55:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[01:55:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[01:55:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9354  0.8620  0.8503  0.5795  0.2292
[01:55:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:55:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:55:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:55:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:56:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_conv2d_add_nn_relu_14"
[01:56:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:56:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:56:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c928a5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9204f08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9289bc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c836a258)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f49198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9242b48)]: 1840 failure(s)
[01:56:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 208 candidate(s)
[01:58:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c928a5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9204f08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9289bc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c836a258)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f49198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9242b48)]: 681 failure(s)
[01:59:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c928a5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9204f08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9289bc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c836a258)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f49198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9242b48)]: 516 failure(s)
[02:01:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c928a5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9204f08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9289bc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c836a258)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f49198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9242b48)]: 514 failure(s)
[02:02:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c928a5d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9204f08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9289bc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c836a258)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f49198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9242b48)]: 559 failure(s)
[02:03:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9998  0.9996  0.9992  0.9991  0.9991  0.9989  0.9988  0.9988  0.9984  0.9983  0.9982  0.9981  0.9979  0.9976
[17 : 32]:	0.9975  0.9974  0.9972  0.9967  0.9966  0.9962  0.9959  0.9956  0.9955  0.9954  0.9953  0.9950  0.9948  0.9943  0.9942  0.9941
[02:03:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:03:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:03:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:04:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:04:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_nn_conv2d_add_nn_relu_15"
[02:04:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:04:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:05:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f95f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c924f838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93943d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c932dd38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9333058)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9394258)]: 1898 failure(s)
[02:05:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 150 candidate(s)
[02:06:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f95f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c924f838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93943d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c932dd38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9333058)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9394258)]: 710 failure(s)
[02:07:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f95f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c924f838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93943d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c932dd38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9333058)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9394258)]: 584 failure(s)
[02:09:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f95f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c924f838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93943d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c932dd38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9333058)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9394258)]: 540 failure(s)
[02:10:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f95f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c924f838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93943d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c932dd38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9333058)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9394258)]: 557 failure(s)
[02:11:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9996  0.9995  0.9995  0.9993  0.9993  0.9991  0.9989  0.9986  0.9985  0.9985  0.9983  0.9983  0.9981  0.9980  0.9977
[17 : 32]:	0.9977  0.9971  0.9964  0.9963  0.9962  0.9960  0.9958  0.9957  0.9957  0.9954  0.9949  0.9944  0.9943  0.9936  0.9936  0.9934
[02:11:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:11:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:11:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:11:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:12:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_conv2d_add_nn_relu_16"
[02:12:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:12:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:12:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9221668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93df1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93b80c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c93ae078)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93ccbf8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93b8168)]: 1754 failure(s)
[02:12:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 294 candidate(s)
[02:13:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9221668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93df1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93b80c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c93ae078)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93ccbf8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93b8168)]: 489 failure(s)
[02:14:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9221668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93df1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93b80c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c93ae078)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93ccbf8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93b8168)]: 439 failure(s)
[02:16:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9221668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93df1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93b80c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c93ae078)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93ccbf8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93b8168)]: 395 failure(s)
[02:17:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9221668)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93df1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93b80c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c93ae078)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93ccbf8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93b8168)]: 365 failure(s)
[02:18:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9996  0.9990  0.9984  0.9983  0.9979  0.9978  0.9976  0.9975  0.9975  0.9972  0.9971  0.9968  0.9966  0.9964
[17 : 32]:	0.9963  0.9962  0.9961  0.9958  0.9952  0.9951  0.9949  0.9949  0.9949  0.9946  0.9937  0.9935  0.9926  0.9923  0.9914  0.9914
[02:18:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:18:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:18:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:19:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:19:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_concatenate_3"
[02:19:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:19:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:19:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[02:19:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:20:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[02:20:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[02:21:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[02:21:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[02:21:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8819  0.5351  0.3254  0.2479  0.0050
[02:21:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:21:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:21:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:21:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:21:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_conv2d_add_nn_relu_17"
[02:21:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:21:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:22:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f9a118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92497e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93948b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9055d08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93c5f58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93cf6f8)]: 1887 failure(s)
[02:22:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 161 candidate(s)
[02:23:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f9a118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92497e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93948b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9055d08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93c5f58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93cf6f8)]: 633 failure(s)
[02:25:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f9a118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92497e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93948b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9055d08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93c5f58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93cf6f8)]: 564 failure(s)
[02:26:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f9a118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92497e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93948b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9055d08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93c5f58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93cf6f8)]: 512 failure(s)
[02:28:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f9a118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92497e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93948b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9055d08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c93c5f58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c93cf6f8)]: 489 failure(s)
[02:28:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9997  0.9996  0.9990  0.9988  0.9986  0.9986  0.9985  0.9983  0.9982  0.9981  0.9980  0.9980  0.9978  0.9975
[17 : 32]:	0.9973  0.9973  0.9972  0.9970  0.9965  0.9962  0.9962  0.9960  0.9959  0.9958  0.9949  0.9949  0.9948  0.9946  0.9944  0.9943
[02:29:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:29:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:29:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:29:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:29:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_avg_pool2d"
[02:29:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:29:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:30:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927c828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93902b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933e218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f8a9a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9346748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933e2b8)]: 0 failure(s)
[02:30:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:30:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927c828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93902b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933e218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f8a9a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9346748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933e2b8)]: 0 failure(s)
[02:31:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927c828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93902b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933e218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f8a9a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9346748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933e2b8)]: 0 failure(s)
[02:31:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927c828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93902b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933e218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f8a9a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9346748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933e2b8)]: 0 failure(s)
[02:32:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927c828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93902b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933e218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f8a9a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9346748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933e2b8)]: 0 failure(s)
[02:32:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9920  0.9525  0.9477  0.9416  0.8684  0.8576  0.8007  0.7983  0.7871  0.7523  0.7393  0.7312  0.6885  0.6861  0.6770  0.6377
[17 : 32]:	0.6028  0.5968  0.5847  0.5806  0.5484  0.5331  0.5067  0.4413  0.4307  0.3723  0.3693  0.3411  0.3335  0.3131  0.3049  0.2910
[02:32:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:32:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 31 candidates(s) for measurement
[02:32:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 31 sample(s) to builder
[02:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 31 sample(s) to runner
[02:32:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_reshape"
[02:32:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:32:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:32:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[02:32:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:33:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[02:33:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[02:33:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[02:33:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[02:33:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8724  0.6834  0.5411  0.4824  0.0534
[02:33:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:33:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:33:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:33:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:34:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #0: GFLOPs: 264.3087. Time: 0.1889 ms. Best GFLOPs: 264.3087
[02:34:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i0_1_i1_1_i2_1_i3_1_fused])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 780 // 195)
                                        v2 = T.axis.spatial(15, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 195 // 15)
                                        v3 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 780)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 13, 1, 2, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy, xx = T.axis.remap("SS", [i2_3, i0_1_i1_1_i2_1_i3_1_fused])
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 64, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(4, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i2_4_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(120):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 6240 // 195)
                                    v2 = T.axis.spatial(15, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 195 // 15)
                                    v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 96)
                                        v1 = T.axis.spatial(64, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 96 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 3, 1, 4, 1, 1, 2, 1, 1, 1, 2, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(13, i2_4)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(64, i4_0 * 32 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 4, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 16, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #3: GFLOPs: 150.8549. Time: 0.3310 ms. Best GFLOPs: 264.3087
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #4: GFLOPs: 299.7038. Time: 0.1666 ms. Best GFLOPs: 299.7038
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 450)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) // 18)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 18 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 < 4608)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_4)
                                yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 208, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 208])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 13, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 225)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(64, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 13, 1, 1, 3, 3, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1800)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 9216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 3, 1, 4, 1, 1, 1, 3, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 208, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 208, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #8: GFLOPs: 109.4583. Time: 0.4561 ms. Best GFLOPs: 299.7038
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i0_1_i1_1_i2_1_i3_1_fused, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 1560 // 195)
                                    v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 195 // 13)
                                    v3 = T.axis.spatial(15, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 1560)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 24)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 24 // 3)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 1, 1, 2, 1, 13, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy, xx = T.axis.remap("SS", [i0_1_i1_1_i2_1_i3_1_fused, i3_3])
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #10: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(676, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 169 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13)
                            xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 + i5_0 + 0)
                                        v3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 13 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 6)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 169 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 169 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13)
                                xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 169 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 + ax2)
                            v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #11: GFLOPs: 680.7305. Time: 0.0733 ms. Best GFLOPs: 680.7305
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #12: GFLOPs: 11.5642. Time: 4.3174 ms. Best GFLOPs: 680.7305
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #13: GFLOPs: 308.3116. Time: 0.1619 ms. Best GFLOPs: 680.7305
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #14: GFLOPs: 692.5530. Time: 0.0721 ms. Best GFLOPs: 692.5530
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #15: GFLOPs: 1112.0385. Time: 0.0449 ms. Best GFLOPs: 1112.0385
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #16: GFLOPs: 9.5404. Time: 5.2332 ms. Best GFLOPs: 1112.0385
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #17: GFLOPs: 98.5382. Time: 0.5067 ms. Best GFLOPs: 1112.0385
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #18: GFLOPs: 1020.7062. Time: 0.0489 ms. Best GFLOPs: 1112.0385
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #19: GFLOPs: 460.5862. Time: 0.1084 ms. Best GFLOPs: 1112.0385
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #20: GFLOPs: 369.3459. Time: 0.1352 ms. Best GFLOPs: 1112.0385
[02:34:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(8, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_4_init)
                            yy = T.axis.spatial(13, i2_4_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 225)
                                    v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 225 // 15)
                                    v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 900)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(89):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 36)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 36 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 4608)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 4, 3, 3, 1, 8, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_4)
                                yy = T.axis.spatial(13, i2_4)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 4, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 52])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[02:34:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #22: GFLOPs: 126.5434. Time: 0.3945 ms. Best GFLOPs: 1112.0385
[02:34:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #23: GFLOPs: 304.2970. Time: 0.1641 ms. Best GFLOPs: 1112.0385
[02:34:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #24: GFLOPs: 870.1862. Time: 0.0574 ms. Best GFLOPs: 1112.0385
[02:34:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init in T.grid(2, 13, 16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 32 + i1_3_init * 16 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1800)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(60):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 9216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 3, 1, 2, 1, 13, 2, 3, 1, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 32 + i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 32 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(4, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5408 // 169)
                                        v2 = T.axis.spatial(15, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 169 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(64, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 2048)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 1, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i3_4)
                                rc = T.axis.reduce(64, i4_0 * 32 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 8, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 104])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #27: GFLOPs: 724.5319. Time: 0.0689 ms. Best GFLOPs: 1112.0385
[02:34:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #28: GFLOPs: 196.2041. Time: 0.2545 ms. Best GFLOPs: 1112.0385
[02:34:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #29: GFLOPs: 448.6862. Time: 0.1113 ms. Best GFLOPs: 1112.0385
[02:34:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #30: GFLOPs: 21.2289. Time: 2.3518 ms. Best GFLOPs: 1112.0385
[02:34:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #31: GFLOPs: 1059.2233. Time: 0.0471 ms. Best GFLOPs: 1112.0385
[02:34:17] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_conv2d_add_nn_relu"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |            N/A |          N/A |                   N/A |      0 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |            N/A |          N/A |                   N/A |      0 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 89.793

[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #0: GFLOPs: 31.1700. Time: 0.9015 ms. Best GFLOPs: 31.1700
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #1: GFLOPs: 296.1344. Time: 0.0949 ms. Best GFLOPs: 296.1344
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #2: GFLOPs: 114.0459. Time: 0.2464 ms. Best GFLOPs: 296.1344
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #3: GFLOPs: 425.0900. Time: 0.0661 ms. Best GFLOPs: 425.0900
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(39, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(4, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(47):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(39, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 117 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 117 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 117 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 5400)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(45):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(39, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + (ax0_ax1_ax2_ax3_fused_0 * 117 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 216)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 117 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 216 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 117 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 117 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 5184)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(24, 1, 1, 1, 4, 1, 1, 1, 3, 3, 1, 1, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i3_4)
                                rc = T.axis.reduce(48, i4_0 * 24 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 3, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 24, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 39, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 39, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #5: GFLOPs: 315.9886. Time: 0.0889 ms. Best GFLOPs: 425.0900
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(78, thread="threadIdx.x"):
                    for i1_3_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(6, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(78, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 78 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1560 // 195)
                                        v2 = T.axis.spatial(15, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 78 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 195 // 15)
                                        v3 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 78 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(78, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_0 * 234 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(48, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 234 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 234 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 78 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 4, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 6, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[6, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 78, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 78, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i2_3_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(48, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) // 225)
                                    v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 225 // 15)
                                    v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 < 3600)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(34):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) // 144)
                                    v1 = T.axis.spatial(48, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 144 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 < 6912)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 3, 1, 1, 13, 1, 8, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 3, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 208])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 208])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(3, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + i1_4_init)
                            yy = T.axis.spatial(13, i2_4_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2700)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 108)
                                        v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 108 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1296)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 3, 1, 1, 1, 1, 3, 1, 1, 1, 3, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + i1_4)
                                yy = T.axis.spatial(13, i2_4)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 12 + i4_1 * 3 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 4, 1, 3])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #9: GFLOPs: 451.0244. Time: 0.0623 ms. Best GFLOPs: 451.0244
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #10: GFLOPs: 409.6028. Time: 0.0686 ms. Best GFLOPs: 451.0244
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(338, thread="threadIdx.x"):
                    for i1_4_init in T.serial(12):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 169 * 12 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 169 // 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(338, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + ((ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4680 // 195)
                                        v2 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4680)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(338, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + (ax0_ax1_ax2_ax3_fused_0 * 1352 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 1352 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 72 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 1352 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6912)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(24, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 12, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 169 * 12 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 169 // 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 24 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 12, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 169 * 12 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 169 // 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 2, 1, 12])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 24, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 338, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 338, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(13, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + i1_4_init)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 12 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2340 // 195)
                                        v2 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 36)
                                        v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 36 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 1, 1, 1, 13, 1, 3, 1, 1, 1, 3, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + i1_4)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 12 + i4_1 * 3 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 4, 1, 3])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(8, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i2_4_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(48, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 1170 // 195)
                                    v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 195 // 13)
                                    v3 = T.axis.spatial(15, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 1170)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(67):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 18)
                                    v1 = T.axis.spatial(48, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 18 // 3)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 3456)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 8, 1, 1, 6, 1, 1, 1, 2, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(13, i2_4)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 6 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 4, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 6])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 52])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(48, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 585 // 195)
                                    v2 = T.axis.spatial(15, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 195 // 15)
                                    v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 585)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(48, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 13, 1, 3, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 3 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 2, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #15: GFLOPs: 831.6520. Time: 0.0338 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #16: GFLOPs: 427.4631. Time: 0.0657 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(312, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(312, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3600)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(312, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 144)
                                        v1 = T.axis.spatial(48, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 144 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6912)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 3, 1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i3_4)
                                rc = T.axis.reduce(48, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 24, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 312, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 312, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(169, thread="threadIdx.x"):
                    for i1_4_init in T.serial(8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(169, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2700)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(169, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + (ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) // 108)
                                    v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) % 108 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1 < 2592)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 1, 1, 12, 3, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 12 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused * 8 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 3, 1, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 12])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 169, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 169])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #19: GFLOPs: 325.9848. Time: 0.0862 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #20: GFLOPs: 197.6067. Time: 0.1422 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(39, thread="threadIdx.x"):
                    for i2_3_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_1_i1_1_i2_1_i3_1_fused * 3 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(60):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(39, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + ((ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4680 // 195)
                                        v2 = T.axis.spatial(15, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 195 // 15)
                                        v3 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(39, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 72 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 864)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 3, 1, 1, 13, 1, 3, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_1_i1_1_i2_1_i3_1_fused * 3 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 24 + i4_1 * 3 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_1_i1_1_i2_1_i3_1_fused * 3 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 4, 3, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 8, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 39, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 39, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #22: GFLOPs: 120.5893. Time: 0.2330 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #23: GFLOPs: 52.0989. Time: 0.5394 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #24: GFLOPs: 121.8571. Time: 0.2306 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #25: GFLOPs: 391.1326. Time: 0.0718 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #26: GFLOPs: 113.7210. Time: 0.2471 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(12, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i2_4_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                            yy = T.axis.spatial(13, i2_4_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(45):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 4680 // 195)
                                    v2 = T.axis.spatial(15, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 195 // 15)
                                    v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 72 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6912)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(24, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                yy = T.axis.spatial(13, i2_4)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 24 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 12, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 24, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 104])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 104, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #28: GFLOPs: 274.2982. Time: 0.1024 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #29: GFLOPs: 321.2287. Time: 0.0875 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #30: GFLOPs: 103.2199. Time: 0.2722 ms. Best GFLOPs: 831.6520
[02:34:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #31: GFLOPs: 380.6452. Time: 0.0738 ms. Best GFLOPs: 831.6520
[02:34:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_conv2d_add_nn_relu_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |            N/A |          N/A |                   N/A |      0 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 157.369

[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #0: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init, i3_4_init in T.grid(4, 3, 9):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 3 * 4 + i1_3_init)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i2_4_init)
                            xx = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 + 0)
                                    v2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused * 9 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 319 // 29)
                                    v3 = T.axis.spatial(29, (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 29)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1 < 319)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(32, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 1, 3, 3, 1, 1, 3, 9):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 3 * 4 + i1_3)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i2_4)
                                xx = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 3, 9):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 3 * 4 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 16, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 3, 1, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 3, 1, 1, 9])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 48])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 48, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #1: GFLOPs: 214.4407. Time: 0.2515 ms. Best GFLOPs: 214.4407
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(72, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(72, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 3, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 9 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 2 + i1_3_init)
                            yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 9 // 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + i2_4_init)
                            xx = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3132 // 783)
                                        v2 = T.axis.spatial(29, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 783 // 29)
                                        v3 = T.axis.spatial(29, ((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3132)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 3, 1, 1, 3, 1, 1, 3, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 9 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 2 + i1_3)
                                yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 9 // 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + i2_4)
                                xx = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 3, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 9 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 2 + ax1)
                            v2 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 9 // 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 3, 3, 1, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 3, 3, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 72, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 72, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #3: GFLOPs: 1123.8274. Time: 0.0480 ms. Best GFLOPs: 1123.8274
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #4: GFLOPs: 362.5650. Time: 0.1488 ms. Best GFLOPs: 1123.8274
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #5: GFLOPs: 1488.6350. Time: 0.0362 ms. Best GFLOPs: 1488.6350
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #6: GFLOPs: 1272.1807. Time: 0.0424 ms. Best GFLOPs: 1488.6350
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(48, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(108, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(2, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 2 + i1_3_init)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused % 3 * 3 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 9)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(108, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2376 // 297)
                                        v2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused * 9 + ((ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 297 // 27)
                                        v3 = T.axis.spatial(29, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2376)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(108, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 1, 1, 2, 1, 3, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 2 + i1_3)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused % 3 * 3 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 9)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 2 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused % 3 * 3 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 9 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 4, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 3, 3, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 9, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 108, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 108, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #8: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(9, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(32, 9):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 9 * 32 + i1_4_init)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 3 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 290 // 145)
                                        v2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused * 3 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 145 // 29)
                                        v3 = T.axis.spatial(29, ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 290)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 32, 1, 9):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 9 * 32 + i1_4)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 3 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 9):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 9 * 32 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 3 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[9, 1, 3, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 3, 1, 9])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 36, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 36, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(3, 2, 27):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy = T.axis.spatial(27, i2_4_init)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused % 3 * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 522 // 261)
                                    v2 = T.axis.spatial(29, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 261 // 9)
                                    v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused * 9 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 522)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 3, 2, 1, 1, 1, 2, 27, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy = T.axis.spatial(27, i2_4)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused % 3 * 3 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 27, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(27, ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused % 3 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 27])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 3, 1, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #10: GFLOPs: 817.7716. Time: 0.0660 ms. Best GFLOPs: 1488.6350
[02:34:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(9, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init in T.grid(4, 3, 3, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 16 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i2_3_init)
                            xx = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(174):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 6264 // 783)
                                    v2 = T.axis.spatial(29, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 783 // 29)
                                    v3 = T.axis.spatial(29, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 29)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 3, 1, 4, 3, 3, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 16 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i2_3)
                                xx = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 3 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 3, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 16 + ax1)
                            v2 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 9, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 9, 1, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 36])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 36, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #12: GFLOPs: 647.3750. Time: 0.0833 ms. Best GFLOPs: 1488.6350
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(432, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(4, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 4 + i1_3_init)
                            yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused % 3 * 3 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(432, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1) % 2552 // 319)
                                    v2 = T.axis.spatial(29, (ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1) % 319 // 11)
                                    v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused * 9 + (ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1) % 11)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1 < 2552)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(432, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 864 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 864 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 864 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 864 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 9216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 1, 1, 4, 1, 1, 2, 1, 3, 1, 1, 1, 3):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 4 + i1_3)
                                yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused % 3 * 3 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 4 + ax1)
                            v2 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused % 3 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 16, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 27, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 3, 1, 1, 3])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 432])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 432, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(324, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(4, 2, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 81 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 81 // 9)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(324, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3132 // 783)
                                        v2 = T.axis.spatial(29, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 783 // 29)
                                        v3 = T.axis.spatial(29, ((ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3132)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(324, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 4, 1, 1, 2, 1, 1, 1, 2, 1, 3):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 81 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 81 // 9)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 81 * 8 + ax1)
                            v2 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 81 // 9 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 4, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 3, 9, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 9, 1, 3])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 324, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 324, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #15: GFLOPs: 255.1199. Time: 0.2114 ms. Best GFLOPs: 1488.6350
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #16: GFLOPs: 935.2557. Time: 0.0577 ms. Best GFLOPs: 1488.6350
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #17: GFLOPs: 1064.4758. Time: 0.0507 ms. Best GFLOPs: 1488.6350
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(72, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(72, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(3, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 9 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 2 + i1_4_init)
                            yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + i2_3_init)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 * 3 + i0_2_i1_2_i2_2_i3_2_fused % 3)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(33):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) % 2320 // 145)
                                    v2 = T.axis.spatial(29, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) % 145 // 5)
                                    v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 9 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) % 5)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 < 2320)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 9 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 144)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 144 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 1, 3, 1, 16, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 9 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 2 + i1_4)
                                yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + i2_3)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 * 3 + i0_2_i1_2_i2_2_i3_2_fused % 3)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 3, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 9 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 2 + ax1)
                            v2 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 * 3 + i0_2_i1_2_i2_2_i3_2_fused % 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 3, 3, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[9, 1, 3, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 72])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 72, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #19: GFLOPs: 119.6376. Time: 0.4508 ms. Best GFLOPs: 1488.6350
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(243, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(16, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 64 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 27)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(243, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 243 + ax0_ax1_ax2_ax3_fused_1) % 4752 // 297)
                                    v2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 243 + ax0_ax1_ax2_ax3_fused_1) % 297 // 27)
                                    v3 = T.axis.spatial(29, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 243 + ax0_ax1_ax2_ax3_fused_1) % 27)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 243 + ax0_ax1_ax2_ax3_fused_1 < 4752)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(243, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 243 + ax0_ax1_ax2_ax3_fused_1) // 48)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 243 + ax0_ax1_ax2_ax3_fused_1) % 48 // 3)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 243 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 243 + ax0_ax1_ax2_ax3_fused_1 < 3072)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 16, 1, 1, 2, 3, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 64 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 27)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 64 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 27 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 16, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 9, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 27, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 243])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 243])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #21: GFLOPs: 1204.4346. Time: 0.0448 ms. Best GFLOPs: 1488.6350
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(54, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(4, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 27 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 4 + i1_3_init)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 27 // 3 * 3 + i2_3_init)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 440 // 55)
                                    v2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 27 // 3 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 55 // 11)
                                    v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 11)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1 < 440)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(43):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 27 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4608)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 4, 3, 1, 8, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 27 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 4 + i1_3)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 27 // 3 * 3 + i2_3)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 3, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 27 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 4 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 27 // 3 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 4, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[9, 1, 1, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 1, 9, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 36])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 36, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(9, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(96, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(9, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 3 * 4 + i1_4_init)
                            yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + i2_3_init)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 594 // 297)
                                        v2 = T.axis.spatial(29, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 297 // 11)
                                        v3 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused * 9 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 11)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 594)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 9, 1, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 3 * 4 + i1_4)
                                yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + i2_3)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 9, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 3 * 4 + ax1)
                            v2 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 3, 9, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 9, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 96, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 96, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #24: GFLOPs: 447.7731. Time: 0.1205 ms. Best GFLOPs: 1488.6350
[02:34:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(324, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(8, 3, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 81 * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 81 // 27 * 3 + i2_3_init)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(324, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 2552 // 319)
                                        v2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + ((ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 319 // 29)
                                        v3 = T.axis.spatial(29, ((ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2552)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(324, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) // 72)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) % 72 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1 < 4608)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 3, 1, 8, 3, 3, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 81 * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 81 // 27 * 3 + i2_3)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 3, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 81 * 16 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 81 // 27 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 3, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 27, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 324, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 324])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #26: GFLOPs: 112.6477. Time: 0.4788 ms. Best GFLOPs: 1488.6350
[02:34:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #27: GFLOPs: 518.1321. Time: 0.1041 ms. Best GFLOPs: 1488.6350
[02:34:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #28: GFLOPs: 286.4232. Time: 0.1883 ms. Best GFLOPs: 1488.6350
[02:34:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(108, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 3, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 3 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 2 + i1_3_init)
                            yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 9 * 3 + i2_4_init)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(108, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 841)
                                        v2 = T.axis.spatial(29, (ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 841 // 29)
                                        v3 = T.axis.spatial(29, (ax0_ax1_ax2_ax3_fused_0 * 432 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6728)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(108, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 3, 1, 2, 1, 3, 4, 1, 1, 1, 1, 3, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 3 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 2 + i1_3)
                                yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 9 * 3 + i2_4)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 3, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 3 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 2 + ax1)
                            v2 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 9 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 4, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 3, 3, 1, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 9, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 108, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 108, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #30: GFLOPs: 361.5850. Time: 0.1492 ms. Best GFLOPs: 1488.6350
[02:34:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 29, 29], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(12, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(324, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i2_4_init in T.grid(8, 3):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 81 * 8 + i1_3_init)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 81 // 27 * 3 + i2_4_init)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(26):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(324, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) % 8352 // 261)
                                        v2 = T.axis.spatial(29, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) % 261 // 29)
                                        v3 = T.axis.spatial(29, (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) % 29)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1 < 8352)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 28 and 1 <= v3 and v3 < 28, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(324, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) // 96)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) % 96 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 3, 1, 8, 1, 1, 1, 1, 1, 1, 1, 3, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 81 * 8 + i1_3)
                                    yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 81 // 27 * 3 + i2_4)
                                    xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_0, i6_1])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 3, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 3 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 81 * 8 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 81 // 27 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 4, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 3, 1, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 27, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 32, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 324])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 324])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l172)
[02:34:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_conv2d_add_nn_relu_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 229.831

[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(110, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                    for i1_4_init in T.serial(16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 55 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_4_init)
                            yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 55)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 684 // 171)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 55 + ((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 171 // 57)
                                        v3 = T.axis.spatial(57, ((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 57)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 684)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 55 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 36)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 36 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 55 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_4)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 55)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 55 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 55 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[55, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 55, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 55, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #1: GFLOPs: 215.3623. Time: 0.2607 ms. Best GFLOPs: 215.3623
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(10, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(110, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(4, 11, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 // 11 * 11 + i2_3_init)
                            xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(46):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 5016 // 627)
                                    v2 = T.axis.spatial(57, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 627 // 11)
                                    v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 11)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 < 5016)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 330 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 330 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 330 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 3, 1, 1, 4, 11, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 // 11 * 11 + i2_3)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 11, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 // 11 * 11 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 2, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 5, 11, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[5, 1, 11, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 110])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 110, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #3: GFLOPs: 362.3747. Time: 0.1549 ms. Best GFLOPs: 362.3747
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(5, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(88, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 5, 11):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 11 * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i2_4_init)
                            xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(33):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(88, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 5720 // 715)
                                        v2 = T.axis.spatial(57, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 715 // 13)
                                        v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused * 11 + ((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 5720)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(88, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 264 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 264 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 264 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 1, 1, 8, 1, 1, 1, 2, 5, 11):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 11 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i2_4)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 5, 11):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 11 * 2 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 11, 1, 5])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[5, 1, 1, 1, 11])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 88, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 88, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(176, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(110, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(59):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 6384 // 399)
                                    v2 = T.axis.spatial(57, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 399 // 7)
                                    v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 < 6384)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 144)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 144 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1 in T.grid(1, 1):
                            for i1_4_init, i3_4_init in T.grid(2, 5):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 2 + i1_4_init)
                                    yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55)
                                    xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i3_4_init)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 16, 3, 1, 1, 2, 1, 5):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 2 + i1_4)
                                    yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55)
                                    xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_2, i6_1])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 5):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 2 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 2, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[11, 1, 1, 1, 5])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 110])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 110, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l178)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #6: GFLOPs: 768.9260. Time: 0.0730 ms. Best GFLOPs: 768.9260
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #7: GFLOPs: 42.1967. Time: 1.3305 ms. Best GFLOPs: 768.9260
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #8: GFLOPs: 29.4966. Time: 1.9034 ms. Best GFLOPs: 768.9260
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #9: GFLOPs: 46.9425. Time: 1.1960 ms. Best GFLOPs: 768.9260
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(176, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(5, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                    for i1_4_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 4 + i1_4_init)
                            yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i0_1_i1_1_i2_1_i3_1_fused)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3192 // 399)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + ((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 399 // 57)
                                        v3 = T.axis.spatial(57, ((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 57)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 3192)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 4, 3, 3, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 4 + i1_4)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i0_1_i1_1_i2_1_i3_1_fused)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 4 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[11, 5, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 55, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 55, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #11: GFLOPs: 2639.6107. Time: 0.0213 ms. Best GFLOPs: 2639.6107
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(22, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(50, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(4, 2, 11):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 25 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 * 11 + i2_4_init)
                            xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(50, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 50 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3080 // 385)
                                        v2 = T.axis.spatial(57, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 50 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 385 // 7)
                                        v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + ((ax0_ax1_ax2_ax3_fused_0 * 50 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 50 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3080)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(50, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 200 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 200 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 200 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 50 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 8, 1, 3, 1, 2, 11, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 25 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 * 11 + i2_4)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 11, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 25 * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 * 11 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 2, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 5, 1, 11])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[11, 1, 5, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 50, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 50, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(10, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(110, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(4, 11, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 // 11 * 11 + i2_3_init)
                            xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(46):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 5016 // 627)
                                    v2 = T.axis.spatial(57, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 627 // 11)
                                    v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 11)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 < 5016)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 440 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 440 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 440 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 3, 1, 1, 4, 11, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 // 11 * 11 + i2_3)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 11, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 // 11 * 11 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 2, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 5, 11, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[5, 1, 11, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 110])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 110, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(11, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(80, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                    for i1_4_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 5 * 4 + i1_4_init)
                            yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused // 5)
                            xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) % 798 // 399)
                                    v2 = T.axis.spatial(57, (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) % 399 // 7)
                                    v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused * 5 + (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1 < 798)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 5 * 4 + i1_4)
                                yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused // 5)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 5 * 4 + ax1)
                            v2 = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused // 5 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 5, 11, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[11, 1, 5, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 55])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 55, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(25, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(11, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(176, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i1_3_init in T.serial(4):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 11 * 4 + i1_3_init)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused // 5 * 11 + i0_1_i1_1_i2_1_i3_1_fused)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(176, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 176 + ax0_ax1_ax2_ax3_fused_1) % 2288 // 143)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused // 5 * 11 + (ax0_ax1_ax2_ax3_fused_0 * 176 + ax0_ax1_ax2_ax3_fused_1) % 143 // 11)
                                        v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 176 + ax0_ax1_ax2_ax3_fused_1) % 11)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(176, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 704 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                            v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 704 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 48 // 3)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 704 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i6_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 176 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 8, 3, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 11 * 4 + i1_3)
                                    yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused // 5 * 11 + i0_1_i1_1_i2_1_i3_1_fused)
                                    xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                                    rc = T.axis.reduce(16, i4_1 * 8 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 11 * 4 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused // 5 * 11 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[5, 11, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[5, 1, 11, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 176])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 176, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l175)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #16: GFLOPs: 7.9149. Time: 7.0934 ms. Best GFLOPs: 2639.6107
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #17: GFLOPs: 17.2758. Time: 3.2499 ms. Best GFLOPs: 2639.6107
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #18: GFLOPs: 2044.5636. Time: 0.0275 ms. Best GFLOPs: 2639.6107
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #19: GFLOPs: 1466.5261. Time: 0.0383 ms. Best GFLOPs: 2639.6107
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #20: GFLOPs: 1834.9838. Time: 0.0306 ms. Best GFLOPs: 2639.6107
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(22, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(400, thread="threadIdx.x"):
                    for i3_4_init in T.serial(11):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 25)
                            yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i0_2_i1_2_i2_2_i3_2_fused % 25 // 5)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(400, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 400 + ax0_ax1_ax2_ax3_fused_1) % 3080 // 385)
                                    v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + (ax0_ax1_ax2_ax3_fused_0 * 400 + ax0_ax1_ax2_ax3_fused_1) % 385 // 55)
                                    v3 = T.axis.spatial(57, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 400 + ax0_ax1_ax2_ax3_fused_1) % 55)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 400 + ax0_ax1_ax2_ax3_fused_1 < 3080)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(400, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 25)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i0_2_i1_2_i2_2_i3_2_fused % 25 // 5)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 11):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 11 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 25 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 11 * 5 + i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[11, 1, 5, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 5, 1, 11])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 400])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 400, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(11, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(800, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i1_3_init, i2_3_init in T.grid(2, 11):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 25 * 2 + i1_3_init)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 * 11 + i2_3_init)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(800, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(16, ((ax0_ax1_ax2_ax3_fused_0 * 800 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 4560 // 285)
                                            v2 = T.axis.spatial(57, ((ax0_ax1_ax2_ax3_fused_0 * 800 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 285 // 5)
                                            v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused * 5 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 800 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 800 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 4560)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(800, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 800 + ax0_ax1_ax2_ax3_fused_1) // 48)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 800 + ax0_ax1_ax2_ax3_fused_1) % 48 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 800 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 800 + ax0_ax1_ax2_ax3_fused_1 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 11, 1, 8, 3, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 25 * 2 + i1_3)
                                    yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 * 11 + i2_3)
                                    xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5)
                                    rc = T.axis.reduce(16, i4_1 * 8 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 11, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 25 * 2 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 * 11 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 5, 11, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[11, 1, 5, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 800, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 800])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l175)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(5, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(88, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(110, thread="threadIdx.x"):
                    for i1_3_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 11 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 4 + i1_3_init)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55)
                            xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_1_i1_1_i2_1_i3_1_fused % 11)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(26):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 2860 // 715)
                                    v2 = T.axis.spatial(57, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 715 // 13)
                                    v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused * 11 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 330 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 330 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 330 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 4, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 11 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 4 + i1_3)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_1_i1_1_i2_1_i3_1_fused % 11)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 11 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 4 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_1_i1_1_i2_1_i3_1_fused % 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 2, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[5, 11, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 110])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 110, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #24: GFLOPs: 80.7203. Time: 0.6955 ms. Best GFLOPs: 2639.6107
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(5, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(88, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(4, 11, 5):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 11 * 4 + i1_4_init)
                            yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i2_4_init)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(88, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 2964 // 741)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused * 11 + ((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 741 // 57)
                                        v3 = T.axis.spatial(57, ((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 57)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2964)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(88, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 352 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 36)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 352 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 36 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 352 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 352 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 3, 1, 1, 1, 1, 2, 1, 1, 1, 4, 11, 5):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 11 * 4 + i1_4)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i2_4)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 11, 5):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 11 * 4 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[5, 1, 1, 1, 11])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 11, 1, 5])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 88, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 88, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #26: GFLOPs: 1623.8622. Time: 0.0346 ms. Best GFLOPs: 2639.6107
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(5, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(11, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(11, 5):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 5 + i2_4_init)
                            xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(45):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 2860 // 715)
                                    v2 = T.axis.spatial(57, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 715 // 13)
                                    v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused * 11 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 2860)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 11, 4, 1, 3, 1, 1, 5, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 5 + i2_4)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i3_3)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 5, 11):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 5 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 11, 1, 1, 5])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[5, 1, 1, 11, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(5, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(88, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(8, 55):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 11 * 8 + i1_3_init)
                            yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                            xx = T.axis.spatial(55, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(88, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 2964 // 741)
                                        v2 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused * 11 + ((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 741 // 57)
                                        v3 = T.axis.spatial(57, ((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 57)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2964)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(27):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(88, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) // 36)
                                    v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) % 36 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1 < 2304)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 3, 1, 8, 1, 55, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 11 * 8 + i1_3)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                                xx = T.axis.spatial(55, i3_3)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 55):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 11 * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11 + ax2)
                            v3 = T.axis.spatial(55, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[5, 1, 11, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 55, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 88, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 88])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #29: GFLOPs: 61.0939. Time: 0.9190 ms. Best GFLOPs: 2639.6107
[02:34:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 57, 57], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(10, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(110, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(4, 11, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 // 11 * 11 + i2_3_init)
                            xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5016 // 627)
                                        v2 = T.axis.spatial(57, ((ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 627 // 11)
                                        v3 = T.axis.spatial(57, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 11)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 5016)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 56 and 1 <= v3 and v3 < 56, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 440 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 440 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 440 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 4, 11, 1, 8, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 // 11 * 11 + i2_3)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 11, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 // 11 * 11 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 2, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 5, 11, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[5, 1, 11, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 110, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 110, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:35:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_3"] Trial #31: GFLOPs: 1021.8346. Time: 0.0549 ms. Best GFLOPs: 2639.6107
[02:35:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_conv2d_add_nn_relu_3"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 272.37

[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #0: GFLOPs: 807.2686. Time: 0.0547 ms. Best GFLOPs: 807.2686
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #1: GFLOPs: 211.3396. Time: 0.2089 ms. Best GFLOPs: 807.2686
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 111, 111), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 111, 111], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(37, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(74, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(2, 16, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 37 * 32 + i1_3_init * 16 + i1_4_init)
                            yy = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused % 37 * 3 + i2_4_init)
                            xx = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(74, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(224, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 74 + ax0_ax1_ax2_ax3_fused_1) % 1547 // 7)
                                    v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused * 6 + (ax0_ax1_ax2_ax3_fused_0 * 74 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 74 + ax0_ax1_ax2_ax3_fused_1 < 1547)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(74, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1, v2 = T.axis.remap("SS", [i4_0, i5_0])
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 16, 3, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 37 * 32 + i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused % 37 * 3 + i2_4)
                                xx = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + i0_1_i1_1_i2_1_i3_1_fused)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 3, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 37 * 32 + ax1)
                            v2 = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused % 37 * 3 + ax2)
                            v3 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 37, 1, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[37, 3, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 74])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 74, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #3: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 111, 111), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 111, 111], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(111, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i2_4_init in T.grid(3, 37):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(111, i2_3_init * 37 + i2_4_init)
                            xx = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(224, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 669 // 3)
                                    v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 669)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 37, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(111, i2_3 * 37 + i2_4)
                                xx, rc, ry, rx = T.axis.remap("SRRR", [i0_0_i1_0_i2_0_i3_0_fused, i4_0, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 111, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(111, ax2)
                            v3 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 3, 37])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[111, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #4: GFLOPs: 1866.1607. Time: 0.0237 ms. Best GFLOPs: 1866.1607
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #5: GFLOPs: 1627.9534. Time: 0.0271 ms. Best GFLOPs: 1866.1607
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 111, 111), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 111, 111], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(37, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(148, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(148, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) % 4683 // 1561)
                                    v2 = T.axis.spatial(224, (ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) % 1561 // 7)
                                    v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused * 6 + (ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1 < 4683)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(148, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) // 27)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) % 27 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1 < 1728)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(4, 3, 2, 3):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 37 * 8 + i1_3_init * 2 + i1_4_init)
                                yy = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused % 37 * 3 + i2_4_init)
                                xx = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 4, 1, 3, 1, 3, 3, 1, 2, 3, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 37 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused % 37 * 3 + i2_4)
                                xx = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 3, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 37 * 8 + ax1)
                            v2 = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused % 37 * 3 + ax2)
                            v3 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 37, 1, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[37, 1, 1, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 148])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 148])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l174)
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 111, 111), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 111, 111], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(296, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(24, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(37, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init in T.serial(3):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 3)
                                yy = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + i2_3_init)
                                xx = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused % 3)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(126):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(37, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 4641 // 1547)
                                        v2 = T.axis.spatial(224, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 1547 // 7)
                                        v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 37 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1 < 4641)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(37, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                            v2 = T.axis.spatial(3, i5_0)
                                            v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 72)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 3)
                                    yy = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + i2_3)
                                    xx = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused % 3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_0, i6_1])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 3, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 3 + ax1)
                            v2 = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + ax2)
                            v3 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused % 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 8, 1, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 37, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[37, 3, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 37])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 37, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l174)
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 111, 111), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 111, 111], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(37, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(37, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i2_3_init, i3_3_init, i1_4_init in T.grid(2, 3, 3, 16):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3_init * 16 + i1_4_init)
                                yy = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + i2_3_init)
                                xx = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(90):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(37, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 3315 // 1105)
                                        v2 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused * 6 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 1105 // 221)
                                        v3 = T.axis.spatial(224, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 221)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1 < 3315)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(37, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 111 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 111 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 16, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3 * 16 + i1_4)
                                    yy = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + i2_3)
                                    xx = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_0, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 3, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + ax1)
                            v2 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + ax2)
                            v3 = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[37, 1, 1, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 37, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 37])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 37, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l174)
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #9: GFLOPs: 1670.9533. Time: 0.0264 ms. Best GFLOPs: 1866.1607
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #10: GFLOPs: 687.6901. Time: 0.0642 ms. Best GFLOPs: 1866.1607
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #11: GFLOPs: 812.5353. Time: 0.0543 ms. Best GFLOPs: 1866.1607
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #12: GFLOPs: 919.3240. Time: 0.0480 ms. Best GFLOPs: 1866.1607
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #13: GFLOPs: 20.3875. Time: 2.1660 ms. Best GFLOPs: 1866.1607
[02:35:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 111, 111), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 111, 111], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(37, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(24, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(37, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(3, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 3 * 8 + i1_4_init)
                            yy = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + i0_1_i1_1_i2_1_i3_1_fused % 3)
                            xx = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(31):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(37, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused * 6 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 1115 // 223)
                                    v3 = T.axis.spatial(224, (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 223)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1 < 1115)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(37, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 74 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1, v2 = T.axis.remap("SS", [i4_0, i5_0])
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 74 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 3 * 8 + i1_4)
                                yy = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + i0_1_i1_1_i2_1_i3_1_fused % 3)
                                xx = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 3 * 8 + ax1)
                            v2 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused * 3 + i0_1_i1_1_i2_1_i3_1_fused % 3 + ax2)
                            v3 = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 1, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[37, 3, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 37, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 37])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 37, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #15: GFLOPs: 420.4848. Time: 0.1050 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #16: GFLOPs: 1407.0788. Time: 0.0314 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #17: GFLOPs: 102.0493. Time: 0.4327 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #18: GFLOPs: 751.3483. Time: 0.0588 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #19: GFLOPs: 1643.9332. Time: 0.0269 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #20: GFLOPs: 552.3631. Time: 0.0799 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #21: GFLOPs: 1130.6258. Time: 0.0391 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #22: GFLOPs: 347.4348. Time: 0.1271 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 111, 111), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 111, 111], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(74, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(148, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(148, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) % 4683 // 1561)
                                    v2 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 37 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) % 1561 // 223)
                                    v3 = T.axis.spatial(224, (ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) % 223)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1 < 4683)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(148, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 592 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 27)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 592 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 27 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 592 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 592 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 148 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 864)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i3_3_init, i1_4_init in T.grid(2, 3, 4):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 37 * 8 + i1_3_init * 4 + i1_4_init)
                                yy = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused)
                                xx = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused % 37 * 3 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 1, 3, 1, 3, 3, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 37 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused)
                                xx = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused % 37 * 3 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 37 * 8 + ax1)
                            v2 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused % 37 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[37, 3, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 37, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 148])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 148, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l176)
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #24: GFLOPs: 606.6520. Time: 0.0728 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #25: GFLOPs: 117.1735. Time: 0.3769 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 111, 111), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 111, 111], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(74, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(37, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i2_3_init, i1_4_init in T.grid(8, 3, 4):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 32 + i1_3_init * 4 + i1_4_init)
                                yy = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + i2_3_init)
                                xx = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(90):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(37, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 3315 // 1105)
                                        v2 = T.axis.spatial(224, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 1105 // 5)
                                        v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 37 * 6 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1) % 5)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 37 + ax0_ax1_ax2_ax3_fused_1 < 3315)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(37, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 3, 1, 3, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + i2_3)
                                    xx = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_0, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 3, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 37 * 32 + ax1)
                            v2 = T.axis.spatial(111, i0_2_i1_2_i2_2_i3_2_fused * 3 + ax2)
                            v3 = T.axis.spatial(111, i0_0_i1_0_i2_0_i3_0_fused % 37 * 3 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 37, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[37, 3, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 37])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 37, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l174)
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #27: GFLOPs: 58.9595. Time: 0.7490 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #28: GFLOPs: 636.5090. Time: 0.0694 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #29: GFLOPs: 959.9917. Time: 0.0460 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #30: GFLOPs: 1432.0077. Time: 0.0308 ms. Best GFLOPs: 1866.1607
[02:35:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_4"] Trial #31: GFLOPs: 315.2316. Time: 0.1401 ms. Best GFLOPs: 1866.1607
[02:35:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_conv2d_add_nn_relu_4"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 296.033

[02:35:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 22.9220. Time: 0.0760 ms. Best GFLOPs: 22.9220
[02:35:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 295.4171. Time: 0.0059 ms. Best GFLOPs: 295.4171
[02:35:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 63.1757. Time: 0.0276 ms. Best GFLOPs: 295.4171
[02:35:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 309.7677. Time: 0.0056 ms. Best GFLOPs: 309.7677
[02:35:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 37.8059. Time: 0.0461 ms. Best GFLOPs: 309.7677
[02:35:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 165
Total latency (us): 301.658

[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #0: GFLOPs: 178.8390. Time: 0.0352 ms. Best GFLOPs: 178.8390
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #1: GFLOPs: 49.5352. Time: 0.1270 ms. Best GFLOPs: 178.8390
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #2: GFLOPs: 139.0180. Time: 0.0453 ms. Best GFLOPs: 178.8390
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #3: GFLOPs: 88.6666. Time: 0.0710 ms. Best GFLOPs: 178.8390
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(11, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(44, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(4, 5, 5):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 11 * 4 + i1_3_init)
                            yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i2_3_init)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(50):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(44, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 176 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 275)
                                        v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + (ax0_ax1_ax2_ax3_fused_0 * 176 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 275 // 55)
                                        v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 176 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(44, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(64, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 44 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 512)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 5, 1, 32, 1, 1, 1, 1, 1, 5):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 11 * 4 + i1_3)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i2_3)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 32 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 5, 5):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 11 * 4 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[11, 1, 1, 5, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 11, 1, 5])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 44, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 44, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(10, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(4, 2, 11):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 5 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + i2_4_init)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(220):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) // 3025)
                                    v2 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) % 3025 // 55)
                                    v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) % 55)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 2, 1, 1, 1, 2, 11, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 5 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + i2_4)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 11, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 5 * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 5, 1, 1, 11])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 55])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 55, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 2, 11, 5):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused // 11 * 11 + i2_4_init)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(110):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3025)
                                        v2 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3025 // 55)
                                        v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax0_ax1_ax2_ax3_fused_1 // 4)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + ax0_ax1_ax2_ax3_fused_1 % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 32)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 2, 11, 5):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused // 11 * 11 + i2_4)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 11, 5):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused // 11 * 11 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 5, 1, 11])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 11, 1, 5])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 55, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 55])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #7: GFLOPs: 113.8377. Time: 0.0553 ms. Best GFLOPs: 178.8390
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #8: GFLOPs: 102.1070. Time: 0.0616 ms. Best GFLOPs: 178.8390
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #9: GFLOPs: 74.1593. Time: 0.0848 ms. Best GFLOPs: 178.8390
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #10: GFLOPs: 1228.1753. Time: 0.0051 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #11: GFLOPs: 4.3436. Time: 1.4486 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #12: GFLOPs: 188.2581. Time: 0.0334 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #13: GFLOPs: 1084.2691. Time: 0.0058 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #14: GFLOPs: 203.1881. Time: 0.0310 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #15: GFLOPs: 339.8865. Time: 0.0185 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #16: GFLOPs: 374.6864. Time: 0.0168 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #17: GFLOPs: 98.4350. Time: 0.0639 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(5, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init in T.grid(8, 11, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused // 5)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(220):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) // 3025)
                                    v2 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) % 3025 // 55)
                                    v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) % 55)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 8, 1, 11, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused // 5)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 11):
                        with T.block("conv2d_nchw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused // 5 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 5, 11, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 5, 11, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 55])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 55, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(5, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init in T.grid(8, 11, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused // 5)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(55):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3025)
                                        v2 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3025 // 55)
                                        v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 8, 1, 11, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused // 5)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 11):
                        with T.block("conv2d_nchw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused // 5 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 5, 11, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 5, 11, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 55, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 55, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #20: GFLOPs: 603.8279. Time: 0.0104 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #21: GFLOPs: 121.7480. Time: 0.0517 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #22: GFLOPs: 250.7962. Time: 0.0251 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #23: GFLOPs: 749.1772. Time: 0.0084 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #24: GFLOPs: 224.5124. Time: 0.0280 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #25: GFLOPs: 781.4590. Time: 0.0081 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #26: GFLOPs: 234.2991. Time: 0.0269 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #27: GFLOPs: 98.0889. Time: 0.0641 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(5, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(121, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(8, 5):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_4_init)
                            yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused // 11)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(121, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 484 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 605)
                                        v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + (ax0_ax1_ax2_ax3_fused_0 * 484 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 605 // 55)
                                        v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 484 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(121, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 121 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 121 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 121 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 8, 1, 5):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_4)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused // 11)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 55, 55], "float32"], ["TENSOR", [16, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 5):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_2_i1_2_i2_2_i3_2_fused // 11 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[5, 1, 11, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 11, 1, 5])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 121, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 121])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #29: GFLOPs: 425.7285. Time: 0.0148 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #30: GFLOPs: 36.3128. Time: 0.1733 ms. Best GFLOPs: 1228.1753
[02:35:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_5"] Trial #31: GFLOPs: 291.0593. Time: 0.0216 ms. Best GFLOPs: 1228.1753
[02:36:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_conv2d_add_nn_relu_5"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 197
Total latency (us): 306.781

[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #0: GFLOPs: 498.7555. Time: 0.0250 ms. Best GFLOPs: 498.7555
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #1: GFLOPs: 9.5408. Time: 1.3088 ms. Best GFLOPs: 498.7555
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #2: GFLOPs: 646.1033. Time: 0.0193 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #3: GFLOPs: 14.0413. Time: 0.8893 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #4: GFLOPs: 411.1555. Time: 0.0304 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #5: GFLOPs: 285.1332. Time: 0.0438 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #6: GFLOPs: 91.0693. Time: 0.1371 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #7: GFLOPs: 15.4749. Time: 0.8069 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #8: GFLOPs: 87.5212. Time: 0.1427 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #9: GFLOPs: 559.5657. Time: 0.0223 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #10: GFLOPs: 461.8742. Time: 0.0270 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #11: GFLOPs: 622.3175. Time: 0.0201 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #12: GFLOPs: 46.2532. Time: 0.2700 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(11, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(44, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(50, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 11 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 25 * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_2_i1_2_i2_2_i3_2_fused % 25 // 5)
                            xx = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 55, 55], "float32"], ["TENSOR", [16, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(44):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(50, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 100 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 275)
                                        v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + (ax0_ax1_ax2_ax3_fused_0 * 100 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 275 // 55)
                                        v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 100 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(50, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 50 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 50 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 50 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 11 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 25 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_2_i1_2_i2_2_i3_2_fused % 25 // 5)
                                xx = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 55, 55], "float32"], ["TENSOR", [16, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 11 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 25 * 2 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 + ax2)
                            v3 = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i0_2_i1_2_i2_2_i3_2_fused % 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 2, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[11, 1, 5, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 11, 5, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 50, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 50])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #14: GFLOPs: 300.4029. Time: 0.0416 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #15: GFLOPs: 198.4712. Time: 0.0629 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #16: GFLOPs: 155.0799. Time: 0.0805 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(50, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init in T.grid(2, 11, 11, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 25 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 * 11 + i2_3_init)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 55, 55], "float32"], ["TENSOR", [16, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(121):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(50, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 100 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3025)
                                        v2 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 100 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3025 // 55)
                                        v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 100 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(50, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 11, 11, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 25 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 * 11 + i2_3)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 55, 55], "float32"], ["TENSOR", [16, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 11, 11):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 25 * 4 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 25 // 5 * 11 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 5, 11, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 5, 11, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 50, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 50, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #18: GFLOPs: 485.2607. Time: 0.0257 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #19: GFLOPs: 611.4981. Time: 0.0204 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #20: GFLOPs: 402.5076. Time: 0.0310 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #21: GFLOPs: 52.3079. Time: 0.2387 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #22: GFLOPs: 461.6095. Time: 0.0271 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #23: GFLOPs: 7.9160. Time: 1.5775 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #24: GFLOPs: 286.5312. Time: 0.0436 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #25: GFLOPs: 195.6552. Time: 0.0638 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #26: GFLOPs: 32.1675. Time: 0.3882 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #27: GFLOPs: 352.1972. Time: 0.0355 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #28: GFLOPs: 147.1914. Time: 0.0848 ms. Best GFLOPs: 646.1033
[02:36:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 55, 55), "float32"], placeholder_1: T.Buffer[(16, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(22, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(88, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(5, 5):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 11 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 11)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i2_4_init)
                            xx = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 55, 55], "float32"], ["TENSOR", [16, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(35):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(88, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0)
                                    v2 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) // 55)
                                    v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) % 55)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1 < 3025)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(88, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(128, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 16)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 5, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 11 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 11)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i2_4)
                                xx = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i3_3)
                                rc = T.axis.reduce(128, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 55, 55], "float32"], ["TENSOR", [16, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 5, 5):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 11 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 11 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + ax2)
                            v3 = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 11, 1, 5])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 11, 1, 5, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 88])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 88, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:36:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #30: GFLOPs: 694.8327. Time: 0.0180 ms. Best GFLOPs: 694.8327
[02:36:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_6"] Trial #31: GFLOPs: 56.8180. Time: 0.2198 ms. Best GFLOPs: 694.8327
[02:36:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_conv2d_add_nn_relu_6"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 229
Total latency (us): 324.753

[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #0: GFLOPs: 42.9149. Time: 0.1534 ms. Best GFLOPs: 42.9149
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(5, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(176, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(110, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(44):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 605)
                                        v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 605 // 55)
                                        v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(110, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 110 + ax0_ax1_ax2_ax3_fused_1 < 1024)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init in T.serial(2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 11 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 2 + i1_3_init)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_1_i1_1_i2_1_i3_1_fused % 11)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 11 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 2 + i1_3)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_1_i1_1_i2_1_i3_1_fused % 11)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55)
                                rc = T.axis.reduce(16, i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 11 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 2 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i0_1_i1_1_i2_1_i3_1_fused % 11 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 2, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[5, 11, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 110, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 110])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l176)
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #2: GFLOPs: 539.3421. Time: 0.0122 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #3: GFLOPs: 495.0857. Time: 0.0133 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #4: GFLOPs: 158.7633. Time: 0.0415 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #5: GFLOPs: 276.8648. Time: 0.0238 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #6: GFLOPs: 180.6114. Time: 0.0364 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #7: GFLOPs: 378.0574. Time: 0.0174 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(11, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(88, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i3_4_init in T.grid(4, 5, 2, 5):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 11 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i2_3_init)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(88, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) // 275)
                                    v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) % 275 // 55)
                                    v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) % 55)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1 < 1100)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(88, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 176 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 176 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 88 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 256)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 5, 1, 1, 1, 1, 1, 2, 1, 5):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 11 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i2_3)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 5, 5):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 11 * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 11 * 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[11, 1, 1, 5, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 11, 1, 5])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 88])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 88, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #9: GFLOPs: 83.2732. Time: 0.0790 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #10: GFLOPs: 94.3477. Time: 0.0698 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #11: GFLOPs: 22.3171. Time: 0.2949 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #12: GFLOPs: 256.1134. Time: 0.0257 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #13: GFLOPs: 245.1523. Time: 0.0269 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #14: GFLOPs: 178.6901. Time: 0.0368 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #15: GFLOPs: 192.0244. Time: 0.0343 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(176, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(4, 5):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 11 * 4 + i1_4_init)
                            yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i2_4_init)
                            xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(55):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0)
                                    v2 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) // 55)
                                    v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) % 55)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(16, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 5, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 11 * 4 + i1_4)
                                yy = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + i2_4)
                                xx, rc = T.axis.remap("SR", [i0_2_i1_2_i2_2_i3_2_fused, i4_0])
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 5, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 11 * 4 + ax1)
                            v2 = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 11 * 5 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 11, 1, 1, 5])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 55])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 55, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(11, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(220, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(8, 5):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + i1_3_init)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55)
                            xx = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 5 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(220, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 880 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3025)
                                        v2 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 880 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3025 // 55)
                                        v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 880 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 12100)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(220, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused_1 // 4)
                                    v1 = T.axis.spatial(16, i4_0 * 4 + ax0_ax1_ax2_ax3_fused_1 % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 128)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 1, 1, 5):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + i1_3)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55)
                                xx = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 5 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 5):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 55 * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 55 + ax2)
                            v3 = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused * 5 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 11, 1, 1, 5])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 220, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 220])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(11, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(5, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(35):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 275)
                                        v2 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 275 // 5)
                                        v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 5)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 4400)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 1, 1, 1):
                            for i1_4_init, i2_4_init in T.grid(2, 55):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                                    yy = T.axis.spatial(55, i2_4_init)
                                    xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_1_i1_1_i2_1_i3_1_fused)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 55, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    yy = T.axis.spatial(55, i2_4)
                                    xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_1_i1_1_i2_1_i3_1_fused)
                                    rc = T.axis.reduce(16, i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 55, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(55, ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 55])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[11, 5, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l183)
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #19: GFLOPs: 34.9566. Time: 0.1883 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #20: GFLOPs: 313.6360. Time: 0.0210 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(10, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(11, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(55, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(44):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 605)
                                        v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 605 // 55)
                                        v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 220 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(55, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 55 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init in T.serial(32):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i1_3_init)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_1_i1_1_i2_1_i3_1_fused)
                                xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 32, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i1_3)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_1_i1_1_i2_1_i3_1_fused)
                                xx, rc = T.axis.remap("SR", [i0_2_i1_2_i2_2_i3_2_fused, i4_1])
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 32, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[5, 11, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 55, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 55, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 55])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l176)
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(11, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(40, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(110):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) // 275)
                                    v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) % 275 // 55)
                                    v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) % 55)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 160 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 160 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 8, 1, 1):
                            for i2_4_init, i3_4_init in T.grid(5, 11):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 5 * 8 + i1_3)
                                    yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i2_4_init)
                                    xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + i3_4_init)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 5, 11):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 5 * 8 + i1_3)
                                    yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + i2_4)
                                    xx = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + i3_4)
                                    rc = T.axis.reduce(16, i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 5, 11):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 5 * 8 + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 5 + ax2)
                            v3 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused % 5 * 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[11, 1, 1, 1, 5])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 5, 1, 11])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 40])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 40, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l183)
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #23: GFLOPs: 60.1747. Time: 0.1094 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #24: GFLOPs: 512.4066. Time: 0.0128 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(5, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(10, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(11, 11):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 5 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i2_3_init)
                            xx = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(38):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 605)
                                    v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 605 // 55)
                                    v3 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 55)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 1210)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 1, 11):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 5 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + i2_3)
                                xx = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 11, 11):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 5 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused * 11 + ax2)
                            v3 = T.axis.spatial(55, i0_1_i1_1_i2_1_i3_1_fused % 5 * 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[5, 1, 1, 11, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 5, 1, 1, 11])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #26: GFLOPs: 201.0260. Time: 0.0327 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #27: GFLOPs: 189.8077. Time: 0.0347 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #28: GFLOPs: 346.1316. Time: 0.0190 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 55, 55), "float32"], placeholder_1: T.Buffer[(64, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 55, 55), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 55, 55], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 55, 55], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(10, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(121, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(2, 16, 5):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i1_3_init * 16 + i1_4_init)
                            yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused // 11 * 5 + i2_4_init)
                            xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(121, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 242 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 605)
                                        v2 = T.axis.spatial(55, (ax0_ax1_ax2_ax3_fused_0 * 242 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 605 // 11)
                                        v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + (ax0_ax1_ax2_ax3_fused_0 * 242 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 11)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(121, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 128)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 16, 5, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused // 11 * 5 + i2_4)
                                xx = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 55, 55], "float32"], ["TENSOR", [64, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 5, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 5 * 32 + ax1)
                            v2 = T.axis.spatial(55, i0_2_i1_2_i2_2_i3_2_fused // 11 * 5 + ax2)
                            v3 = T.axis.spatial(55, i0_0_i1_0_i2_0_i3_0_fused % 5 * 11 + i0_2_i1_2_i2_2_i3_2_fused % 11 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 11, 1, 5])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[5, 1, 11, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 121, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 121, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #30: GFLOPs: 104.3450. Time: 0.0631 ms. Best GFLOPs: 539.3421
[02:36:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_7"] Trial #31: GFLOPs: 533.7297. Time: 0.0123 ms. Best GFLOPs: 539.3421
[02:36:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_conv2d_add_nn_relu_7"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 261
Total latency (us): 349.162

[02:36:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate"] Trial #0: GFLOPs: 0.0000. Time: 0.0236 ms. Best GFLOPs: 0.0000
[02:36:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate"] Trial #1: GFLOPs: 0.0000. Time: 0.0174 ms. Best GFLOPs: 0.0000
[02:36:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate"] Trial #2: GFLOPs: 0.0000. Time: 0.0465 ms. Best GFLOPs: 0.0000
[02:36:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate"] Trial #3: GFLOPs: 0.0000. Time: 0.0104 ms. Best GFLOPs: 0.0000
[02:36:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate"] Trial #4: GFLOPs: 0.0000. Time: 0.0104 ms. Best GFLOPs: 0.0000
[02:36:44] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_concatenate"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 266
Total latency (us): 369.88

[02:36:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d_1"] Trial #0: GFLOPs: 136.5206. Time: 0.0062 ms. Best GFLOPs: 136.5206
[02:36:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d_1"] Trial #1: GFLOPs: 17.3330. Time: 0.0485 ms. Best GFLOPs: 136.5206
[02:36:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d_1"] Trial #2: GFLOPs: 91.6346. Time: 0.0092 ms. Best GFLOPs: 136.5206
[02:36:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d_1"] Trial #3: GFLOPs: 115.3083. Time: 0.0073 ms. Best GFLOPs: 136.5206
[02:36:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d_1"] Trial #4: GFLOPs: 20.5033. Time: 0.0410 ms. Best GFLOPs: 136.5206
[02:37:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_max_pool2d_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 271
Total latency (us): 376.032

[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #0: GFLOPs: 403.0030. Time: 0.0149 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #1: GFLOPs: 116.4211. Time: 0.0517 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #2: GFLOPs: 54.9603. Time: 0.1095 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #3: GFLOPs: 321.0209. Time: 0.0187 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #4: GFLOPs: 3.5468. Time: 1.6969 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #5: GFLOPs: 78.2687. Time: 0.0769 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(9, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(3, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 9 * 8 + i1_4_init)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused // 3 * 9 + i0_1_i1_1_i2_1_i3_1_fused * 3 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 81)
                                        v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused // 3 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 81 // 9)
                                        v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 8, 1, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 9 * 8 + i1_4)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused // 3 * 9 + i0_1_i1_1_i2_1_i3_1_fused * 3 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 9 * 8 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused // 3 * 9 + i0_1_i1_1_i2_1_i3_1_fused * 3 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 3, 3, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 1, 3, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 36, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 36])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(3, 2, 3, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 3 * 2 + i1_4_init)
                            yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i2_3_init * 3 + i2_4_init)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 243)
                                        v2 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 243 // 9)
                                        v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3888)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 512)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 3, 1, 16, 1, 1, 1, 2, 3, 3):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 3 * 2 + i1_4)
                                yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i2_3 * 3 + i2_4)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 9, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 3 * 2 + ax1)
                            v2 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 3, 1, 3, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 1, 3, 1, 3])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 48, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 48, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #8: GFLOPs: 61.4572. Time: 0.0979 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 9, 27):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 3 * 2 + i1_3_init)
                            yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + i2_3_init)
                            xx = T.axis.spatial(27, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 729)
                                        v2 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 729 // 27)
                                        v3 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1458)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 9, 27, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 3 * 2 + i1_3)
                                yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + i2_3)
                                xx = T.axis.spatial(27, i3_3)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 9, 27):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 3 * 2 + ax1)
                            v2 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + ax2)
                            v3 = T.axis.spatial(27, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 3, 9, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 27, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 48, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 48, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #10: GFLOPs: 362.7854. Time: 0.0166 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(36, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(54, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(54, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 81)
                                        v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 // 3 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 81 // 9)
                                        v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(54, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 9 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 128)
                                        v1 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 128)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 54 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init in T.grid(3, 4):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 9 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 4 + i1_4_init)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 // 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 3)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 3, 16, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 9 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 4 + i1_4)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 // 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 3)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_3)
                                rc = T.axis.reduce(128, i4_1 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 9 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 4 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 // 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 3 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 2, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 9, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 1, 3, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 8, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 54, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 54, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l178)
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #12: GFLOPs: 67.7785. Time: 0.0888 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #13: GFLOPs: 95.1436. Time: 0.0633 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #14: GFLOPs: 59.9080. Time: 0.1005 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #15: GFLOPs: 79.3256. Time: 0.0759 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #16: GFLOPs: 161.5944. Time: 0.0372 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #17: GFLOPs: 6.9082. Time: 0.8712 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #18: GFLOPs: 161.3571. Time: 0.0373 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(81, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(16, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 3 * 16 + i1_3_init)
                            yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused // 9 * 3 + i2_3_init)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(81, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 162 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 243)
                                        v2 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 162 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 243 // 9)
                                        v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 162 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(81, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 3 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(128, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 81 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 512)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 16, 3, 1, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 3 * 16 + i1_3)
                                yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused // 9 * 3 + i2_3)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9)
                                rc = T.axis.reduce(128, i4_0 * 32 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 3, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 3 * 16 + ax1)
                            v2 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused // 9 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 9, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 1, 9, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 81, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 81, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #20: GFLOPs: 251.2725. Time: 0.0240 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #21: GFLOPs: 24.7922. Time: 0.2428 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #22: GFLOPs: 119.6905. Time: 0.0503 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #23: GFLOPs: 105.4888. Time: 0.0571 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #24: GFLOPs: 71.2763. Time: 0.0844 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 2, 3, 9):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 9 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + i2_4_init)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(324):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) // 729)
                                    v2 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 729 // 27)
                                    v3 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 27)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 3, 9):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 9 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + i2_4)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 3, 9):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 9 * 8 + ax1)
                            v2 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 3, 3, 1, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 3, 1, 9])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 36])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 36])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(9, 3, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 2 + i1_4_init)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i2_3_init)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(27):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) // 243)
                                    v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 243 // 27)
                                    v3 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 27)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1 < 128)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 9, 3, 4, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 2 + i1_4)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i2_3)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 27, 27], "float32"], ["TENSOR", [32, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 9, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 9 * 2 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 4, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 1, 9, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 9, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 36])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 36])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #27: GFLOPs: 87.3663. Time: 0.0689 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #28: GFLOPs: 12.7844. Time: 0.4708 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #29: GFLOPs: 42.2107. Time: 0.1426 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #30: GFLOPs: 6.8254. Time: 0.8818 ms. Best GFLOPs: 403.0030
[02:37:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_8"] Trial #31: GFLOPs: 252.4667. Time: 0.0238 ms. Best GFLOPs: 403.0030
[02:37:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_conv2d_add_nn_relu_8"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 303
Total latency (us): 390.966

[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #0: GFLOPs: 277.9935. Time: 0.0431 ms. Best GFLOPs: 277.9935
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(27, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(72, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(4, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 9 * 4 + i1_4_init)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 27)
                                        v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused)
                                        v3 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 64)
                                        v1 = T.axis.spatial(256, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 64)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2048)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 3):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 9 * 4 + i1_4)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 64 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 9 * 4 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[27, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 9, 1, 3])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 16, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 72, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 72, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #2: GFLOPs: 48.2845. Time: 0.2483 ms. Best GFLOPs: 277.9935
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #3: GFLOPs: 409.2493. Time: 0.0293 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #4: GFLOPs: 129.0892. Time: 0.0929 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #5: GFLOPs: 94.6625. Time: 0.1267 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #6: GFLOPs: 150.5704. Time: 0.0796 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #7: GFLOPs: 39.9277. Time: 0.3003 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(72, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(54, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(3, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 9 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 2 + i1_4_init)
                            yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27 // 3 * 3 + i2_3_init)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 * 3 + i0_2_i1_2_i2_2_i3_2_fused % 3)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(54, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 81)
                                        v2 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 81 // 3)
                                        v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(54, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 9 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 54 + ax0_ax1_ax2_ax3_fused_1) // 64)
                                    v1 = T.axis.spatial(256, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 54 + ax0_ax1_ax2_ax3_fused_1) % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 54 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 9 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 2 + i1_4)
                                yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27 // 3 * 3 + i2_3)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 * 3 + i0_2_i1_2_i2_2_i3_2_fused % 3)
                                rc = T.axis.reduce(256, i4_0 * 64 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 3, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 9 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 2 + ax1)
                            v2 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27 // 3 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 9 * 3 + i0_2_i1_2_i2_2_i3_2_fused % 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 2, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 9, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[9, 1, 3, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 64, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 54, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 54])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #9: GFLOPs: 217.0788. Time: 0.0552 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #10: GFLOPs: 13.3168. Time: 0.9004 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #11: GFLOPs: 229.0024. Time: 0.0524 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #12: GFLOPs: 7.4895. Time: 1.6010 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #13: GFLOPs: 313.6238. Time: 0.0382 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #14: GFLOPs: 154.1260. Time: 0.0778 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #15: GFLOPs: 344.5614. Time: 0.0348 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #16: GFLOPs: 254.2039. Time: 0.0472 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(72, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(3, 9):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 9)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + i2_3_init)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 243)
                                        v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 243 // 27)
                                        v3 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1944)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 256)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 3, 1, 4, 1, 1, 1, 1, 1, 9):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 9)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + i2_3)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 3, 9):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 9 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 // 3 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 3 * 9 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 3, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 3, 1, 9])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 72, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 72, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #18: GFLOPs: 154.9937. Time: 0.0774 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #19: GFLOPs: 383.0554. Time: 0.0313 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(81, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 3, 16, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i1_3_init * 16 + i1_4_init)
                            yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused // 9 * 3 + i2_4_init)
                            xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(81, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 729)
                                        v2 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 729 // 27)
                                        v3 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 324 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(81, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 128)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 3, 4, 1, 1, 1, 16, 3, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused // 9 * 3 + i2_4)
                                xx = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 3, 3):
                        with T.block("conv2d_nchw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused // 9 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 9 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 9, 1, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 9, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 81, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 81, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #21: GFLOPs: 318.1145. Time: 0.0377 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #22: GFLOPs: 73.4423. Time: 0.1633 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #23: GFLOPs: 270.8457. Time: 0.0443 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(9, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(96, thread="threadIdx.x"):
                    for i2_3_init, i2_4_init, i3_4_init in T.grid(3, 3, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 3)
                            yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused // 3 * 9 + i2_3_init * 3 + i2_4_init)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 81)
                                        v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused // 3 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 81 // 9)
                                        v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2592)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(256, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 3)
                                yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused // 3 * 9 + i2_3 * 3 + i2_4)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_4)
                                rc = T.axis.reduce(256, i4_0 * 32 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 9, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 3 + ax1)
                            v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused // 3 * 9 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 1, 3, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 1, 3, 1, 3])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 96, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 96, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #25: GFLOPs: 320.7121. Time: 0.0374 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], placeholder_1: T.Buffer[(32, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(54, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init in T.grid(4, 3, 3, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27 // 3 * 3 + i2_3_init)
                            xx = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(54):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(54, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 729)
                                        v2 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 729 // 27)
                                        v3 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(54, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 216 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 54 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 256)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 4, 3, 3, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27 // 3 * 3 + i2_3)
                                xx = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_3)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 27, 27], "float32"], ["TENSOR", [32, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 3, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 27 * 8 + ax1)
                            v2 = T.axis.spatial(27, i0_2_i1_2_i2_2_i3_2_fused % 27 // 3 * 3 + ax2)
                            v3 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 9, 3, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 3, 3, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 54, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 54, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #27: GFLOPs: 46.6428. Time: 0.2571 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #28: GFLOPs: 79.2313. Time: 0.1513 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #29: GFLOPs: 26.1366. Time: 0.4588 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #30: GFLOPs: 171.3531. Time: 0.0700 ms. Best GFLOPs: 409.2493
[02:37:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_9"] Trial #31: GFLOPs: 16.0182. Time: 0.7486 ms. Best GFLOPs: 409.2493
[02:37:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_conv2d_add_nn_relu_9"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 335
Total latency (us): 420.265

[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #0: GFLOPs: 12.5618. Time: 0.4903 ms. Best GFLOPs: 12.5618
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #1: GFLOPs: 874.3239. Time: 0.0070 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #2: GFLOPs: 534.1711. Time: 0.0115 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #3: GFLOPs: 109.3004. Time: 0.0563 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #4: GFLOPs: 170.2091. Time: 0.0362 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #5: GFLOPs: 78.9403. Time: 0.0780 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #6: GFLOPs: 154.6833. Time: 0.0398 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #7: GFLOPs: 55.7254. Time: 0.1105 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #8: GFLOPs: 101.8064. Time: 0.0605 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #9: GFLOPs: 139.5638. Time: 0.0441 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(54, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(2, 32, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 27 * 64 + i1_3_init * 32 + i1_4_init)
                            yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 3)
                            xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(54, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 243)
                                        v2 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 243 // 9)
                                        v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(54, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 54 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 54 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 54 + ax0_ax1_ax2_ax3_fused_1 < 1024)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 32, 1, 3):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 27 * 64 + i1_3 * 32 + i1_4)
                                yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 3)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 1, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 27 * 64 + ax1)
                            v2 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 27 // 3 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 3, 9, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 1, 3, 1, 3])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 54, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 54])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #11: GFLOPs: 151.6103. Time: 0.0406 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #12: GFLOPs: 401.2370. Time: 0.0153 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #13: GFLOPs: 153.8810. Time: 0.0400 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #14: GFLOPs: 47.0982. Time: 0.1308 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #15: GFLOPs: 433.9158. Time: 0.0142 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #16: GFLOPs: 101.1202. Time: 0.0609 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #17: GFLOPs: 24.0658. Time: 0.2559 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #18: GFLOPs: 632.8257. Time: 0.0097 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #19: GFLOPs: 628.7901. Time: 0.0098 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #20: GFLOPs: 440.4392. Time: 0.0140 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #21: GFLOPs: 720.7133. Time: 0.0085 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #22: GFLOPs: 365.9932. Time: 0.0168 ms. Best GFLOPs: 874.3239
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #23: GFLOPs: 908.6679. Time: 0.0068 ms. Best GFLOPs: 908.6679
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 27, 27), "float32"], placeholder_1: T.Buffer[(128, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 27, 27], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 27, 27], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(108):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 243)
                                        v2 = T.axis.spatial(27, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 243 // 9)
                                        v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(29):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 4096)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i3_3_init, i1_4_init in T.grid(2, 9, 16):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 9 * 32 + i1_3_init * 16 + i1_4_init)
                                yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 1, 9, 4, 1, 1, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 9 * 32 + i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9)
                                xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + i3_3)
                                rc = T.axis.reduce(32, i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 27, 27], "float32"], ["TENSOR", [128, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 9):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 9 * 32 + ax1)
                            v2 = T.axis.spatial(27, i0_1_i1_1_i2_1_i3_1_fused * 9 + i0_2_i1_2_i2_2_i3_2_fused % 9 + ax2)
                            v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused * 9 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 3, 9, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 1, 1, 9, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 8, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 36, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 36, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l178)
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #25: GFLOPs: 275.9237. Time: 0.0223 ms. Best GFLOPs: 908.6679
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #26: GFLOPs: 58.1243. Time: 0.1060 ms. Best GFLOPs: 908.6679
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #27: GFLOPs: 65.4323. Time: 0.0941 ms. Best GFLOPs: 908.6679
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #28: GFLOPs: 109.5671. Time: 0.0562 ms. Best GFLOPs: 908.6679
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #29: GFLOPs: 454.8950. Time: 0.0135 ms. Best GFLOPs: 908.6679
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #30: GFLOPs: 41.4790. Time: 0.1485 ms. Best GFLOPs: 908.6679
[02:37:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_10"] Trial #31: GFLOPs: 339.8039. Time: 0.0181 ms. Best GFLOPs: 908.6679
[02:37:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_conv2d_add_nn_relu_10"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 367
Total latency (us): 433.82

[02:37:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_concatenate_1"] Trial #0: GFLOPs: 0.0000. Time: 0.0235 ms. Best GFLOPs: 0.0000
[02:37:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_concatenate_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0181 ms. Best GFLOPs: 0.0000
[02:37:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_concatenate_1"] Trial #2: GFLOPs: 0.0000. Time: 0.0215 ms. Best GFLOPs: 0.0000
[02:37:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_concatenate_1"] Trial #3: GFLOPs: 0.0000. Time: 0.0260 ms. Best GFLOPs: 0.0000
[02:37:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_concatenate_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0349 ms. Best GFLOPs: 0.0000
[02:37:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_concatenate_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 372
Total latency (us): 470.093

[02:37:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_max_pool2d_2"] Trial #0: GFLOPs: 16.5634. Time: 0.0235 ms. Best GFLOPs: 16.5634
[02:37:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_max_pool2d_2"] Trial #1: GFLOPs: 10.0998. Time: 0.0386 ms. Best GFLOPs: 16.5634
[02:37:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_max_pool2d_2"] Trial #2: GFLOPs: 115.9578. Time: 0.0034 ms. Best GFLOPs: 115.9578
[02:37:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_max_pool2d_2"] Trial #3: GFLOPs: 107.3807. Time: 0.0036 ms. Best GFLOPs: 115.9578
[02:37:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_max_pool2d_2"] Trial #4: GFLOPs: 115.4565. Time: 0.0034 ms. Best GFLOPs: 115.9578
[02:37:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_max_pool2d_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 377
Total latency (us): 473.451

[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #0: GFLOPs: 131.8815. Time: 0.0316 ms. Best GFLOPs: 131.8815
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #1: GFLOPs: 47.8766. Time: 0.0871 ms. Best GFLOPs: 131.8815
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], placeholder_1: T.Buffer[(48, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1), "float32"], T_relu: T.Buffer[(1, 48, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([48, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init in T.serial(3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 13 * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + i1_3_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 13, 13], "float32"], ["TENSOR", [48, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 13)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(60):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 13 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 128)
                                    v1 = T.axis.spatial(256, i4_0 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 128)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 3072)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 3, 1, 1, 32, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 13 * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + i1_3)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                rc = T.axis.reduce(256, i4_0 * 128 + i4_1 * 32 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 13, 13], "float32"], ["TENSOR", [48, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 13 * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 3 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 4, 3, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 52])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #3: GFLOPs: 42.4770. Time: 0.0982 ms. Best GFLOPs: 131.8815
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #4: GFLOPs: 23.9118. Time: 0.1744 ms. Best GFLOPs: 131.8815
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #5: GFLOPs: 147.2106. Time: 0.0283 ms. Best GFLOPs: 147.2106
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #6: GFLOPs: 41.5293. Time: 0.1004 ms. Best GFLOPs: 147.2106
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #7: GFLOPs: 82.7713. Time: 0.0504 ms. Best GFLOPs: 147.2106
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #8: GFLOPs: 58.8094. Time: 0.0709 ms. Best GFLOPs: 147.2106
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #9: GFLOPs: 178.1832. Time: 0.0234 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #10: GFLOPs: 67.9063. Time: 0.0614 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #11: GFLOPs: 144.5677. Time: 0.0288 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #12: GFLOPs: 68.3449. Time: 0.0610 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #13: GFLOPs: 64.8121. Time: 0.0643 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #14: GFLOPs: 53.6286. Time: 0.0777 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #15: GFLOPs: 43.1756. Time: 0.0966 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #16: GFLOPs: 162.9309. Time: 0.0256 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #17: GFLOPs: 109.2016. Time: 0.0382 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #18: GFLOPs: 64.0899. Time: 0.0651 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #19: GFLOPs: 98.9639. Time: 0.0421 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #20: GFLOPs: 127.0346. Time: 0.0328 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #21: GFLOPs: 83.6083. Time: 0.0499 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #22: GFLOPs: 43.4991. Time: 0.0959 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #23: GFLOPs: 80.5598. Time: 0.0518 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #24: GFLOPs: 122.9206. Time: 0.0339 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #25: GFLOPs: 134.7482. Time: 0.0309 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #26: GFLOPs: 141.3881. Time: 0.0295 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #27: GFLOPs: 26.6657. Time: 0.1564 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #28: GFLOPs: 79.9178. Time: 0.0522 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #29: GFLOPs: 129.6944. Time: 0.0321 ms. Best GFLOPs: 178.1832
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #30: GFLOPs: 205.0041. Time: 0.0203 ms. Best GFLOPs: 205.0041
[02:37:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_11"] Trial #31: GFLOPs: 209.2465. Time: 0.0199 ms. Best GFLOPs: 209.2465
[02:38:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_conv2d_add_nn_relu_11"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 409
Total latency (us): 493.378

[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #0: GFLOPs: 131.3582. Time: 0.0476 ms. Best GFLOPs: 131.3582
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #1: GFLOPs: 63.5792. Time: 0.0982 ms. Best GFLOPs: 131.3582
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #2: GFLOPs: 52.3282. Time: 0.1194 ms. Best GFLOPs: 131.3582
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(48, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1), "float32"], T_relu: T.Buffer[(1, 48, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([48, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(156, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(4, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(48, i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [48, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(35):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(156, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1) // 169)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 < 5408)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(156, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(48, (ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(384, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 1, 1, 8, 1, 1, 1, 1, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(48, i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i3_4)
                                rc = T.axis.reduce(384, i4_0 * 32 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [48, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(48, i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 12, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 4, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 156])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 156, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #4: GFLOPs: 145.5326. Time: 0.0429 ms. Best GFLOPs: 145.5326
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #5: GFLOPs: 83.6781. Time: 0.0746 ms. Best GFLOPs: 145.5326
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #6: GFLOPs: 11.5963. Time: 0.5386 ms. Best GFLOPs: 145.5326
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #7: GFLOPs: 148.4633. Time: 0.0421 ms. Best GFLOPs: 148.4633
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #8: GFLOPs: 244.7019. Time: 0.0255 ms. Best GFLOPs: 244.7019
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #9: GFLOPs: 78.9691. Time: 0.0791 ms. Best GFLOPs: 244.7019
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #10: GFLOPs: 100.4889. Time: 0.0622 ms. Best GFLOPs: 244.7019
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #11: GFLOPs: 62.6772. Time: 0.0997 ms. Best GFLOPs: 244.7019
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #12: GFLOPs: 162.6108. Time: 0.0384 ms. Best GFLOPs: 244.7019
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #13: GFLOPs: 137.7221. Time: 0.0454 ms. Best GFLOPs: 244.7019
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #14: GFLOPs: 358.7259. Time: 0.0174 ms. Best GFLOPs: 358.7259
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #15: GFLOPs: 403.1107. Time: 0.0155 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #16: GFLOPs: 90.2053. Time: 0.0692 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #17: GFLOPs: 322.1402. Time: 0.0194 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #18: GFLOPs: 94.1793. Time: 0.0663 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #19: GFLOPs: 168.4737. Time: 0.0371 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #20: GFLOPs: 113.7733. Time: 0.0549 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #21: GFLOPs: 147.0232. Time: 0.0425 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #22: GFLOPs: 194.6537. Time: 0.0321 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #23: GFLOPs: 88.6470. Time: 0.0705 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(48, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1), "float32"], T_relu: T.Buffer[(1, 48, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([48, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(39, thread="threadIdx.x"):
                    for i1_3_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [48, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(70):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(39, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i4_0 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 117 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 169)
                                        v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 117 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 169 // 13)
                                        v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 117 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 8112)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(39, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused * 24 + (ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) // 48)
                                    v1 = T.axis.spatial(384, i4_0 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) % 48)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1 < 1152)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 1, 1, 12, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                rc = T.axis.reduce(384, i4_0 * 48 + i4_1 * 12 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [48, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 3, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 12])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 39, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 39])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #25: GFLOPs: 121.9566. Time: 0.0512 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #26: GFLOPs: 137.3676. Time: 0.0455 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #27: GFLOPs: 195.5240. Time: 0.0319 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #28: GFLOPs: 320.2403. Time: 0.0195 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #29: GFLOPs: 160.4587. Time: 0.0389 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #30: GFLOPs: 65.8441. Time: 0.0949 ms. Best GFLOPs: 403.1107
[02:38:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_relu_12"] Trial #31: GFLOPs: 52.7357. Time: 0.1184 ms. Best GFLOPs: 403.1107
[02:38:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_conv2d_add_nn_relu_12"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 441
Total latency (us): 508.873

[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #0: GFLOPs: 458.3667. Time: 0.0069 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #1: GFLOPs: 80.6172. Time: 0.0394 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(78, thread="threadIdx.x"):
                    for i1_3_init in T.serial(16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(78, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 13)
                                        v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(60):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(78, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 78 + ax0_ax1_ax2_ax3_fused_1) // 24)
                                    v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 78 + ax0_ax1_ax2_ax3_fused_1) % 24)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 78 + ax0_ax1_ax2_ax3_fused_1 < 4608)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 16, 1, 1, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused)
                                rc = T.axis.reduce(48, i4_0 * 24 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 6, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 6, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 78, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 78])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #3: GFLOPs: 8.0070. Time: 0.3971 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #4: GFLOPs: 130.3233. Time: 0.0244 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #5: GFLOPs: 435.1878. Time: 0.0073 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #6: GFLOPs: 102.9974. Time: 0.0309 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #7: GFLOPs: 84.7324. Time: 0.0375 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #8: GFLOPs: 290.4189. Time: 0.0109 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #9: GFLOPs: 184.5920. Time: 0.0172 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #10: GFLOPs: 89.2999. Time: 0.0356 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #11: GFLOPs: 86.5759. Time: 0.0367 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #12: GFLOPs: 117.6139. Time: 0.0270 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #13: GFLOPs: 5.9843. Time: 0.5314 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(39, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 169)
                                        v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 169 // 13)
                                        v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 4056)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4608)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(12, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 24 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 4, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 12, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(169, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 24):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 48 + i1_3_init * 24 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(169, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) // 169)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(169, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 24, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 48 + i1_3 * 24 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 12 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 48, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 48 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 2, 24])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 6, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 169])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 169, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(3, 8, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 24 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(13, i2_4_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(48, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 169)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 507)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(48, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 8, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 24 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(13, i2_4)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 3 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 24, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 24 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 3, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #17: GFLOPs: 97.8271. Time: 0.0325 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #18: GFLOPs: 41.6935. Time: 0.0763 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #19: GFLOPs: 285.2160. Time: 0.0111 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(169, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(4, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i1_3_init * 3 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(6, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(169, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 169)
                                        v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 169 // 13)
                                        v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(169, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + (ax0_ax1_ax2_ax3_fused_0 * 507 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(48, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 507 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 3, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i1_3 * 3 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 12, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused * 12 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 1, 4, 3])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[6, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 169, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 169, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(4, 13, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 169)
                                        v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 169 // 13)
                                        v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2028)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 4, 13, 1, 4, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 12 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 4, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 3, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #22: GFLOPs: 8.2806. Time: 0.3840 ms. Best GFLOPs: 458.3667
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #23: GFLOPs: 564.6165. Time: 0.0056 ms. Best GFLOPs: 564.6165
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #24: GFLOPs: 214.7780. Time: 0.0148 ms. Best GFLOPs: 564.6165
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #25: GFLOPs: 204.6348. Time: 0.0155 ms. Best GFLOPs: 564.6165
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #26: GFLOPs: 140.6064. Time: 0.0226 ms. Best GFLOPs: 564.6165
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #27: GFLOPs: 263.7649. Time: 0.0121 ms. Best GFLOPs: 564.6165
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(39, thread="threadIdx.x"):
                    for i1_3_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(104):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(39, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) // 169)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(39, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + (ax0_ax1_ax2_ax3_fused_0 * 78 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(48, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 78 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 39 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 12, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 24 + i4_1 * 12 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 3, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 2, 12])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 39])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 39, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #29: GFLOPs: 15.3667. Time: 0.2069 ms. Best GFLOPs: 564.6165
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #30: GFLOPs: 190.9920. Time: 0.0166 ms. Best GFLOPs: 564.6165
[02:38:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_13"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 13, 13), "float32"], placeholder_1: T.Buffer[(192, 48, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 48, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 48, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init in T.grid(4, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(13, i2_4_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 169)
                                        v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 169 // 13)
                                        v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2028)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 12)
                                    v1 = T.axis.spatial(48, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 12)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 768)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 6, 1, 1, 1, 1, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(13, i2_4)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(48, i4_0 * 12 + i4_1 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 48, 13, 13], "float32"], ["TENSOR", [192, 48, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 4, 4, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 6])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 52])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:38:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_conv2d_add_nn_relu_13"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 473
Total latency (us): 520.137

[02:38:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_concatenate_2"] Trial #0: GFLOPs: 0.0000. Time: 0.0602 ms. Best GFLOPs: 0.0000
[02:38:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_concatenate_2"] Trial #1: GFLOPs: 0.0000. Time: 0.0406 ms. Best GFLOPs: 0.0000
[02:38:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_concatenate_2"] Trial #2: GFLOPs: 0.0000. Time: 0.0059 ms. Best GFLOPs: 0.0000
[02:38:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_concatenate_2"] Trial #3: GFLOPs: 0.0000. Time: 0.0051 ms. Best GFLOPs: 0.0000
[02:38:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_concatenate_2"] Trial #4: GFLOPs: 0.0000. Time: 0.0220 ms. Best GFLOPs: 0.0000
[02:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_concatenate_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 478
Total latency (us): 530.335

[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #0: GFLOPs: 292.9282. Time: 0.0284 ms. Best GFLOPs: 292.9282
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #1: GFLOPs: 115.0173. Time: 0.0724 ms. Best GFLOPs: 292.9282
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #2: GFLOPs: 423.5929. Time: 0.0197 ms. Best GFLOPs: 423.5929
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #3: GFLOPs: 804.0702. Time: 0.0104 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #4: GFLOPs: 113.5402. Time: 0.0734 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #5: GFLOPs: 261.1225. Time: 0.0319 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #6: GFLOPs: 131.4782. Time: 0.0633 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #7: GFLOPs: 223.2448. Time: 0.0373 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #8: GFLOPs: 132.2474. Time: 0.0630 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #9: GFLOPs: 58.8002. Time: 0.1416 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #10: GFLOPs: 127.9264. Time: 0.0651 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #11: GFLOPs: 158.4306. Time: 0.0526 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #12: GFLOPs: 349.5498. Time: 0.0238 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #13: GFLOPs: 63.8429. Time: 0.1305 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #14: GFLOPs: 329.0374. Time: 0.0253 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #15: GFLOPs: 461.7122. Time: 0.0180 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #16: GFLOPs: 647.8898. Time: 0.0129 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #17: GFLOPs: 509.3779. Time: 0.0163 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #18: GFLOPs: 143.4651. Time: 0.0581 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #19: GFLOPs: 302.9722. Time: 0.0275 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #20: GFLOPs: 724.4294. Time: 0.0115 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #21: GFLOPs: 268.3544. Time: 0.0310 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init in T.grid(8, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3_init)
                            yy = T.axis.spatial(13, i2_4_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) // 169)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(384, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2048)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 8, 1, 1, 4, 1, 1, 1, 1, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3)
                                yy = T.axis.spatial(13, i2_4)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(384, i4_0 * 32 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 8, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 104])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 104, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #23: GFLOPs: 110.7200. Time: 0.0752 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #24: GFLOPs: 288.5651. Time: 0.0289 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #25: GFLOPs: 18.2662. Time: 0.4559 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #26: GFLOPs: 102.0016. Time: 0.0816 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #27: GFLOPs: 378.8162. Time: 0.0220 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #28: GFLOPs: 402.6467. Time: 0.0207 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #29: GFLOPs: 291.2938. Time: 0.0286 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #30: GFLOPs: 92.2304. Time: 0.0903 ms. Best GFLOPs: 804.0702
[02:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_nn_relu_14"] Trial #31: GFLOPs: 260.8642. Time: 0.0319 ms. Best GFLOPs: 804.0702
[02:39:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_conv2d_add_nn_relu_14"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 510
Total latency (us): 540.693

[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #0: GFLOPs: 232.6503. Time: 0.0477 ms. Best GFLOPs: 232.6503
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #1: GFLOPs: 286.4310. Time: 0.0387 ms. Best GFLOPs: 286.4310
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #2: GFLOPs: 287.8313. Time: 0.0386 ms. Best GFLOPs: 287.8313
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #3: GFLOPs: 292.4123. Time: 0.0380 ms. Best GFLOPs: 292.4123
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #4: GFLOPs: 55.0293. Time: 0.2017 ms. Best GFLOPs: 292.4123
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #5: GFLOPs: 75.7024. Time: 0.1466 ms. Best GFLOPs: 292.4123
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #6: GFLOPs: 90.7479. Time: 0.1223 ms. Best GFLOPs: 292.4123
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #7: GFLOPs: 78.7593. Time: 0.1409 ms. Best GFLOPs: 292.4123
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #8: GFLOPs: 47.7606. Time: 0.2324 ms. Best GFLOPs: 292.4123
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #9: GFLOPs: 47.7482. Time: 0.2324 ms. Best GFLOPs: 292.4123
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #10: GFLOPs: 125.4709. Time: 0.0884 ms. Best GFLOPs: 292.4123
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #11: GFLOPs: 70.7317. Time: 0.1569 ms. Best GFLOPs: 292.4123
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #12: GFLOPs: 285.4200. Time: 0.0389 ms. Best GFLOPs: 292.4123
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #13: GFLOPs: 457.7618. Time: 0.0242 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #14: GFLOPs: 143.4453. Time: 0.0774 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #15: GFLOPs: 55.9460. Time: 0.1984 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #16: GFLOPs: 146.6470. Time: 0.0757 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #17: GFLOPs: 375.1790. Time: 0.0296 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #18: GFLOPs: 15.6744. Time: 0.7080 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #19: GFLOPs: 314.0153. Time: 0.0353 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #20: GFLOPs: 114.9746. Time: 0.0965 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #21: GFLOPs: 137.7235. Time: 0.0806 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #22: GFLOPs: 97.4595. Time: 0.1139 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #23: GFLOPs: 146.6185. Time: 0.0757 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #24: GFLOPs: 77.1440. Time: 0.1439 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], placeholder_1: T.Buffer[(64, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(169, thread="threadIdx.x"):
                    for i1_4_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 13, 13], "float32"], ["TENSOR", [64, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(169, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 169)
                                        v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 169 // 13)
                                        v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(169, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) // 64)
                                    v1 = T.axis.spatial(512, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1 < 1024)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(512, i4_0 * 64 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 13, 13], "float32"], ["TENSOR", [64, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 64, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 169, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 169])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #26: GFLOPs: 50.1578. Time: 0.2212 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #27: GFLOPs: 9.6584. Time: 1.1490 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #28: GFLOPs: 373.1805. Time: 0.0297 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #29: GFLOPs: 30.8926. Time: 0.3592 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #30: GFLOPs: 121.2986. Time: 0.0915 ms. Best GFLOPs: 457.7618
[02:39:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_15"] Trial #31: GFLOPs: 294.9485. Time: 0.0376 ms. Best GFLOPs: 457.7618
[02:39:37] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_nn_conv2d_add_nn_relu_15"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 542
Total latency (us): 564.935

[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #0: GFLOPs: 30.6235. Time: 0.1837 ms. Best GFLOPs: 30.6235
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #1: GFLOPs: 218.9529. Time: 0.0257 ms. Best GFLOPs: 218.9529
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #2: GFLOPs: 46.2235. Time: 0.1217 ms. Best GFLOPs: 218.9529
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(13, 13, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(43):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 169)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 1352)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 13, 13, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_3, i3_3])
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #4: GFLOPs: 105.9457. Time: 0.0531 ms. Best GFLOPs: 218.9529
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(4, 13, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(43):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 169)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 1352)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 13, 13, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                yy, xx = T.axis.remap("SS", [i2_3, i3_3])
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 32, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #6: GFLOPs: 528.9112. Time: 0.0106 ms. Best GFLOPs: 528.9112
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #7: GFLOPs: 231.9046. Time: 0.0243 ms. Best GFLOPs: 528.9112
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #8: GFLOPs: 640.8462. Time: 0.0088 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #9: GFLOPs: 83.6705. Time: 0.0672 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #10: GFLOPs: 401.3119. Time: 0.0140 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #11: GFLOPs: 58.9293. Time: 0.0954 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #12: GFLOPs: 297.1067. Time: 0.0189 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #13: GFLOPs: 216.6890. Time: 0.0260 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #14: GFLOPs: 61.9732. Time: 0.0908 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #15: GFLOPs: 263.7782. Time: 0.0213 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #16: GFLOPs: 85.5149. Time: 0.0658 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #17: GFLOPs: 251.0860. Time: 0.0224 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #18: GFLOPs: 472.4246. Time: 0.0119 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #19: GFLOPs: 571.6737. Time: 0.0098 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #20: GFLOPs: 104.1289. Time: 0.0540 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #21: GFLOPs: 101.0256. Time: 0.0557 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_4_init in T.serial(64):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 13 * 64 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 169)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 64, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 13 * 64 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 13 * 64 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 64])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #23: GFLOPs: 57.6367. Time: 0.0976 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_4_init in T.serial(16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 169)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 256)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 4, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #25: GFLOPs: 228.5492. Time: 0.0246 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #26: GFLOPs: 73.4114. Time: 0.0766 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #27: GFLOPs: 255.4695. Time: 0.0220 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #28: GFLOPs: 137.8993. Time: 0.0408 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(208, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_3_init)
                            yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(26):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 169)
                                        v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 169 // 13)
                                        v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2048)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_3)
                                yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 13, 13], "float32"], ["TENSOR", [256, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 16, 4, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #30: GFLOPs: 32.9535. Time: 0.1707 ms. Best GFLOPs: 640.8462
[02:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_nn_relu_16"] Trial #31: GFLOPs: 501.2712. Time: 0.0112 ms. Best GFLOPs: 640.8462
[02:40:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_conv2d_add_nn_relu_16"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |            
 23 |            fused_concatenate_3 |         1 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 574
Total latency (us): 582.488

[02:40:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_3"] Trial #0: GFLOPs: 0.0000. Time: 0.0046 ms. Best GFLOPs: 0.0000
[02:40:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_3"] Trial #1: GFLOPs: 0.0000. Time: 0.0034 ms. Best GFLOPs: 0.0000
[02:40:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_3"] Trial #2: GFLOPs: 0.0000. Time: 0.0111 ms. Best GFLOPs: 0.0000
[02:40:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_3"] Trial #3: GFLOPs: 0.0000. Time: 0.0155 ms. Best GFLOPs: 0.0000
[02:40:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_3"] Trial #4: GFLOPs: 0.0000. Time: 0.0211 ms. Best GFLOPs: 0.0000
[02:40:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_concatenate_3"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |            
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 579
Total latency (us): 589.285

[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #0: GFLOPs: 573.7949. Time: 0.3022 ms. Best GFLOPs: 573.7949
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #1: GFLOPs: 953.9485. Time: 0.1818 ms. Best GFLOPs: 953.9485
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #2: GFLOPs: 11.4601. Time: 15.1302 ms. Best GFLOPs: 953.9485
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #3: GFLOPs: 1729.3874. Time: 0.1003 ms. Best GFLOPs: 1729.3874
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #4: GFLOPs: 584.2697. Time: 0.2968 ms. Best GFLOPs: 1729.3874
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #5: GFLOPs: 1714.0963. Time: 0.1012 ms. Best GFLOPs: 1729.3874
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #6: GFLOPs: 2.2738. Time: 76.2572 ms. Best GFLOPs: 1729.3874
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], placeholder_1: T.Buffer[(1000, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1000, 1, 1), "float32"], T_relu: T.Buffer[(1, 1000, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1000, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1000, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(130, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(50, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1000, i0_1_i1_1_i2_1_i3_1_fused // 13 * 100 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(13, i2_4_init)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 13, 13], "float32"], ["TENSOR", [1000, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(50, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 200 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 169)
                                        v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 200 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 169 // 13)
                                        v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 200 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 50 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 676)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(40):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(50, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1000, (ax0_ax1_ax2_ax3_fused_0 * 100 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(512, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 100 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1000, i0_1_i1_1_i2_1_i3_1_fused // 13 * 100 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(13, i2_4)
                                xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                rc = T.axis.reduce(512, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 13, 13], "float32"], ["TENSOR", [1000, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1000, i0_1_i1_1_i2_1_i3_1_fused // 13 * 100 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 10, 50, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 50, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 50, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #8: GFLOPs: 106.9667. Time: 1.6210 ms. Best GFLOPs: 1729.3874
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #9: GFLOPs: 2056.6289. Time: 0.0843 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #10: GFLOPs: 6.6522. Time: 26.0658 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #11: GFLOPs: 695.5809. Time: 0.2493 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #12: GFLOPs: 1966.4471. Time: 0.0882 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #13: GFLOPs: 634.8884. Time: 0.2731 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #14: GFLOPs: 91.6514. Time: 1.8919 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #15: GFLOPs: 938.4869. Time: 0.1848 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #16: GFLOPs: 1839.4802. Time: 0.0943 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #17: GFLOPs: 1269.6770. Time: 0.1366 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #18: GFLOPs: 829.9337. Time: 0.2089 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #19: GFLOPs: 2.3435. Time: 73.9885 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #20: GFLOPs: 447.4601. Time: 0.3875 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #21: GFLOPs: 19.9677. Time: 8.6837 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #22: GFLOPs: 900.5181. Time: 0.1925 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #23: GFLOPs: 420.4778. Time: 0.4124 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #24: GFLOPs: 5.0123. Time: 34.5938 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #25: GFLOPs: 52.1606. Time: 3.3242 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], placeholder_1: T.Buffer[(1000, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1000, 1, 1), "float32"], T_relu: T.Buffer[(1, 1000, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1000, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1000, 512, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(5, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(40, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(5, 13, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused * 200 + i0_2_i1_2_i2_2_i3_2_fused * 5 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i2_4_init, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 13, 13], "float32"], ["TENSOR", [1000, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(34):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) // 169)
                                    v2 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(13, (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1 < 1352)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused * 200 + (ax0_ax1_ax2_ax3_fused_0 * 80 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 80 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 5, 1, 13, 1, 1, 1, 1, 1, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused * 200 + i0_2_i1_2_i2_2_i3_2_fused * 5 + i1_3)
                                yy, xx = T.axis.remap("SS", [i2_4, i3_3])
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 13, 13], "float32"], ["TENSOR", [1000, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 5, 13, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused * 200 + i0_2_i1_2_i2_2_i3_2_fused * 5 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[5, 1, 40, 5, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 40])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 40, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #27: GFLOPs: 713.1814. Time: 0.2431 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #28: GFLOPs: 1451.7684. Time: 0.1194 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #29: GFLOPs: 27.0282. Time: 6.4153 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #30: GFLOPs: 8.7602. Time: 19.7934 ms. Best GFLOPs: 2056.6289
[02:40:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_17"] Trial #31: GFLOPs: 19.8655. Time: 8.7284 ms. Best GFLOPs: 2056.6289
[02:40:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_conv2d_add_nn_relu_17"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |            
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 611
Total latency (us): 673.595

[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #0: GFLOPs: 19.9329. Time: 0.0095 ms. Best GFLOPs: 19.9329
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #1: GFLOPs: 16.6769. Time: 0.0114 ms. Best GFLOPs: 19.9329
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #2: GFLOPs: 9.4719. Time: 0.0201 ms. Best GFLOPs: 19.9329
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #3: GFLOPs: 3.8809. Time: 0.0490 ms. Best GFLOPs: 19.9329
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #4: GFLOPs: 57.2748. Time: 0.0033 ms. Best GFLOPs: 57.2748
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #5: GFLOPs: 64.5587. Time: 0.0029 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #6: GFLOPs: 5.4937. Time: 0.0346 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #7: GFLOPs: 11.6969. Time: 0.0162 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #8: GFLOPs: 11.9241. Time: 0.0159 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #9: GFLOPs: 28.7896. Time: 0.0066 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #10: GFLOPs: 25.0578. Time: 0.0076 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #11: GFLOPs: 30.4892. Time: 0.0062 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #12: GFLOPs: 55.5084. Time: 0.0034 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #13: GFLOPs: 31.8218. Time: 0.0060 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #14: GFLOPs: 3.0821. Time: 0.0616 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #15: GFLOPs: 44.6762. Time: 0.0043 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #16: GFLOPs: 6.1055. Time: 0.0311 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #17: GFLOPs: 6.0582. Time: 0.0314 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #18: GFLOPs: 10.9297. Time: 0.0174 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #19: GFLOPs: 55.4792. Time: 0.0034 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #20: GFLOPs: 14.8037. Time: 0.0128 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #21: GFLOPs: 20.2440. Time: 0.0094 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #22: GFLOPs: 5.7097. Time: 0.0333 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #23: GFLOPs: 37.5599. Time: 0.0051 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #24: GFLOPs: 55.5042. Time: 0.0034 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #25: GFLOPs: 2.0985. Time: 0.0905 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #26: GFLOPs: 27.4978. Time: 0.0069 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #27: GFLOPs: 8.0148. Time: 0.0237 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #28: GFLOPs: 32.3421. Time: 0.0059 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #29: GFLOPs: 7.0995. Time: 0.0268 ms. Best GFLOPs: 64.5587
[02:40:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #30: GFLOPs: 19.9471. Time: 0.0095 ms. Best GFLOPs: 64.5587
[02:41:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_nn_avg_pool2d"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |            
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 642
Total latency (us): 676.538

[02:41:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0220 ms. Best GFLOPs: 0.0000
[02:41:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0056 ms. Best GFLOPs: 0.0000
[02:41:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0055 ms. Best GFLOPs: 0.0000
[02:41:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0156 ms. Best GFLOPs: 0.0000
[02:41:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0088 ms. Best GFLOPs: 0.0000
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_reshape"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |            
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |            
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |            
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_conv2d_add_nn_relu"
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #0 has finished. Remaining task(s): 26
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_conv2d_add_nn_relu_17"
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #24 has finished. Remaining task(s): 25
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_conv2d_add_nn_relu_2"
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #2 has finished. Remaining task(s): 24
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_nn_relu_1"
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #1 has finished. Remaining task(s): 23
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_conv2d_add_nn_relu_3"
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #3 has finished. Remaining task(s): 22
[02:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_concatenate_1"
[02:41:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:41:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:42:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:42:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:42:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:43:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:44:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:44:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:45:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:45:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:45:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_concatenate_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |            
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |            
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |            
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |            
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |            
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_conv2d_add_nn_relu_9"
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #12 has finished. Remaining task(s): 21
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_conv2d_add_nn_relu_7"
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #8 has finished. Remaining task(s): 20
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_nn_conv2d_add_nn_relu_15"
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #21 has finished. Remaining task(s): 19
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_conv2d_add_nn_relu_4"
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #4 has finished. Remaining task(s): 18
[02:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_concatenate"
[02:45:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:45:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:45:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[02:45:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:46:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[02:47:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[02:48:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[02:48:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[02:49:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:49:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:49:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:49:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:49:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:49:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_concatenate"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |            
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |            
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[02:49:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_conv2d_add_nn_relu_11"
[02:49:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #16 has finished. Remaining task(s): 17
[02:49:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_concatenate_1"
[02:49:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:49:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:49:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:49:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:50:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:51:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:51:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:52:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:52:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:52:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:52:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_concatenate_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |            
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |            
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |            
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |            
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_conv2d_add_nn_relu_6"
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #7 has finished. Remaining task(s): 16
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_conv2d_add_nn_relu_16"
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #22 has finished. Remaining task(s): 15
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_conv2d_add_nn_relu_12"
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #17 has finished. Remaining task(s): 14
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_conv2d_add_nn_relu_8"
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #11 has finished. Remaining task(s): 13
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_conv2d_add_nn_relu_10"
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #13 has finished. Remaining task(s): 12
[02:52:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_concatenate_1"
[02:52:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:52:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:53:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:53:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:53:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:54:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:54:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:55:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[02:55:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:55:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:55:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:55:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:55:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:55:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_concatenate_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |            
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[02:55:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_conv2d_add_nn_relu_13"
[02:55:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #18 has finished. Remaining task(s): 11
[02:55:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_concatenate"
[02:55:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:56:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:56:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[02:56:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:57:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[02:58:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[02:59:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:00:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:00:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:00:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:00:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:00:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:00:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:00:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_concatenate"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:00:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_conv2d_add_nn_relu_14"
[03:00:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #20 has finished. Remaining task(s): 10
[03:00:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_concatenate_2"
[03:00:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:00:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:01:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:01:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:01:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:02:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:02:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:03:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:03:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:03:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:03:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:03:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:03:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:03:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_concatenate_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:03:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_concatenate_1"
[03:03:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:03:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:03:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[03:03:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:04:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[03:05:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[03:05:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[03:05:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[03:06:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:06:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:06:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_concatenate_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |            
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_concatenate_1"
[03:06:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:06:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:06:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[03:06:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:06:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[03:07:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[03:07:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[03:08:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98c78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93e39e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93424a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c92292c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c90275b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8f93f48)]: 0 failure(s)
[03:09:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:09:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:09:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:09:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #14 has finished. Remaining task(s): 9
[03:09:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_concatenate"
[03:09:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:09:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:09:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:09:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:10:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:10:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:11:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:11:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:12:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:12:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:12:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:12:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:12:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:12:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_concatenate"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:12:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_concatenate_3"
[03:12:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:12:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:12:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:12:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:13:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:13:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:14:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:15:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:15:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:15:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:15:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:15:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:15:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:15:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_concatenate_3"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:15:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_max_pool2d_1"
[03:15:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:15:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:15:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:15:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:16:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:16:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:16:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:16:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:17:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:17:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:17:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:17:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:17:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:17:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_max_pool2d_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:17:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d"
[03:17:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:17:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:17:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:17:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:17:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:18:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:18:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:18:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:19:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:19:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:19:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:19:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:19:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:19:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:19:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_reshape"
[03:19:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:19:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:19:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:19:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:19:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:19:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:20:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:20:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:20:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:20:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:20:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:20:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:20:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:20:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_reshape"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:20:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_concatenate"
[03:20:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:20:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:20:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:20:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:21:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:21:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:22:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:23:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:23:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:23:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:23:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:23:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:23:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:23:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_concatenate"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:23:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_conv2d_add_nn_relu_5"
[03:23:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #6 has finished. Remaining task(s): 8
[03:23:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_concatenate_2"
[03:23:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:23:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:23:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:23:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:24:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:24:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:25:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:25:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:26:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:26:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:26:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:26:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:26:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:26:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_concatenate_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |            
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:26:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_concatenate"
[03:26:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:26:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:26:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:26:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:27:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:27:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:28:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:28:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9239e98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c9328038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c82f2b38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f33de8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f367a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f3988)]: 0 failure(s)
[03:29:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:29:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:29:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:29:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #9 has finished. Remaining task(s): 7
[03:29:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_concatenate_2"
[03:29:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:29:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:29:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:29:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:30:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:30:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:30:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:31:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:31:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:31:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:31:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:31:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:31:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:31:48] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_concatenate_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:31:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_concatenate_3"
[03:31:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:31:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:32:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:32:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:32:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:33:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:33:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:34:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:34:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:34:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:34:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:34:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:34:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:34:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_concatenate_3"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:34:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_2"
[03:34:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:34:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:34:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[03:34:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:34:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[03:35:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[03:35:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[03:35:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[03:36:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:36:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:36:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:36:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:36:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:36:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_max_pool2d_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:36:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_max_pool2d_1"
[03:36:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:36:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:36:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:36:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:36:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:37:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:37:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:37:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:38:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:38:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:38:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:38:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:38:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:38:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_max_pool2d_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     31 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 647
Total latency (us): 682.037

[03:38:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_avg_pool2d"
[03:38:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:38:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 31 candidate(s) from database
[03:38:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927c828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93902b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933e218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f8a9a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9346748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933e2b8)]: 0 failure(s)
[03:38:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2017 candidate(s)
[03:38:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927c828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93902b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933e218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f8a9a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9346748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933e2b8)]: 0 failure(s)
[03:39:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927c828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93902b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933e218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f8a9a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9346748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933e2b8)]: 0 failure(s)
[03:39:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927c828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93902b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933e218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f8a9a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9346748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933e2b8)]: 0 failure(s)
[03:40:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927c828)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93902b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933e218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f8a9a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9346748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933e2b8)]: 0 failure(s)
[03:40:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 1 candidates:
[1 : 1]:	0.9648
[03:40:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 1 candidate(s) with evolutionary search
[03:40:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 1 candidates(s) for measurement
[03:40:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 1 sample(s) to builder
[03:40:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 1 sample(s) to runner
[03:40:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_avg_pool2d"] Trial #31: GFLOPs: 34.0856. Time: 0.0056 ms. Best GFLOPs: 64.5587
[03:40:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_nn_avg_pool2d"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:40:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d"
[03:40:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:40:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:41:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:41:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:41:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:41:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:41:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:42:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:42:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:42:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:42:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:42:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:42:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:42:44] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:42:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_reshape"
[03:42:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:42:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:42:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:42:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:42:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:43:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:43:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:43:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:43:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:43:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:43:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:43:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:43:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:43:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_reshape"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:43:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_concatenate_2"
[03:43:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:43:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:43:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:43:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:43:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:44:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:44:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:45:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:45:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:45:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:45:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_concatenate_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_concatenate_3"
[03:45:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:45:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:45:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:45:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:45:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:46:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:46:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:47:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:47:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:47:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:47:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:47:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:47:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:47:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_concatenate_3"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:47:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_max_pool2d_1"
[03:47:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:47:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:47:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:47:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:47:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:48:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:48:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:48:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:49:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:49:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:49:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:49:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:49:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:49:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_max_pool2d_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |            
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:49:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_concatenate_2"
[03:49:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:49:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:49:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:49:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:49:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:50:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:50:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:50:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c927f768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c73a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8366ec8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f396a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c9050f08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9225fb8)]: 0 failure(s)
[03:50:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:50:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:50:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:50:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #19 has finished. Remaining task(s): 6
[03:50:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d"
[03:50:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:50:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:51:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:51:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:51:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:51:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:51:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:52:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:52:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:52:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:52:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:52:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:52:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:52:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |          Y 
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:52:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_reshape"
[03:52:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:52:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:52:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:52:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:52:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:52:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:52:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:53:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:53:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:53:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:53:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:53:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:53:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:53:18] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_reshape"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |          Y 
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:53:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_concatenate_3"
[03:53:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:53:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:53:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:53:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:53:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:54:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:54:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:54:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:55:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:55:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:55:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:55:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:55:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:55:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_concatenate_3"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |          Y 
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:55:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_2"
[03:55:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:55:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:55:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[03:55:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:55:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[03:55:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[03:56:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[03:56:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[03:56:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:56:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:56:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:56:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:56:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:56:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_max_pool2d_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |          Y 
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:56:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_max_pool2d_1"
[03:56:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:56:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:56:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:56:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:56:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:57:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:57:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:57:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[03:57:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:57:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:57:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:57:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:57:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:57:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_max_pool2d_1"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |          Y 
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |            
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:57:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_avg_pool2d"
[03:57:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #25 has finished. Remaining task(s): 5
[03:57:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d"
[03:57:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:57:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:58:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:58:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:58:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:58:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:58:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:59:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[03:59:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:59:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:59:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:59:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:59:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:59:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |          Y 
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |          Y 
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:59:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_reshape"
[03:59:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:59:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:59:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:59:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:59:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:59:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:59:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:59:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[03:59:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:59:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:59:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_reshape"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |            
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |            
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |          Y 
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |            
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |          Y 
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[03:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_concatenate_3"
[03:59:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:59:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:59:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[03:59:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:00:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[04:00:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[04:00:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[04:01:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c92788b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c62c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c93447c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f67248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f96fa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c9344868)]: 0 failure(s)
[04:01:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:01:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:01:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:01:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #23 has finished. Remaining task(s): 4
[04:01:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_max_pool2d_1"
[04:01:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:01:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:01:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[04:01:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:01:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[04:01:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[04:01:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[04:02:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c93d8cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c92255f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c923f7b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c9284308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82ef1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c91f0568)]: 0 failure(s)
[04:02:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:02:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:02:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:02:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #10 has finished. Remaining task(s): 3
[04:02:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d"
[04:02:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:02:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:02:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[04:02:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:02:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[04:02:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[04:03:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[04:03:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c923a2b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c920e258)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c8fd3aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c925b148)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c82dc028)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c8fd42b8)]: 0 failure(s)
[04:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:03:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #5 has finished. Remaining task(s): 2
[04:03:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_2"
[04:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:03:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:03:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:03:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:04:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:04:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:04:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:04:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:04:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:04:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:04:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:04:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:04:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_max_pool2d_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |          Y 
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |          Y 
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |          Y 
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |          Y 
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |          Y 
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |            
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[04:04:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_reshape"
[04:04:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:04:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:04:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[04:04:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:05:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[04:05:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[04:05:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[04:05:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c9330a28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c93c0a98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c933f008)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c8f1f588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c8f6e348)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c933f0a8)]: 0 failure(s)
[04:05:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:05:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:05:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:05:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #26 has finished. Remaining task(s): 1
[04:05:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_2"
[04:05:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:05:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:05:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:05:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:05:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:06:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:06:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:06:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:06:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:06:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:06:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_max_pool2d_2"
 ID |                           Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu |  49926656 |      2 |      1112.0385 |      44.8965 |               89.7930 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 |  28099968 |      2 |       831.6520 |      33.7881 |               67.5763 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  53934336 |      2 |      1488.6350 |      36.2307 |               72.4615 |     32 |          Y 
  3 |  fused_nn_conv2d_add_nn_relu_3 |  56144000 |      2 |      2639.6107 |      21.2698 |               42.5396 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_4 |  44158464 |      1 |      1866.1607 |      23.6627 |               23.6627 |     32 |          Y 
  5 |            fused_nn_max_pool2d |   1742400 |      1 |       309.7677 |       5.6249 |                5.6249 |      5 |          Y 
  6 |  fused_nn_conv2d_add_nn_relu_5 |   6292000 |      1 |      1228.1753 |       5.1230 |                5.1230 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_6 |  12487200 |      1 |       694.8327 |      17.9715 |               17.9715 |     32 |          Y 
  8 |  fused_nn_conv2d_add_nn_relu_7 |   6582400 |      2 |       539.3421 |      12.2045 |               24.4090 |     32 |          Y 
  9 |              fused_concatenate |         1 |      2 |         0.0001 |      10.3594 |               20.7188 |      5 |          Y 
 10 |          fused_nn_max_pool2d_1 |    839808 |      1 |       136.5206 |       6.1515 |                6.1515 |      5 |          Y 
 11 |  fused_nn_conv2d_add_nn_relu_8 |   6018624 |      1 |       403.0030 |      14.9344 |               14.9344 |     32 |          Y 
 12 |  fused_nn_conv2d_add_nn_relu_9 |  11990592 |      1 |       409.2493 |      29.2990 |               29.2990 |     32 |          Y 
 13 | fused_nn_conv2d_add_nn_relu_10 |   6158592 |      2 |       908.6679 |       6.7776 |               13.5552 |     32 |          Y 
 14 |            fused_concatenate_1 |         1 |      2 |         0.0001 |      18.1364 |               36.2727 |      5 |          Y 
 15 |          fused_nn_max_pool2d_2 |    389376 |      1 |       115.9578 |       3.3579 |                3.3579 |      5 |            
 16 | fused_nn_conv2d_add_nn_relu_11 |   4169568 |      1 |       209.2465 |      19.9266 |               19.9266 |     32 |          Y 
 17 | fused_nn_conv2d_add_nn_relu_12 |   6246240 |      1 |       403.1107 |      15.4951 |               15.4951 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_13 |   3179904 |      2 |       564.6165 |       5.6320 |               11.2639 |     32 |          Y 
 19 |            fused_concatenate_2 |         1 |      2 |         0.0002 |       5.0992 |               10.1984 |      5 |          Y 
 20 | fused_nn_conv2d_add_nn_relu_14 |   8328320 |      1 |       804.0702 |      10.3577 |               10.3577 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_15 |  11097216 |      1 |       457.7618 |      24.2423 |               24.2423 |     32 |          Y 
 22 | fused_nn_conv2d_add_nn_relu_16 |   5624320 |      2 |       640.8462 |       8.7764 |               17.5528 |     32 |          Y 
 23 |            fused_concatenate_3 |         1 |      2 |         0.0003 |       3.3984 |                6.7968 |      5 |          Y 
 24 | fused_nn_conv2d_add_nn_relu_17 | 173394000 |      1 |      2056.6289 |      84.3098 |               84.3098 |     32 |          Y 
 25 |            fused_nn_avg_pool2d |    190000 |      1 |        64.5587 |       2.9431 |                2.9431 |     32 |          Y 
 26 |                  fused_reshape |         1 |      1 |         0.0002 |       5.4998 |                5.4998 |      5 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------
Total trials: 648
Total latency (us): 682.037

[04:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_2"
[04:06:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:06:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:06:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:06:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:06:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:07:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:07:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:07:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55e2c8f98af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55e2c91f93e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55e2c9398f78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55e2c83051c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55e2c92753d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55e2c922c2b8)]: 0 failure(s)
[04:07:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:07:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:07:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:07:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #15 has finished. Remaining task(s): 0
Starting to build with relay.
/home/yj/anaconda3/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
