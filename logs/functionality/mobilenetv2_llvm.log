nohup: ignoring input
[23:13:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_layout_transform"
[23:13:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[23:13:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:13:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:13:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[23:13:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 226, 226, 3):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 225 and 1 <= i3_1 and i3_1 < 225, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 112, 112, 4, 3, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:13:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:13:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 28, 7, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 9, 33, 3):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(226, i2_0 * 8 + ax2)
                        i3 = T.axis.spatial(226, i3_0 * 32 + ax3)
                        i4 = T.axis.spatial(3, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 225 and 1 <= i3 and i3 < 225, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 3, 3, 1, 2, 2, 8, 1, 3, 1, 1, 1, 2, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_0 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 16 + i3_2 * 2 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_0, i5_1, i6_0, i7_0])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 1, 2, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 8, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[23:13:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 226, 226, 3):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 225 and 1 <= i3_1 and i3_1 < 225, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 2, 28, 7, 4, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 3, 1, 2, 2, 8, 1, 3, 1, 1, 1, 2, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_0 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 16 + i3_2 * 2 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_0, i5_1, i6_0, i7_0])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 4, 16, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(112, i2_0 * 4 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 * 16 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 1, 2, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 8, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:13:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 226, 226, 3):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 225 and 1 <= i3_1 and i3_1 < 225, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 28, 7, 4):
                for i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 3, 3, 1, 2, 2, 8, 1, 3, 1, 1, 1, 2, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_0 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 16 + i3_2 * 2 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_0, i5_1, i6_0, i7_0])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 4, 16, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(112, i2_0 * 4 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 * 16 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 1, 2, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 8, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[23:13:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[23:13:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 112, 112, 4, 32, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:13:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:13:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 14, 28, 2, 16, 1, 1, 1, 1, 4, 4, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_1)
                    oh = T.axis.spatial(112, i2_1 * 56 + i2_2 * 4 + i2_3)
                    ow = T.axis.spatial(112, i3_2 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 28, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:13:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 1, 1, 2, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 14, 28, 2, 16, 1, 1, 1, 1, 4, 4, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(112, i2_1 * 56 + i2_2 * 4 + i2_3)
                        ow = T.axis.spatial(112, i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 56, 112, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 2 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(112, i2_1 * 56 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 28, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:13:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 14, 28, 2, 16, 1, 1, 1, 1, 4, 4, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(112, i2_1 * 56 + i2_2 * 4 + i2_3)
                        ow = T.axis.spatial(112, i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 112, 112, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 28, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:13:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"
[23:13:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 114, 114, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 8, 112, 112, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 1, 4, 4, 1, 1, 4, 2, 14):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 16, 4, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(114, i2_0 * 28 + i2_1 * 14 + ax2)
                        i3 = T.axis.spatial(114, i3_0 * 28 + i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 3, 1, 1, 2, 1, 2, 3, 1, 1, 2, 7, 2, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_0 * 28 + i2_1 * 14 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 28 + i3_1 * 2 + i3_3)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 14, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:14:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0 in T.grid(1, 1, 4, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 30, 30, 4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(114, i2_0 * 28 + ax2)
                        i3 = T.axis.spatial(114, i3_0 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 2, 14, 2):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 1, 2, 1, 2, 3, 1, 1, 2, 7, 2, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(112, i2_0 * 28 + i2_1 * 14 + i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(112, i3_0 * 28 + i3_1 * 2 + i3_3)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 2, 2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(8, i1_1 * 2 + ax1)
                            ax2_1 = T.axis.spatial(112, i2_0 * 28 + i2_1 * 14 + ax2)
                            ax3_1 = T.axis.spatial(112, i3_0 * 28 + i3_1 * 2 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 14, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 4, 4, 1):
                for i0_1, i1_1 in T.grid(1, 4):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 30, 30, 4):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(8, i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(114, i2_0 * 28 + ax2)
                            i3 = T.axis.spatial(114, i3_0 * 28 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 14, 2, 1, 3, 1, 1, 2, 1, 2, 3, 1, 1, 2, 7, 2, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(112, i2_0 * 28 + i2_1 * 14 + i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(112, i3_0 * 28 + i3_1 * 2 + i3_3)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 28, 28, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(112, i2_0 * 28 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 * 28 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 14, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_contrib_conv2d_NCHWc_add"
[23:14:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(4, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 4, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 4, 112, 112, 4, 32, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[23:14:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(4, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 4, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 4, 1, 1, 1, 14, 14, 1, 1, 1, 1, 1, 4, 2, 2, 4, 32, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i1_2)
                    oh = T.axis.spatial(112, i2_0 * 28 + i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(112, i3_0 * 28 + i3_1 * 2 + i3_2)
                    oc_block, ic = T.axis.remap("SR", [i4_2, i5_1])
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 4, 112, 112, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 14, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 14, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[1, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[23:14:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(4, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 4, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 4, 1, 1, 1, 14, 14, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 4, 2, 2, 4, 32, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_2)
                        oh = T.axis.spatial(112, i2_0 * 28 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(112, i3_0 * 28 + i3_1 * 2 + i3_2)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_1])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 2, 4):
                    with T.block("T_add"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(112, i2_0 * 28 + i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 * 28 + i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 14, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 14, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[1, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(4, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 4, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 4, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 4, 4, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 14, 14, 1, 1, 1, 1, 1, 4, 2, 2, 4, 32, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_2)
                        oh = T.axis.spatial(112, i2_0 * 28 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(112, i3_0 * 28 + i3_1 * 2 + i3_2)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_1])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 28, 4):
                    with T.block("T_add"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(112, i2_0 * 28 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 * 28 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 14, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 14, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[1, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
[23:14:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 24, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 24, 112, 112, 4, 16, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 112, 112, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 4, 7, 1, 4, 16, 1, 1, 1, 2, 16, 56, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i1_1 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(112, i2_2 * 16 + i2_3)
                    ow = T.axis.spatial(112, i3_0 * 56 + i3_3)
                    oc_block, ic = T.axis.remap("SR", [i4_2, i5_1])
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 24, 112, 112, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 3, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 16])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 56])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 1, 3, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 4, 7, 1, 4, 16, 1, 1, 1, 2, 16, 56, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_2 * 16 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 56 + i3_3)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_1])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 112, 56, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(112, ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 * 56 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 3, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 16])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 56])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 4, 7, 1, 4, 16, 1, 1, 1, 2, 16, 56, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_2 * 16 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 56 + i3_3)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_1])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 24, 112, 56, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(112, i3_0 * 56 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 3, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 16])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 56])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"
[23:14:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 114, 114, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 24, 56, 56, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2 in T.grid(1, 3, 1, 28, 1, 1, 2, 2, 1, 1, 1, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 57, 5, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(24, i1_0 * 8 + i1_1 * 4 + i1_2 + ax1)
                        i2 = T.axis.spatial(114, i2_1 * 56 + ax2)
                        i3 = T.axis.spatial(114, i3_0 * 4 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 3, 3, 1, 1, 28, 2, 4):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(24, i1_0 * 8 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(56, i2_1 * 28 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 2 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 28])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[28, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=13)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:14:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 24, 114, 114, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 3, 1, 28, 1, 1, 2, 2, 1, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 4, 1, 1, 1, 3, 3, 1, 1, 28, 2, 4):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(24, i1_0 * 8 + i1_1_1 * 4 + i1_2)
                        oh = T.axis.spatial(56, i2_1_1 * 28 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 2 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 2, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1_0 * 8 + i1_1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_1_1 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 28])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[28, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 3, 1, 28, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(1, 2, 2, 1, 1, 1, 1, 1, 4, 1, 1, 1, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 55, 5, 4):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(24, i1_0 * 8 + i1_1 * 4 + i1_2 + ax1)
                            i2 = T.axis.spatial(114, i2_1 * 56 + i5_1 + ax2)
                            i3 = T.axis.spatial(114, i3_0 * 4 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 28, 2, 4):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(24, i1_0 * 8 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(56, i2_1 * 28 + i2_3)
                            ow = T.axis.spatial(56, i3_0 * 2 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 56, 2, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(56, ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 28])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[28, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=17)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"
[23:14:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 6, 56, 56, 4, 96, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [6, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[23:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 4, 2, 1, 2, 56, 1, 1, 8, 1, 1, 1, 1, 1, 14, 1, 12, 1, 1, 1, 1, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(6, i1_0 * 2 + i1_1)
                    oh = T.axis.spatial(56, i2_1)
                    ow = T.axis.spatial(56, i3_0 * 14 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(96, i5_0 * 12 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [6, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 6, 56, 56, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[3, 2, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 56, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 14, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 12])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[23:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 3, 1, 4, 2, 1, 2, 56, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 1, 14, 1, 12, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(56, i2_1)
                        ow = T.axis.spatial(56, i3_0 * 14 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(96, i5_0 * 12 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [6, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 14, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(6, i1_0 * 2 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[3, 2, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 56, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 14, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 12])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 3, 1, 4, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 56, 1, 1, 8, 1, 1, 1, 1, 1, 14, 1, 12, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(56, i2_1)
                        ow = T.axis.spatial(56, i3_0 * 14 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(96, i5_0 * 12 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [6, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 56, 14, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(6, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(56, ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[3, 2, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 56, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 14, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 12])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"
[23:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 58, 58, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 36, 56, 56, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 36, 58, 58, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 7, 2, 1, 3, 1, 2, 2, 3, 3, 1, 2, 28, 2, 1, 1, 1, 1, 3, 2, 2, 1):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(36, i1_0 * 18 + i1_1_1 * 6 + i1_2 * 3 + i1_3)
                    oh = T.axis.spatial(56, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(56, i3_0 * 8 + i3_1_1 * 4 + i3_2 * 2 + i3_3)
                    oci = T.axis.spatial(4, i4_0 * 2 + i4_1_1)
                    kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                    T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 3, 2, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 28, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 7, 2, 1, 3, 1, 2, 2):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 28, 2, 1, 1, 1, 1, 3, 2, 2, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i1_0 * 18 + i1_1 * 6 + i1_2 * 3 + i1_3)
                        oh = T.axis.spatial(56, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 8 + i3_1 * 4 + i3_2 * 2 + i3_3)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 56, 4, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(36, i1_0 * 18 + i1_1 * 6 + ax1)
                        ax2_1 = T.axis.spatial(56, ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 8 + i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 3, 2, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 28, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 7, 2):
                for i0_1, i1_1 in T.grid(1, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 58, 10, 2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(36, i1_0 * 18 + i1_1 * 6 + ax1)
                            i2 = T.axis.spatial(58, ax2)
                            i3 = T.axis.spatial(58, i3_0 * 8 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 3, 3, 1, 2, 28, 2, 1, 1, 1, 1, 3, 2, 2, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i1_0 * 18 + i1_1 * 6 + i1_2 * 3 + i1_3)
                            oh = T.axis.spatial(56, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(56, i3_0 * 8 + i3_1 * 4 + i3_2 * 2 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 18, 56, 8, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(36, i1_0 * 18 + ax1)
                        ax2_1 = T.axis.spatial(56, ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 8 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 3, 2, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 28, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"
[23:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 6, 56, 56, 4, 144, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 56, 56, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[23:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 2, 1, 1, 1, 7, 1, 24, 1, 1, 1, 1, 28, 1, 2, 6, 1, 1, 1, 6, 1, 4, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(6, i1_3)
                    oh = T.axis.spatial(56, i2_0 * 28 + i2_2)
                    ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(144, i5_0 * 6 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 6, 56, 56, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 28, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 2, 2, 1, 1, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(24, 1, 1, 1, 1, 28, 1, 2, 6, 1, 1, 1, 6, 1, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_2)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(144, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 28, 4, 2):
                    with T.block("T_add_1"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 28 + i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 28, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 2, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 24, 1, 1, 1, 1, 28, 1, 2, 6, 1, 1, 1, 6, 1, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_2)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(144, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 28, 28, 2):
                    with T.block("T_add_1"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 28 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 28, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
[23:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 36, 56, 56, 4, 24, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 6, 1, 1, 1, 1, 3, 14, 2, 2, 8, 1, 1, 1, 1, 1, 14, 2, 3, 1, 1, 1, 2, 4, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(36, i1_0 * 6 + i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(56, i2_1 * 4 + i2_3)
                    ow = T.axis.spatial(56, i3_1 * 28 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(24, i5_0 * 3 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[6, 3, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 6, 1, 1, 1, 1, 3, 14, 2, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 1, 14, 2, 3, 1, 1, 1, 2, 4, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(36, i1_0 * 6 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 4 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 28 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(24, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 4, 28, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(36, i1_0 * 6 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_1 * 4 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_1 * 28 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[6, 3, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 6, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 14, 2, 2, 8, 1, 1, 1, 1, 1, 14, 2, 3, 1, 1, 1, 2, 4, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(36, i1_0 * 6 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 4 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 28 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(24, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 56, 56, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(36, i1_0 * 6 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[6, 3, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"
[23:14:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 58, 58, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 36, 28, 28, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 7, 2, 1, 1, 1, 1, 2, 4, 2, 2, 3, 3, 1, 18, 1, 7, 2):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(36, i1_2 * 18 + i1_3)
                    oh = T.axis.spatial(28, i2_1 * 4 + i2_2)
                    ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 7 + i3_3)
                    oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                    T.reads(placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 57 and 1 <= ow * 2 + kw and ow * 2 + kw < 57, placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 36, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 18])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 1, 7, 2, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(1, 1, 1, 2, 4, 2, 2, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 18, 1, 15, 2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(36, i1_2 * 18 + ax1)
                            i2 = T.axis.spatial(58, i2_1 * 8 + i2_2 * 2 + i5_1 + ax2)
                            i3 = T.axis.spatial(58, i3_1 * 28 + i3_2 * 14 + ax3)
                            i4 = T.axis.spatial(4, i4_2 * 2 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 18, 1, 7, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i1_2 * 18 + i1_3)
                            oh = T.axis.spatial(28, i2_1 * 4 + i2_2)
                            ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 7 + i3_3)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 36, 4, 14, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(28, i2_1 * 4 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_1 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 18])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=17)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(1, 1, 7, 2, 1, 1, 1, 1, 2, 4, 2, 2, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 18, 1, 15, 2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(36, i1_2 * 18 + ax1)
                            i2 = T.axis.spatial(58, i2_1 * 8 + i2_2 * 2 + i5_1 + ax2)
                            i3 = T.axis.spatial(58, i3_1 * 28 + i3_2 * 14 + ax3)
                            i4 = T.axis.spatial(4, i4_2 * 2 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 18, 1, 7, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i1_2 * 18 + i1_3)
                            oh = T.axis.spatial(28, i2_1 * 4 + i2_2)
                            ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 7 + i3_3)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 36, 28, 28, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 18])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=17)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"
[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 28, 28, 4, 144, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 28, 28, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 4, 1, 1, 1, 1, 1, 1, 2, 36, 1, 1, 1, 1, 7, 28, 1, 4, 1, 1, 1, 2, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_0 * 7 + i2_2)
                    ow = T.axis.spatial(28, i3_2)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(144, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 28, 28, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 28, 28, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 28, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[36, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 4, 1, 1, 1, 1, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(36, 1, 1, 1, 1, 7, 28, 1, 4, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(144, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 28, 28, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 28, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(28, ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 28, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[36, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 4, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 36, 1, 1, 1, 1, 7, 28, 1, 4, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(144, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 28, 28, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 28, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 7 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 28, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[36, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"
[23:14:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 28, 28, 4, 192, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[23:14:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 2, 1, 2, 1, 28, 2, 32, 1, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 2, 14, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_2 * 14 + i2_3)
                    ow = T.axis.spatial(28, i3_1)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(192, i5_0 * 6 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 28, 28, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 2, 1, 2, 1, 28, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 2, 14, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_2 * 14 + i2_3)
                        ow = T.axis.spatial(28, i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(192, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 28, 1, 1):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3_1 = T.axis.spatial(28, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 28, 2, 32, 1, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 2, 14, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_2 * 14 + i2_3)
                        ow = T.axis.spatial(28, i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(192, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 28, 2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
[23:14:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 48, 28, 28, 4, 32, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 28, 28, 4], "float32"], ["TENSOR", [48, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 1, 14, 1, 2, 8, 1, 1, 1, 2, 2, 1, 1, 4, 1, 1, 1, 24, 1, 28, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(48, i1_2 * 24 + i1_3)
                    oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(28, i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 28, 28, 4], "float32"], ["TENSOR", [48, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 24])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 1, 14, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 2, 2, 1, 1, 4, 1, 1, 1, 24, 1, 28, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i1_2 * 24 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 28, 28, 4], "float32"], ["TENSOR", [48, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 2, 28, 1):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(28, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(28, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 24])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 14, 1, 2, 8, 1, 1, 1, 2, 2, 1, 1, 4, 1, 1, 1, 24, 1, 28, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i1_2 * 24 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 28, 28, 4], "float32"], ["TENSOR", [48, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 28, 28, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 24])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"
[23:14:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 30, 30, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 48, 28, 28, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 30, 30, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 24, 1, 1, 2, 1, 3, 1, 2, 4, 1, 2, 3, 1, 1, 1, 7, 14, 1):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(48, i1_1_1 * 2 + i1_2)
                    oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(28, i3_0 * 14 + i3_3)
                    oci = T.axis.spatial(4, i4_1_1 * 2 + i4_2)
                    kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                    T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 24, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:14:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 1, 24, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 30, 16, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(30, ax2)
                        i3 = T.axis.spatial(30, i3_0 * 14 + ax3)
                        i4 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 2, 4, 1, 2, 3, 1, 1, 1, 7, 14, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(48, i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_3)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 28, 14, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(48, i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 24, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 24, 1, 1, 2, 1, 3, 1, 2, 4, 1, 2, 3, 1, 1, 1, 7, 14, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(48, i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_3)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 29 and 1 <= ow + kw and ow + kw < 29, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 28, 14, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(28, i3_0 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 24, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"
[23:14:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 28, 28, 4, 192, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[23:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 1, 1, 1, 1, 2, 4, 1, 96, 1, 1, 1, 4, 7, 7, 1, 2, 1, 1, 1, 2, 1, 1, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_0 * 14 + i2_1 * 7 + i2_2)
                    ow = T.axis.spatial(28, i3_1 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(192, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[96, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[23:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 1, 1, 1, 1, 2, 4, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(96, 1, 1, 1, 4, 7, 7, 1, 2, 1, 1, 1, 2, 1, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(192, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 7, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_1 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[96, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 4, 1, 96, 1, 1, 1, 4, 7, 7, 1, 2, 1, 1, 1, 2, 1, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(192, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 28, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 14 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[96, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"
[23:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 30, 30, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 96, 28, 28, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 30, 30, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 1, 1, 1, 16, 7, 2, 1, 1, 1, 1, 6, 1, 7, 2, 3, 3, 1, 1, 1, 2, 2):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(96, i1_1_1 * 6 + i1_2)
                    oh = T.axis.spatial(28, i2_0 * 7 + i2_1_1)
                    ow = T.axis.spatial(28, i3_1_1 * 14 + i3_2 * 2 + i3_3)
                    oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                    T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 6, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:14:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 1, 4, 1, 1, 1, 16, 7, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 3, 16, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(96, i1_1 * 6 + ax1)
                        i2 = T.axis.spatial(30, i2_0 * 7 + i2_1 + ax2)
                        i3 = T.axis.spatial(30, i3_1 * 14 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1 in T.serial(1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 6, 1, 7, 2, 3, 3, 1, 1, 1, 2, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i1_1 * 6 + i1_2)
                            oh = T.axis.spatial(28, i2_0 * 7 + i2_1)
                            ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 2 + i3_3)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 1, 14, 4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(96, i1_1 * 6 + ax1)
                            ax2_1 = T.axis.spatial(28, i2_0 * 7 + i2_1 + ax2)
                            ax3_1 = T.axis.spatial(28, i3_1 * 14 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 6, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 4, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2 in T.grid(1, 16, 7, 2, 1, 1, 1, 1, 6, 1, 7):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 3, 4, 4):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(96, i1_1 * 6 + i1_2 + ax1)
                            i2 = T.axis.spatial(30, i2_0 * 7 + i2_1 + ax2)
                            i3 = T.axis.spatial(30, i3_1 * 14 + i3_2 * 2 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 3, 3, 1, 1, 1, 2, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i1_1 * 6 + i1_2)
                            oh = T.axis.spatial(28, i2_0 * 7 + i2_1)
                            ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 2 + i3_3)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 96, 7, 28, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(28, i2_0 * 7 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 6, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=15)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"
[23:14:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 28, 28, 4, 384, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[23:14:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 14, 1, 1, 1, 2, 1, 1, 192, 1, 1, 1, 2, 14, 2, 4, 2, 1, 1, 1, 2, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_1 * 14 + i2_2)
                    ow = T.axis.spatial(28, i3_0 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(384, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[192, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 14, 1, 1, 1, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(192, 1, 1, 1, 2, 14, 2, 4, 2, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(384, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 2, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_1 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[192, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 14, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 1, 192, 1, 1, 1, 2, 14, 2, 4, 2, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(384, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 2, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[192, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
[23:14:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 96, 28, 28, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 28, 28, 4], "float32"], ["TENSOR", [96, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 7, 1, 1, 6, 1, 1, 1, 2, 1, 1, 1, 4, 2, 2, 2, 32, 1, 1, 1, 2, 2, 2, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(96, i1_0 * 48 + i1_1 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_0 * 4 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(28, i3_0 * 4 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 28, 28, 4], "float32"], ["TENSOR", [96, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 28, 28, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 6, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 7, 7, 1, 1, 6, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 4, 2, 2, 2, 32, 1, 1, 1, 2, 2, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(96, i1_0 * 48 + i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 28, 28, 4], "float32"], ["TENSOR", [96, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 4, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(96, i1_0 * 48 + i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 4 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 6, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 7, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 6, 1, 1, 1, 2, 1, 1, 1, 4, 2, 2, 2, 32, 1, 1, 1, 2, 2, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(96, i1_0 * 48 + i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 28, 28, 4], "float32"], ["TENSOR", [96, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 4, 4, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(96, i1_0 * 48 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 4 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 6, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"
[23:14:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 30, 30, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 96, 14, 14, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 2, 1, 2, 1, 2, 2, 3, 3, 1, 1, 2, 1, 1, 1, 1, 1, 48, 7, 1, 1):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(96, i1_1 * 48 + i1_3)
                    oh = T.axis.spatial(14, i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                    oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                    T.reads(placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 29 and 1 <= ow * 2 + kw and ow * 2 + kw < 29, placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 48])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:14:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 7, 2, 1, 2, 1, 2, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 29, 3, 1):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(96, i1_1 * 48 + ax1)
                        i2 = T.axis.spatial(30, ax2)
                        i3 = T.axis.spatial(30, i3_0 * 4 + i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 2, 1, 1, 1, 1, 1, 48, 7, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i1_1 * 48 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 14, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(96, i1_1 * 48 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 48])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 7, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 2, 1, 2, 2, 3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 27, 1, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(96, i1_1 * 48 + ax1)
                            i2 = T.axis.spatial(30, i5_0 + ax2)
                            i3 = T.axis.spatial(30, i3_0 * 4 + i3_1 * 2 + i6_0 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 1, 1, 1, 1, 48, 7, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i1_1 * 48 + i1_3)
                            oh = T.axis.spatial(14, i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 96, 14, 2, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 48])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"
[23:14:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 24, 14, 14, 4, 384, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[23:14:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 192, 1, 1, 1, 6, 2, 1, 1, 2, 1, 1, 1, 2, 7, 7, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i1_0 * 12 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(14, i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(384, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 24, 14, 14, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 6, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[192, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[23:14:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 2, 1, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(192, 1, 1, 1, 6, 2, 1, 1, 2, 1, 1, 1, 2, 7, 7, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i1_0 * 12 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(384, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 14, 7, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1_0 * 12 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 6, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[192, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 192, 1, 1, 1, 6, 2, 1, 1, 2, 1, 1, 1, 2, 7, 7, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i1_0 * 12 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(384, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 14, 7, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1_0 * 12 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 6, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[192, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"
[23:14:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 16, 16, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 144, 14, 14, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 1, 1, 1, 2, 1, 7, 1, 3, 3, 1, 2, 1, 2, 2, 1, 1, 1, 18, 7, 1, 2):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(144, i1_0 * 72 + i1_1 * 36 + i1_2 * 18 + i1_3)
                    oh = T.axis.spatial(14, i2_0 * 7 + i2_3)
                    ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                    oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                    T.reads(placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 15 and 1 <= ow + kw and ow + kw < 15, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 18])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:14:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 1, 1, 1, 2, 1, 7, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 1, 2, 2, 1, 1, 1, 18, 7, 1, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i1_0 * 72 + i1_1 * 36 + i1_2 * 18 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                        oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 15 and 1 <= ow + kw and ow + kw < 15, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 36, 7, 2, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(144, i1_0 * 72 + i1_1 * 36 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 18])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 72, 16, 16, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(144, i1_0 * 72 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0 in T.grid(2, 1, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 7, 1, 3, 3, 1, 2, 1, 2, 2, 1, 1, 1, 18, 7, 1, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(144, i1_0 * 72 + i1_1 * 36 + i1_2 * 18 + i1_3)
                            oh = T.axis.spatial(14, i2_0 * 7 + i2_3)
                            ow = T.axis.spatial(14, i3_1 * 2 + i3_2)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 72, 7, 14, 4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(144, i1_0 * 72 + ax1)
                            ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                            ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 18])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"
[23:14:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 24, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 24, 14, 14, 4, 576, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[23:14:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 24, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 7, 1, 1, 6, 7, 1, 2, 18, 1, 1, 1, 1, 2, 1, 2, 32, 1, 1, 1, 2, 1, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i1_0 * 12 + i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(14, i3_0 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(576, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 24, 14, 14, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 6, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[18, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 24, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 7, 1, 1, 6, 7, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(18, 1, 1, 1, 1, 2, 1, 2, 32, 1, 1, 1, 2, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i1_0 * 12 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(576, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 2, 2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1_0 * 12 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 6, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[18, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 24, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 6, 7, 1, 2, 18, 1, 1, 1, 1, 2, 1, 2, 32, 1, 1, 1, 2, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i1_0 * 12 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(576, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 14, 2, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1_0 * 12 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 6, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[18, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
[23:14:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 144, 14, 14, 4, 96, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 14, 1, 1, 1, 48, 1, 2, 2, 6, 1, 1, 1, 1, 1, 7, 1, 16, 1, 1, 1, 3, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(144, i1_1 * 3 + i1_3)
                    oh = T.axis.spatial(14, i2_0)
                    ow = T.axis.spatial(14, i3_1 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(96, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 144, 14, 14, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 48, 1, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 14, 1, 1, 1, 48, 1, 2, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(6, 1, 1, 1, 1, 1, 7, 1, 16, 1, 1, 1, 3, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(144, i1_1 * 3 + i1_3)
                        oh = T.axis.spatial(14, i2_0)
                        ow = T.axis.spatial(14, i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(96, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 3, 1, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(144, i1_1 * 3 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 48, 1, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 14, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 48, 1, 2, 2, 6, 1, 1, 1, 1, 1, 7, 1, 16, 1, 1, 1, 3, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(144, i1_1 * 3 + i1_3)
                        oh = T.axis.spatial(14, i2_0)
                        ow = T.axis.spatial(14, i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(96, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 144, 1, 14, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(14, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 48, 1, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"
[23:14:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 16, 16, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 144, 7, 7, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 7, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 144, 15, 3, 2):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(16, ax2)
                        i3 = T.axis.spatial(16, i3_0 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 9, 1, 1, 1, 1, 3, 1, 2, 7, 1, 2, 3, 1, 1, 8, 1, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i1_1 * 16 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_0])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 144, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 9, 2, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 7, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 144, 15, 3, 2):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(16, ax2)
                        i3 = T.axis.spatial(16, i3_0 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 9, 1, 1, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 2, 7, 1, 2, 3, 1, 1, 8, 1, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(144, i1_1 * 16 + i1_2 * 8 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_2, i3_0])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 7, 1, 2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(144, i1_1 * 16 + ax1)
                            ax2_1 = T.axis.spatial(7, ax2)
                            ax3_1 = T.axis.spatial(7, i3_0 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 9, 2, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 144, 16, 16, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 7, 2):
                for i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 9, 1, 1, 1, 1, 3, 1, 2, 7, 1, 2, 3, 1, 1, 8, 1, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i1_1_1 * 16 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_0])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 144, 7, 1, 2):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(7, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 9, 2, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"
[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 40, 7, 7, 4, 576, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 1, 1, 7, 7, 4, 192, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 20, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(40, i1_0 * 20 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_1, i4_1])
                    ic = T.axis.reduce(576, i5_0 * 3 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 40, 7, 7, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 20])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[192, 3])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 1, 7, 7, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(192, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 20, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i1_0 * 20 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_1, i4_1])
                        ic = T.axis.reduce(576, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 20, 1, 1, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(40, i1_0 * 20 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 20])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[192, 3])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 7, 4, 192, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 20, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i1_0 * 20 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_1, i4_1])
                        ic = T.axis.reduce(576, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 20, 7, 7, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(40, i1_0 * 20 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 20])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[192, 3])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"
[23:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 40, 7, 7, 4, 960, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [40, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 20, 1, 1, 1, 1, 1, 1, 1, 4, 120, 1, 1, 1, 1, 7, 7, 1, 8, 1, 1, 1, 2, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(40, i1_0 * 2 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_2, i4_1])
                    ic = T.axis.reduce(960, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [40, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 40, 7, 7, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[20, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[120, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 20, 1, 1, 1, 1, 1, 1, 1, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(120, 1, 1, 1, 1, 7, 7, 1, 8, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i1_0 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_2, i4_1])
                        ic = T.axis.reduce(960, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [40, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 7, 1):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(40, i1_0 * 2 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[20, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[120, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 20, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 4, 120, 1, 1, 1, 1, 7, 7, 1, 8, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i1_0 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_2, i4_1])
                        ic = T.axis.reduce(960, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [40, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 7, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(40, i1_0 * 2 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[20, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[120, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"
[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 240, 7, 7, 4, 160, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 7, 7, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 16, 7, 1, 1, 32, 1, 1, 1, 3, 1, 7, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(240, i1_1 * 48 + i1_2 * 3 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_3, i4_3])
                    ic = T.axis.reduce(160, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 7, 7, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 5, 16, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[5, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 5, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(5, 1, 1, 1, 16, 7, 1, 1, 32, 1, 1, 1, 3, 1, 7, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(240, i1_1 * 48 + i1_2 * 3 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_3, i4_3])
                        ic = T.axis.reduce(160, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 7, 7, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(240, i1_1 * 48 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 5, 16, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[5, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 5, 1, 1, 1, 5, 1, 1, 1, 16, 7, 1, 1, 32, 1, 1, 1, 3, 1, 7, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(240, i1_1 * 48 + i1_2 * 3 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_3, i4_3])
                        ic = T.axis.reduce(160, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 7, 7, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 240, 7, 7, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 5, 16, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[5, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"
[23:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 9, 9, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 240, 7, 7, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 120, 9, 9, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(240, i1_0 * 120 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 10, 1, 1, 1, 1, 3, 1, 4, 7, 1, 2, 3, 1, 1, 3, 1, 7, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i1_0 * 120 + i1_1 * 12 + i1_2 * 3 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 10, 4, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 2, 1, 10, 1, 1, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 4, 7, 1, 2, 3, 1, 1, 3, 1, 7, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i1_0 * 120 + i1_1 * 12 + i1_2 * 3 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 8 and 1 <= ow + kw and ow + kw < 8, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 7, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(240, i1_0 * 120 + i1_1 * 12 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 10, 4, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 10, 1, 1, 1, 1, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 9, 7, 2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(240, i1_0 * 120 + i1_1 * 12 + ax1)
                            i2 = T.axis.spatial(9, ax2)
                            i3 = T.axis.spatial(9, i6_0 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 7, 1, 2, 3, 1, 1, 3, 1, 7, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(240, i1_0 * 120 + i1_1 * 12 + i1_2 * 3 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 120, 7, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(240, i1_0 * 120 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 10, 4, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"
[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(80, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 80, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 80, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 80, 7, 7, 4, 960, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 80, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(80, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 80, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 80, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 10, 1, 1, 2, 1, 2, 1, 1, 1, 120, 1, 1, 1, 2, 1, 7, 1, 8, 1, 1, 1, 2, 7, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(80, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(960, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 80, 7, 7, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[10, 2, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[120, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(80, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 80, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 80, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 10, 1, 1, 2, 1, 2, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(120, 1, 1, 1, 2, 1, 7, 1, 8, 1, 1, 1, 2, 7, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(80, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(960, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 7, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(80, i1_0 * 8 + i1_1 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[10, 2, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[120, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(80, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 80, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 80, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 10, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 120, 1, 1, 1, 2, 1, 7, 1, 8, 1, 1, 1, 2, 7, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(80, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(960, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 7, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(80, i1_0 * 8 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[10, 2, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[120, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"
[23:14:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(320, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 320, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 320, 7, 7, 4, 320, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 7, 7, 4], "float32"], ["TENSOR", [320, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 7, 7, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(320, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 320, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 5, 7, 1, 4, 64, 1, 1, 1, 64, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(320, i1_2 * 64 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_0, i4_2])
                    ic = T.axis.reduce(320, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 7, 7, 4], "float32"], ["TENSOR", [320, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 320, 7, 7, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 5, 64])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[5, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(320, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 320, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 7, 1, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(5, 1, 1, 1, 5, 7, 1, 4, 64, 1, 1, 1, 64, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(320, i1_2 * 64 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_0, i4_2])
                        ic = T.axis.reduce(320, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 7, 7, 4], "float32"], ["TENSOR", [320, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 320, 7, 1, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(7, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 5, 64])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[5, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(320, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 320, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 5, 1, 1, 1, 5, 7, 1, 4, 64, 1, 1, 1, 64, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(320, i1_2 * 64 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_0, i4_2])
                        ic = T.axis.reduce(320, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 7, 7, 4], "float32"], ["TENSOR", [320, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 320, 7, 1, 4):
                    with T.block("T_relu"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(7, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 5, 64])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[5, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #32: "fused_nn_global_avg_pool2d"
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 320, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 320, 1, 1, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 320, 1, 1, 4, 7, 7):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1, ax4])
                T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 1, 1, 4):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] * T.float32(0.020408163265306121)
    

[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 320, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 320, 1, 1, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 320, 1, 1, 4, 7], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 320, 1, 1, 4):
                for ax0 in T.serial(7):
                    for ax0_1, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 1, 1, 7):
                        with T.block("tensor_rf"):
                            vi5_i6_fused_0 = T.axis.spatial(7, ax0 + ax0_1)
                            ax0_2 = T.axis.spatial(1, ax1)
                            ax1_1 = T.axis.spatial(320, i1 + ax2)
                            ax2_1, ax3_1 = T.axis.remap("SS", [ax3, ax4])
                            ax4_1 = T.axis.spatial(4, i4 + ax5)
                            vi5_i6_fused_1 = T.axis.reduce(7, ax6)
                            T.reads(placeholder[ax0_2, ax1_1, ax2_1 * 7 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_1 * 7 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_1])
                            T.writes(tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0])
                            with T.init():
                                tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = T.float32(0)
                            tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] + placeholder[ax0_2, ax1_1, ax2_1 * 7 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_1 * 7 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_1]
                    for ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 1, 1):
                        with T.block("tensor"):
                            vi5_i6_fused_0, ax0_3 = T.axis.remap("RS", [ax0, ax1])
                            ax1_2 = T.axis.spatial(320, i1 + ax2)
                            ax2_2, ax3_2 = T.axis.remap("SS", [ax3, ax4])
                            ax4_2 = T.axis.spatial(4, i4 + ax5)
                            T.reads(tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0])
                            T.writes(tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2])
                            with T.init():
                                tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = T.float32(0)
                            tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] + tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0]
                with T.block("tensor_1"):
                    ax0_4, ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                    T.writes(tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                    tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] = tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[7, 7])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=5)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 320, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 320, 1, 1, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 320, 1, 1, 4, 7], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 320, 1, 1, 4, 7, 7):
                with T.block("tensor_rf"):
                    vi5_i6_fused_1 = T.axis.spatial(7, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(320, i1)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4, vi5_i6_fused_0 = T.axis.remap("SR", [i4, i5_i6_fused_0])
                    T.reads(placeholder[ax0, ax1, ax2 * 7 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3 * 7 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(0)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] + placeholder[ax0, ax1, ax2 * 7 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3 * 7 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4]
            for i0, i1, i2, i3, i4, i5_i6_fused_1 in T.grid(1, 320, 1, 1, 4, 7):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(7, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(320, i1)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, i4)
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1]
            for i0, i1, i2, i3, i4 in T.grid(1, 320, 1, 1, 4):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[7, 7])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=-1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 320, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 320, 1, 1, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 320, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 1, 7, 7):
                    with T.block("tensor"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(320, i1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        rv0, rv1 = T.axis.remap("RR", [ax5, ax6])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 7 + rv0, ax3_1 * 7 + rv1, ax4_1])
                        T.writes(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder[ax0_1, ax1_1, ax2_1 * 7 + rv0, ax3_1 * 7 + rv1, ax4_1]
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #33: "fused_nn_contrib_conv2d_NCHWc"
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 1, 1, 4), "float32"], placeholder_1: T.Buffer[(250, 320, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 250, 1, 1, 4, 1280, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 320, 1, 1, 4], "float32"], ["TENSOR", [250, 320, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 1, 1, 4), "float32"], placeholder_1: T.Buffer[(250, 320, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc_global = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 5, 1, 1, 4, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 50, 1, 1, 1, 40, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(250, i1_0 * 50 + i1_2)
                        oh = T.axis.spatial(1, 0)
                        ow = T.axis.spatial(1, 0)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(1280, i5_0 * 40 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 320, 1, 1, 4], "float32"], ["TENSOR", [250, 320, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 50, 1, 1, 1):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(250, i1_0 * 50 + ax1)
                        v2, v3 = T.axis.remap("SS", [ax2, ax3])
                        v4 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[5, 1, 50, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 40])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 1, 1, 4), "float32"], placeholder_1: T.Buffer[(250, 320, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc_global = T.alloc_buffer([1, 250, 1, 1, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 5, 1, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 32, 1, 1, 1, 50, 1, 1, 1, 40, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(250, i1_0 * 50 + i1_2)
                        oh = T.axis.spatial(1, 0)
                        ow = T.axis.spatial(1, 0)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(1280, i5_0 * 40 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 320, 1, 1, 4], "float32"], ["TENSOR", [250, 320, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc_global[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 50, 1, 1, 1):
                    with T.block("conv2d_NCHWc_global"):
                        v0 = T.axis.spatial(1, ax0)
                        v1 = T.axis.spatial(250, i1_0 * 50 + ax1)
                        v2, v3 = T.axis.remap("SS", [ax2, ax3])
                        v4 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc_global[v0, v1, v2, v3, v4])
                        T.writes(conv2d_NCHWc[v0, v1, v2, v3, v4])
                        conv2d_NCHWc[v0, v1, v2, v3, v4] = conv2d_NCHWc_global[v0, v1, v2, v3, v4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[5, 1, 50, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 40])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 1, 1, 4), "float32"], placeholder_1: T.Buffer[(250, 320, 1, 1, 4, 4), "float32"], conv2d_NCHWc: T.Buffer[(1, 250, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 5, 1, 1, 4, 1, 1, 1, 1, 1, 32, 1, 1, 1, 50, 1, 1, 1, 40, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(250, i1_0 * 50 + i1_2)
                    oh = T.axis.spatial(1, 0)
                    ow = T.axis.spatial(1, 0)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(1280, i5_0 * 40 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 320, 1, 1, 4], "float32"], ["TENSOR", [250, 320, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[5, 1, 50, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 40])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #34: "fused_layout_transform_reshape"
[23:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 1000, 1, 1], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1000, 1, 1):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 1000 and ax2 < 1 and ax3 < 1, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_layout_trans[0, ax1 % 1000, 0, 0])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 1000, 0, 0]
    

[23:14:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:14:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 1000, 1, 1], dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("T_layout_trans"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(1000, i1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 1000 and ax2_1 < 1 and ax3_1 < 1, placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_layout_trans[0, ax1 % 1000, 0, 0])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = T_layout_trans[0, ax1 % 1000, 0, 0]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[23:14:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[23:14:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[23:14:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:14:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:14:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[23:14:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:14:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[23:14:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[23:14:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[23:14:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[23:14:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 4 candidates:
[1 : 4]:	0.8232  0.3906  0.1941  0.0654
[23:14:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 4 candidate(s) with evolutionary search
[23:14:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 4 candidates(s) for measurement
[23:14:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 4 sample(s) to builder
[23:14:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 4 sample(s) to runner
[23:14:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[23:14:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:14:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:15:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde522de8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddf323d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddfe77d8)]: 0 failure(s)
[23:15:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:16:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde522de8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddf323d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddfe77d8)]: 0 failure(s)
[23:18:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde522de8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddf323d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddfe77d8)]: 0 failure(s)
[23:20:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde522de8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddf323d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddfe77d8)]: 0 failure(s)
[23:22:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde522de8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddf323d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddfe77d8)]: 0 failure(s)
[23:22:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9998  0.9998  0.9997  0.9995  0.9995  0.9994  0.9993  0.9993  0.9993  0.9993  0.9991  0.9989  0.9984  0.9984
[17 : 32]:	0.9983  0.9982  0.9978  0.9977  0.9974  0.9974  0.9973  0.9973  0.9973  0.9972  0.9972  0.9971  0.9971  0.9971  0.9968  0.9968
[23:22:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:22:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:22:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:23:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:23:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[23:23:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:23:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:24:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde8d87d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf8ebda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde71b368)]: 0 failure(s)
[23:24:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:24:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde8d87d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf8ebda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde71b368)]: 0 failure(s)
[23:25:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde8d87d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf8ebda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde71b368)]: 0 failure(s)
[23:26:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde8d87d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf8ebda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde71b368)]: 0 failure(s)
[23:27:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde8d87d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf8ebda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde71b368)]: 0 failure(s)
[23:27:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9998  0.9996  0.9996  0.9993  0.9992  0.9990  0.9989  0.9987  0.9987  0.9986  0.9985  0.9981  0.9980  0.9979
[17 : 32]:	0.9978  0.9975  0.9971  0.9971  0.9970  0.9969  0.9967  0.9965  0.9964  0.9963  0.9959  0.9959  0.9958  0.9958  0.9957  0.9956
[23:27:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:27:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:27:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:28:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:28:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"
[23:28:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:28:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:30:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddbf9228)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbfb1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff9858)]: 0 failure(s)
[23:30:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:32:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddbf9228)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbfb1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff9858)]: 0 failure(s)
[23:34:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddbf9228)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbfb1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff9858)]: 0 failure(s)
[23:36:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddbf9228)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbfb1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff9858)]: 0 failure(s)
[23:38:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddbf9228)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbfb1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff9858)]: 0 failure(s)
[23:38:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9996  0.9993  0.9993  0.9992  0.9991  0.9988  0.9987  0.9985  0.9985  0.9983  0.9981  0.9981  0.9981  0.9976
[17 : 32]:	0.9975  0.9975  0.9970  0.9969  0.9969  0.9968  0.9968  0.9968  0.9967  0.9965  0.9964  0.9964  0.9963  0.9963  0.9958  0.9957
[23:38:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:38:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:39:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:39:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_contrib_conv2d_NCHWc_add"
[23:39:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:39:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:40:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddda8768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbaa858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddf4b098)]: 0 failure(s)
[23:40:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:41:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddda8768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbaa858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddf4b098)]: 0 failure(s)
[23:42:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddda8768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbaa858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddf4b098)]: 0 failure(s)
[23:43:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddda8768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbaa858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddf4b098)]: 0 failure(s)
[23:44:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddda8768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbaa858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddf4b098)]: 0 failure(s)
[23:44:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9998  0.9997  0.9997  0.9997  0.9996  0.9994  0.9992  0.9987  0.9983  0.9980  0.9975  0.9974  0.9973  0.9972
[17 : 32]:	0.9970  0.9968  0.9966  0.9963  0.9961  0.9959  0.9956  0.9956  0.9954  0.9954  0.9954  0.9951  0.9951  0.9950  0.9949  0.9948
[23:44:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:44:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:44:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:45:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
[23:45:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:45:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:46:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6e5908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdde54138)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdde5e158)]: 0 failure(s)
[23:46:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:47:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6e5908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdde54138)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdde5e158)]: 0 failure(s)
[23:48:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6e5908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdde54138)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdde5e158)]: 0 failure(s)
[23:49:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6e5908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdde54138)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdde5e158)]: 0 failure(s)
[23:50:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6e5908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdde54138)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdde5e158)]: 0 failure(s)
[23:50:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9994  0.9994  0.9992  0.9992  0.9991  0.9990  0.9988  0.9988  0.9988  0.9986  0.9986  0.9984  0.9984  0.9983
[17 : 32]:	0.9983  0.9982  0.9981  0.9981  0.9980  0.9978  0.9976  0.9974  0.9973  0.9972  0.9969  0.9968  0.9968  0.9966  0.9965  0.9964
[23:50:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:50:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:50:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:50:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:51:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"
[23:51:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:51:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:52:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde845788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde625828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde8a0838)]: 0 failure(s)
[23:52:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[23:55:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde845788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde625828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde8a0838)]: 0 failure(s)
[23:57:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde845788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde625828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde8a0838)]: 0 failure(s)
[23:59:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde845788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde625828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde8a0838)]: 0 failure(s)
[00:01:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde845788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde625828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde8a0838)]: 0 failure(s)
[00:02:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9997  0.9996  0.9994  0.9993  0.9992  0.9990  0.9990  0.9990  0.9986  0.9986  0.9985  0.9985  0.9982
[17 : 32]:	0.9981  0.9979  0.9979  0.9978  0.9976  0.9976  0.9976  0.9975  0.9973  0.9973  0.9972  0.9970  0.9967  0.9965  0.9963  0.9963
[00:02:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:02:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:02:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:03:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:03:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"
[00:03:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:03:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:04:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7cdc68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde556cf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc6cf8)]: 0 failure(s)
[00:04:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:04:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7cdc68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde556cf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc6cf8)]: 0 failure(s)
[00:05:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7cdc68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde556cf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc6cf8)]: 0 failure(s)
[00:06:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7cdc68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde556cf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc6cf8)]: 0 failure(s)
[00:07:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7cdc68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde556cf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc6cf8)]: 0 failure(s)
[00:07:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9995  0.9995  0.9993  0.9989  0.9989  0.9987  0.9987  0.9987  0.9987  0.9986  0.9983  0.9983  0.9982  0.9982  0.9979  0.9978
[17 : 32]:	0.9978  0.9976  0.9975  0.9974  0.9973  0.9972  0.9970  0.9969  0.9969  0.9969  0.9968  0.9966  0.9966  0.9966  0.9963  0.9963
[00:08:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:08:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:08:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:08:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:08:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"
[00:08:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:08:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:10:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde678fc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf5b82f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde6c8f88)]: 0 failure(s)
[00:10:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:12:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde678fc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf5b82f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde6c8f88)]: 0 failure(s)
[00:14:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde678fc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf5b82f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde6c8f88)]: 0 failure(s)
[00:16:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde678fc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf5b82f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde6c8f88)]: 0 failure(s)
[00:19:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde678fc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf5b82f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde6c8f88)]: 0 failure(s)
[00:19:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9999  0.9999  0.9998  0.9997  0.9995  0.9994  0.9992  0.9991  0.9990  0.9988  0.9987  0.9985  0.9980  0.9976
[17 : 32]:	0.9975  0.9971  0.9967  0.9966  0.9964  0.9963  0.9963  0.9961  0.9961  0.9958  0.9957  0.9956  0.9956  0.9955  0.9954  0.9951
[00:19:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:19:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:19:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:20:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:20:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"
[00:20:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:20:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:21:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdde5d048)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde034a08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddbac2d8)]: 0 failure(s)
[00:21:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:22:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdde5d048)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde034a08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddbac2d8)]: 0 failure(s)
[00:22:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdde5d048)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde034a08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddbac2d8)]: 0 failure(s)
[00:23:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdde5d048)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde034a08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddbac2d8)]: 0 failure(s)
[00:24:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdde5d048)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde034a08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddbac2d8)]: 0 failure(s)
[00:25:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9997  0.9997  0.9997  0.9994  0.9993  0.9992  0.9991  0.9990  0.9990  0.9989  0.9989  0.9987  0.9983  0.9981
[17 : 32]:	0.9975  0.9973  0.9973  0.9973  0.9973  0.9971  0.9969  0.9968  0.9968  0.9967  0.9966  0.9964  0.9962  0.9960  0.9959  0.9954
[00:25:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:25:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:25:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:26:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:26:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
[00:26:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:26:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:26:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee53a88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc63c28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd81d98)]: 0 failure(s)
[00:26:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:27:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee53a88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc63c28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd81d98)]: 0 failure(s)
[00:28:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee53a88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc63c28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd81d98)]: 0 failure(s)
[00:29:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee53a88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc63c28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd81d98)]: 0 failure(s)
[00:30:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee53a88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc63c28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd81d98)]: 0 failure(s)
[00:31:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9995  0.9995  0.9992  0.9990  0.9990  0.9989  0.9989  0.9988  0.9988  0.9987  0.9986  0.9986  0.9985  0.9985
[17 : 32]:	0.9982  0.9982  0.9981  0.9981  0.9980  0.9980  0.9980  0.9974  0.9971  0.9967  0.9967  0.9967  0.9965  0.9963  0.9963  0.9962
[00:31:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:31:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:31:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:31:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:32:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"
[00:32:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:32:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:33:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddf2ded8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdfb29c28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddc5d518)]: 0 failure(s)
[00:33:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:36:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddf2ded8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdfb29c28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddc5d518)]: 0 failure(s)
[00:38:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddf2ded8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdfb29c28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddc5d518)]: 0 failure(s)
[00:40:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddf2ded8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdfb29c28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddc5d518)]: 0 failure(s)
[00:42:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddf2ded8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdfb29c28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddc5d518)]: 0 failure(s)
[00:42:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9996  0.9996  0.9996  0.9996  0.9991  0.9989  0.9989  0.9987  0.9987  0.9985  0.9984  0.9983  0.9982
[17 : 32]:	0.9980  0.9979  0.9979  0.9979  0.9977  0.9977  0.9975  0.9975  0.9974  0.9972  0.9971  0.9971  0.9969  0.9968  0.9967  0.9965
[00:42:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:42:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:42:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:43:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:43:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"
[00:43:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:43:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:44:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdfb24198)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf20d558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdde5a518)]: 0 failure(s)
[00:44:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:45:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdfb24198)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf20d558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdde5a518)]: 0 failure(s)
[00:46:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdfb24198)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf20d558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdde5a518)]: 0 failure(s)
[00:47:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdfb24198)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf20d558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdde5a518)]: 0 failure(s)
[00:48:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdfb24198)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf20d558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdde5a518)]: 0 failure(s)
[00:48:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9998  0.9996  0.9996  0.9995  0.9993  0.9992  0.9990  0.9989  0.9989  0.9987  0.9985  0.9985  0.9984  0.9981
[17 : 32]:	0.9981  0.9979  0.9979  0.9978  0.9975  0.9975  0.9975  0.9971  0.9971  0.9970  0.9969  0.9969  0.9969  0.9967  0.9966  0.9965
[00:48:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:48:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:48:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:49:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:49:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"
[00:49:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:49:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:50:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf90a6b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddda4ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde642cc8)]: 0 failure(s)
[00:50:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:50:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf90a6b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddda4ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde642cc8)]: 0 failure(s)
[00:51:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf90a6b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddda4ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde642cc8)]: 0 failure(s)
[00:52:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf90a6b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddda4ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde642cc8)]: 0 failure(s)
[00:53:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf90a6b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddda4ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde642cc8)]: 0 failure(s)
[00:54:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9998  0.9997  0.9987  0.9986  0.9984  0.9984  0.9984  0.9983  0.9982  0.9981  0.9980  0.9979  0.9978  0.9974
[17 : 32]:	0.9974  0.9974  0.9972  0.9971  0.9970  0.9969  0.9967  0.9966  0.9966  0.9964  0.9963  0.9962  0.9961  0.9960  0.9960  0.9956
[00:54:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:54:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:54:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:54:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:55:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
[00:55:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:55:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:55:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf54d8b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde6ba598)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc9428)]: 0 failure(s)
[00:55:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[00:57:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf54d8b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde6ba598)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc9428)]: 0 failure(s)
[00:57:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf54d8b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde6ba598)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc9428)]: 0 failure(s)
[00:58:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf54d8b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde6ba598)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc9428)]: 0 failure(s)
[00:59:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf54d8b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde6ba598)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc9428)]: 0 failure(s)
[01:00:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9996  0.9995  0.9992  0.9992  0.9991  0.9991  0.9988  0.9988  0.9988  0.9986  0.9984  0.9983  0.9983  0.9980
[17 : 32]:	0.9979  0.9979  0.9978  0.9977  0.9977  0.9976  0.9975  0.9972  0.9972  0.9971  0.9970  0.9968  0.9968  0.9967  0.9964  0.9963
[01:00:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:00:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:00:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:00:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:01:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"
[01:01:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:01:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:03:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddf4e7a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7edeb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf8ecd48)]: 0 failure(s)
[01:03:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:05:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddf4e7a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7edeb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf8ecd48)]: 0 failure(s)
[01:07:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddf4e7a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7edeb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf8ecd48)]: 0 failure(s)
[01:09:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddf4e7a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7edeb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf8ecd48)]: 0 failure(s)
[01:11:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddf4e7a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7edeb8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf8ecd48)]: 0 failure(s)
[01:12:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9998  0.9997  0.9997  0.9997  0.9995  0.9995  0.9994  0.9994  0.9993  0.9991  0.9988  0.9988  0.9987  0.9985
[17 : 32]:	0.9985  0.9985  0.9984  0.9984  0.9982  0.9982  0.9976  0.9975  0.9973  0.9972  0.9970  0.9969  0.9967  0.9967  0.9966  0.9966
[01:12:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:12:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:12:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:13:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:13:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"
[01:13:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:13:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:14:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddfe55b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acded0c148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde89f448)]: 0 failure(s)
[01:14:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:14:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddfe55b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acded0c148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde89f448)]: 0 failure(s)
[01:15:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddfe55b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acded0c148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde89f448)]: 0 failure(s)
[01:16:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddfe55b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acded0c148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde89f448)]: 0 failure(s)
[01:17:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddfe55b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acded0c148)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde89f448)]: 0 failure(s)
[01:18:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9997  0.9997  0.9996  0.9996  0.9995  0.9993  0.9993  0.9990  0.9989  0.9989  0.9988  0.9987  0.9987  0.9984
[17 : 32]:	0.9984  0.9983  0.9982  0.9981  0.9981  0.9980  0.9980  0.9978  0.9978  0.9978  0.9978  0.9978  0.9977  0.9974  0.9974  0.9973
[01:18:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:18:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:18:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:19:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"
[01:19:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:19:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:20:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde033298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf229968)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff3cd8)]: 0 failure(s)
[01:20:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:22:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde033298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf229968)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff3cd8)]: 0 failure(s)
[01:24:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde033298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf229968)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff3cd8)]: 0 failure(s)
[01:26:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde033298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf229968)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff3cd8)]: 0 failure(s)
[01:28:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde033298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf229968)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff3cd8)]: 0 failure(s)
[01:28:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9998  0.9994  0.9993  0.9990  0.9989  0.9987  0.9986  0.9985  0.9985  0.9985  0.9984  0.9984  0.9979  0.9978  0.9978
[17 : 32]:	0.9976  0.9976  0.9975  0.9974  0.9974  0.9974  0.9974  0.9973  0.9972  0.9971  0.9970  0.9970  0.9970  0.9966  0.9966  0.9964
[01:28:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:28:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:29:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:29:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:29:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"
[01:29:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:29:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:30:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde687cc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf2c2408)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde5de028)]: 0 failure(s)
[01:30:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:31:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde687cc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf2c2408)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde5de028)]: 0 failure(s)
[01:32:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde687cc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf2c2408)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde5de028)]: 0 failure(s)
[01:33:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde687cc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf2c2408)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde5de028)]: 0 failure(s)
[01:34:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde687cc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf2c2408)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde5de028)]: 0 failure(s)
[01:35:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9997  0.9996  0.9991  0.9989  0.9988  0.9987  0.9986  0.9986  0.9985  0.9985  0.9984  0.9983  0.9983
[17 : 32]:	0.9977  0.9976  0.9973  0.9972  0.9971  0.9966  0.9966  0.9964  0.9963  0.9962  0.9962  0.9958  0.9956  0.9955  0.9954  0.9953
[01:35:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:35:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:35:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:36:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:36:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
[01:36:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:36:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:36:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee411c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde901c38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde02e1a8)]: 0 failure(s)
[01:36:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:37:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee411c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde901c38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde02e1a8)]: 0 failure(s)
[01:38:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee411c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde901c38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde02e1a8)]: 0 failure(s)
[01:39:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee411c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde901c38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde02e1a8)]: 0 failure(s)
[01:40:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee411c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde901c38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde02e1a8)]: 0 failure(s)
[01:41:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9997  0.9996  0.9993  0.9993  0.9990  0.9989  0.9989  0.9988  0.9986  0.9985  0.9983  0.9983  0.9982  0.9982  0.9979
[17 : 32]:	0.9976  0.9975  0.9973  0.9971  0.9971  0.9970  0.9969  0.9968  0.9965  0.9964  0.9964  0.9958  0.9956  0.9956  0.9955  0.9952
[01:41:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:41:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:41:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:41:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:42:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"
[01:42:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:42:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:43:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5f3d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdddca178)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf2e98f8)]: 0 failure(s)
[01:43:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:45:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5f3d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdddca178)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf2e98f8)]: 0 failure(s)
[01:47:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5f3d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdddca178)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf2e98f8)]: 0 failure(s)
[01:49:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5f3d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdddca178)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf2e98f8)]: 0 failure(s)
[01:51:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5f3d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdddca178)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf2e98f8)]: 0 failure(s)
[01:51:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9996  0.9996  0.9993  0.9993  0.9992  0.9992  0.9991  0.9986  0.9985  0.9984  0.9983  0.9979  0.9975  0.9974
[17 : 32]:	0.9973  0.9973  0.9964  0.9961  0.9961  0.9960  0.9959  0.9958  0.9956  0.9956  0.9956  0.9952  0.9952  0.9951  0.9949  0.9949
[01:51:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:51:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:51:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:52:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:52:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"
[01:52:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:52:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:53:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6df7d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf5b74e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddb60d8)]: 0 failure(s)
[01:53:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:54:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6df7d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf5b74e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddb60d8)]: 0 failure(s)
[01:55:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6df7d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf5b74e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddb60d8)]: 0 failure(s)
[01:56:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6df7d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf5b74e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddb60d8)]: 0 failure(s)
[01:58:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6df7d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf5b74e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddb60d8)]: 0 failure(s)
[01:58:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9998  0.9997  0.9996  0.9995  0.9994  0.9993  0.9992  0.9990  0.9989  0.9986  0.9985  0.9985  0.9983  0.9976
[17 : 32]:	0.9975  0.9974  0.9974  0.9974  0.9972  0.9971  0.9971  0.9968  0.9964  0.9964  0.9963  0.9958  0.9957  0.9957  0.9957  0.9956
[01:58:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:58:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:58:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:58:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:59:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"
[01:59:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:59:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:00:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf5ad0a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde784bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc5a98)]: 0 failure(s)
[02:00:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:02:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf5ad0a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde784bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc5a98)]: 0 failure(s)
[02:04:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf5ad0a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde784bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc5a98)]: 0 failure(s)
[02:06:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf5ad0a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde784bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc5a98)]: 0 failure(s)
[02:08:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf5ad0a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde784bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdddc5a98)]: 0 failure(s)
[02:08:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9997  0.9991  0.9989  0.9988  0.9987  0.9986  0.9985  0.9984  0.9984  0.9981  0.9980  0.9979  0.9979  0.9975  0.9975
[17 : 32]:	0.9974  0.9974  0.9971  0.9969  0.9969  0.9968  0.9967  0.9964  0.9964  0.9962  0.9961  0.9959  0.9957  0.9957  0.9956  0.9954
[02:08:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:08:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:08:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:09:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:09:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"
[02:09:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:09:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:10:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5a7cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdee593c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf229d08)]: 0 failure(s)
[02:10:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:11:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5a7cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdee593c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf229d08)]: 0 failure(s)
[02:11:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5a7cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdee593c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf229d08)]: 0 failure(s)
[02:12:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5a7cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdee593c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf229d08)]: 0 failure(s)
[02:13:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5a7cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdee593c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf229d08)]: 0 failure(s)
[02:14:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9998  0.9997  0.9996  0.9994  0.9994  0.9993  0.9993  0.9992  0.9992  0.9990  0.9989  0.9987  0.9987  0.9986  0.9985
[17 : 32]:	0.9984  0.9982  0.9982  0.9981  0.9981  0.9981  0.9980  0.9977  0.9977  0.9977  0.9975  0.9975  0.9974  0.9974  0.9974  0.9973
[02:14:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:14:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:14:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:14:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:14:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
[02:14:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:14:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:15:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde8d7e38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdecd04b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdeb4c0a8)]: 0 failure(s)
[02:15:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:16:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde8d7e38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdecd04b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdeb4c0a8)]: 0 failure(s)
[02:16:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde8d7e38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdecd04b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdeb4c0a8)]: 0 failure(s)
[02:17:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde8d7e38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdecd04b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdeb4c0a8)]: 0 failure(s)
[02:18:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde8d7e38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdecd04b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdeb4c0a8)]: 0 failure(s)
[02:19:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9998  0.9997  0.9997  0.9996  0.9994  0.9992  0.9992  0.9991  0.9990  0.9988  0.9987  0.9986  0.9986  0.9985
[17 : 32]:	0.9984  0.9984  0.9984  0.9983  0.9983  0.9981  0.9977  0.9976  0.9971  0.9966  0.9966  0.9964  0.9964  0.9963  0.9962  0.9961
[02:19:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:19:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:19:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:20:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:20:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"
[02:20:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:20:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:21:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdddcb2e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf92e188)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddd3e478)]: 0 failure(s)
[02:21:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:23:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdddcb2e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf92e188)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddd3e478)]: 0 failure(s)
[02:25:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdddcb2e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf92e188)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddd3e478)]: 0 failure(s)
[02:26:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdddcb2e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf92e188)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddd3e478)]: 0 failure(s)
[02:28:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdddcb2e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf92e188)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddd3e478)]: 0 failure(s)
[02:28:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9996  0.9996  0.9996  0.9993  0.9993  0.9992  0.9991  0.9988  0.9987  0.9987  0.9986  0.9985  0.9984  0.9984
[17 : 32]:	0.9983  0.9980  0.9979  0.9979  0.9979  0.9978  0.9977  0.9975  0.9973  0.9972  0.9969  0.9969  0.9968  0.9967  0.9965  0.9964
[02:28:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:28:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:28:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:29:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:29:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"
[02:29:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:29:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:30:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdde532c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdddc2838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf32a398)]: 0 failure(s)
[02:30:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:30:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdde532c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdddc2838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf32a398)]: 0 failure(s)
[02:31:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdde532c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdddc2838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf32a398)]: 0 failure(s)
[02:32:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdde532c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdddc2838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf32a398)]: 0 failure(s)
[02:32:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdde532c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdddc2838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf32a398)]: 0 failure(s)
[02:33:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9995  0.9995  0.9993  0.9993  0.9990  0.9990  0.9989  0.9988  0.9987  0.9987  0.9986  0.9985  0.9984  0.9983
[17 : 32]:	0.9980  0.9977  0.9976  0.9975  0.9974  0.9972  0.9972  0.9970  0.9969  0.9969  0.9968  0.9967  0.9966  0.9966  0.9965  0.9965
[02:33:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:33:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:33:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:33:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:34:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"
[02:34:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:34:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:34:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee0dda8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde649688)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf5b21f8)]: 0 failure(s)
[02:34:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:35:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee0dda8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde649688)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf5b21f8)]: 0 failure(s)
[02:36:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee0dda8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde649688)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf5b21f8)]: 0 failure(s)
[02:37:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee0dda8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde649688)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf5b21f8)]: 0 failure(s)
[02:38:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdee0dda8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde649688)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf5b21f8)]: 0 failure(s)
[02:39:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9999  0.9998  0.9997  0.9997  0.9995  0.9995  0.9993  0.9992  0.9991  0.9991  0.9990  0.9989  0.9986  0.9984
[17 : 32]:	0.9984  0.9983  0.9981  0.9980  0.9980  0.9979  0.9979  0.9976  0.9975  0.9973  0.9972  0.9970  0.9968  0.9968  0.9964  0.9964
[02:39:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:39:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:39:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:39:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:40:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"
[02:40:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:40:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:40:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde877c98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c80e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf92faa8)]: 0 failure(s)
[02:40:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:41:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde877c98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c80e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf92faa8)]: 0 failure(s)
[02:42:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde877c98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c80e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf92faa8)]: 0 failure(s)
[02:43:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde877c98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c80e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf92faa8)]: 0 failure(s)
[02:44:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde877c98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c80e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf92faa8)]: 0 failure(s)
[02:44:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9997  0.9996  0.9995  0.9994  0.9993  0.9990  0.9989  0.9985  0.9985  0.9984  0.9984  0.9980  0.9980  0.9980
[17 : 32]:	0.9979  0.9979  0.9979  0.9979  0.9978  0.9978  0.9978  0.9976  0.9975  0.9975  0.9974  0.9973  0.9970  0.9969  0.9968  0.9968
[02:44:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:44:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:44:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:45:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"
[02:45:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:45:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:46:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6e5f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf94e8a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddc01f78)]: 0 failure(s)
[02:46:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:48:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6e5f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf94e8a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddc01f78)]: 0 failure(s)
[02:49:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6e5f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf94e8a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddc01f78)]: 0 failure(s)
[02:51:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6e5f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf94e8a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddc01f78)]: 0 failure(s)
[02:53:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde6e5f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf94e8a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddc01f78)]: 0 failure(s)
[02:53:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9992  0.9991  0.9991  0.9990  0.9989  0.9989  0.9986  0.9984  0.9979  0.9979  0.9977  0.9975  0.9973  0.9972
[17 : 32]:	0.9972  0.9971  0.9970  0.9969  0.9968  0.9962  0.9962  0.9961  0.9961  0.9960  0.9959  0.9959  0.9958  0.9957  0.9957  0.9956
[02:53:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:53:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:53:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:54:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:54:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"
[02:54:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:54:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:54:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf947b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde843f08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde71c0e8)]: 0 failure(s)
[02:54:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:55:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf947b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde843f08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde71c0e8)]: 0 failure(s)
[02:56:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf947b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde843f08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde71c0e8)]: 0 failure(s)
[02:57:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf947b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde843f08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde71c0e8)]: 0 failure(s)
[02:58:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acdf947b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde843f08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde71c0e8)]: 0 failure(s)
[02:58:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  1.0000  0.9999  0.9999  0.9996  0.9995  0.9994  0.9993  0.9990  0.9987  0.9985  0.9982  0.9981  0.9980  0.9979  0.9978
[17 : 32]:	0.9978  0.9977  0.9973  0.9973  0.9972  0.9971  0.9971  0.9970  0.9969  0.9968  0.9967  0.9964  0.9963  0.9963  0.9963  0.9961
[02:58:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:58:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:58:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:59:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:59:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"
[02:59:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:59:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:00:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5f85e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf334d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff93b8)]: 0 failure(s)
[03:00:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[03:00:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5f85e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf334d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff93b8)]: 0 failure(s)
[03:01:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5f85e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf334d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff93b8)]: 0 failure(s)
[03:02:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5f85e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf334d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff93b8)]: 0 failure(s)
[03:02:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde5f85e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acdf334d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acddff93b8)]: 0 failure(s)
[03:03:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9998  0.9998  0.9997  0.9996  0.9996  0.9995  0.9994  0.9991  0.9986  0.9983  0.9982  0.9981  0.9981
[17 : 32]:	0.9981  0.9981  0.9979  0.9978  0.9977  0.9976  0.9975  0.9974  0.9973  0.9973  0.9971  0.9970  0.9969  0.9969  0.9968  0.9968
[03:03:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:03:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:03:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:03:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:04:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #32: "fused_nn_global_avg_pool2d"
[03:04:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:04:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:04:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddc5c708)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde6e06b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7a0a98)]: 0 failure(s)
[03:04:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[03:05:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddc5c708)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde6e06b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7a0a98)]: 0 failure(s)
[03:05:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddc5c708)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde6e06b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7a0a98)]: 0 failure(s)
[03:06:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddc5c708)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde6e06b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7a0a98)]: 0 failure(s)
[03:07:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddc5c708)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde6e06b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7a0a98)]: 0 failure(s)
[03:07:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9985  0.9965  0.9954  0.9915  0.9913  0.9905  0.9870  0.9765  0.9638  0.9575  0.9553  0.9526  0.9481  0.9465  0.9426  0.9366
[17 : 32]:	0.9350  0.9339  0.9313  0.9309  0.9295  0.9291  0.9247  0.9200  0.9139  0.8966  0.8904  0.8875  0.8832  0.8782  0.8698  0.8522
[03:07:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:07:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:07:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:07:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:09:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #33: "fused_nn_contrib_conv2d_NCHWc"
[03:09:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:09:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:09:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddbff598)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbfb4a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf254f88)]: 0 failure(s)
[03:09:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[03:10:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddbff598)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbfb4a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf254f88)]: 0 failure(s)
[03:10:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddbff598)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbfb4a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf254f88)]: 0 failure(s)
[03:11:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddbff598)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbfb4a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf254f88)]: 0 failure(s)
[03:12:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acddbff598)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddbfb4a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdf254f88)]: 0 failure(s)
[03:12:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9998  0.9996  0.9995  0.9995  0.9994  0.9994  0.9994  0.9993  0.9991  0.9991  0.9991  0.9990  0.9988  0.9987
[17 : 32]:	0.9984  0.9983  0.9982  0.9976  0.9975  0.9975  0.9971  0.9967  0.9965  0.9963  0.9962  0.9960  0.9959  0.9958  0.9956  0.9954
[03:12:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:12:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:12:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:12:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:13:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #34: "fused_layout_transform_reshape"
[03:13:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:13:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:13:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:13:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[03:13:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:13:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:13:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:13:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:14:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 12 candidates:
[1 : 12]:	0.8499  0.8296  0.8115  0.7568  0.5889  0.5340  0.4692  0.2924  0.2883  0.2244  0.2222  0.1169
[03:14:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 12 candidate(s) with evolutionary search
[03:14:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 12 candidates(s) for measurement
[03:14:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 12 sample(s) to builder
[03:14:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 12 sample(s) to runner
[03:14:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 0.6423 ms. Best GFLOPs: 0.0000
[03:14:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 0.0423 ms. Best GFLOPs: 0.0000
[03:14:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 0.0899 ms. Best GFLOPs: 0.0000
[03:14:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 0.0612 ms. Best GFLOPs: 0.0000
[03:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 4
Total latency (us): 42.3054

[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #0: GFLOPs: 2.4625. Time: 9.1285 ms. Best GFLOPs: 2.4625
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #1: GFLOPs: 2.1520. Time: 10.4455 ms. Best GFLOPs: 2.4625
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #2: GFLOPs: 3.1788. Time: 7.0714 ms. Best GFLOPs: 3.1788
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #3: GFLOPs: 2.2859. Time: 9.8336 ms. Best GFLOPs: 3.1788
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #4: GFLOPs: 2.2151. Time: 10.1478 ms. Best GFLOPs: 3.1788
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 4, 2):
                for i1_2_init, i2_2_init, i3_2_init, i1_3_init in T.grid(2, 56, 14, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 4 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 56 + i2_2_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 56 + i3_1 * 14 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 113, 29, 1):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 112 + ax2)
                            i3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 112 + i3_1 * 28 + ax3)
                            i4 = T.axis.spatial(3, i5_0 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 225 and 1 <= i3 and i3 < 225, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 56, 14, 1, 1, 1, 1, 1, 2, 1, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 56 + i2_2)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 56 + i3_1 * 14 + i3_2)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 56, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 4, 14, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=8)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b67)
l86 = sch.fuse(l70, l71, l72, l73, l74)
sch.parallel(loop=l86)
sch.annotate(block_or_loop=l86, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l86, ann_key="pragma_unroll_explicit", ann_val=1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l109, l110, l111, l112, l113 = sch.get_loops(block=b69)
l114 = sch.fuse(l109, l110, l111)
sch.parallel(loop=l114)
l115 = sch.fuse(l113)
sch.vectorize(loop=l115)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
b116 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b116)
b139 = sch.decompose_reduction(block=b116, loop=l123)
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #6: GFLOPs: 3.1404. Time: 7.1579 ms. Best GFLOPs: 3.1788
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #7: GFLOPs: 2.8699. Time: 7.8327 ms. Best GFLOPs: 3.1788
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 225, 225):
                for ax4_fused in T.vectorized(3):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(1, 0)
                        i2 = T.axis.spatial(226, ax2)
                        i3 = T.axis.spatial(226, ax3)
                        i4 = T.axis.spatial(3, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 225 and 1 <= i3 and i3 < 225, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 14, 14, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 4, 4, 1, 1):
                    for i2_3_init, i3_3_init in T.grid(2, 8):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 4 + i1_2)
                            oh = T.axis.spatial(112, i2_1 * 8 + i2_2 * 2 + i2_3_init)
                            ow = T.axis.spatial(112, i3_1 * 8 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 3, 1, 1, 2, 8, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 4 + i1_2)
                            oh = T.axis.spatial(112, i2_1 * 8 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(112, i3_1 * 8 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                            ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 8, 8, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 4 + ax1)
                        ax2_1 = T.axis.spatial(112, i2_1 * 8 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_1 * 8 + ax3)
                        ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 4, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 14, 1, 8])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b68)
l81 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l81)
l82 = sch.fuse(l80)
sch.vectorize(loop=l82)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b116 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b116)
b139 = sch.decompose_reduction(block=b116, loop=l131)
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #9: GFLOPs: 2.0963. Time: 10.7229 ms. Best GFLOPs: 3.1788
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #10: GFLOPs: 2.2967. Time: 9.7876 ms. Best GFLOPs: 3.1788
[03:15:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 225, 225):
                for ax4_fused in T.vectorized(3):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(1, 0)
                        i2 = T.axis.spatial(226, ax2)
                        i3 = T.axis.spatial(226, ax3)
                        i4 = T.axis.spatial(3, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 225 and 1 <= i3 and i3 < 225, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(8, 4, 4, 1, 1):
                for i1_2_init, i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 7, 2, 2, 28):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused * 4 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(112, i2_1 * 14 + i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(112, i3_1 * 28 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 7, 1, 1, 3, 3, 1, 1, 2, 2, 28, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(112, i3_1 * 28 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_1, i5_1, i6_1, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 7, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 28])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b67)
l82 = sch.fuse(l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l82)
l83 = sch.fuse(l81)
sch.vectorize(loop=l83)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b111)
b132 = sch.decompose_reduction(block=b111, loop=l118)
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #12: GFLOPs: 1.2273. Time: 18.3151 ms. Best GFLOPs: 3.1788
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(3584, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i1_2_init, i2_2_init, i3_2_init, i2_3_init in T.grid(2, 7, 2, 2):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 896 * 2 + i1_2_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 448 // 56 * 14 + i2_2_init * 2 + i2_3_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 * 2 + i3_2_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 896 // 448 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0 in T.grid(3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 27, 5, 1):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 448 // 56 * 28 + i6_0 + ax2)
                            i3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 * 4 + ax3)
                            i4 = T.axis.spatial(3, i5_0 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 225 and 1 <= i3 and i3 < 225, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(3, 1, 2, 7, 2, 1, 1, 1, 1, 1, 1, 2):
                        for i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 896 * 2 + i1_2)
                                oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 448 // 56 * 14 + i2_2 * 2 + i2_3)
                                ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 * 2 + i3_2)
                                oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 896 // 448 * 2 + i3_3_i4_3_fused)
                                ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 7, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 56, 2, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b67)
l87 = sch.fuse(l70, l71, l72, l73, l74, l75, l76, l77, l78)
sch.parallel(loop=l87)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b68)
l106 = sch.fuse(l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111 = sch.get_loops(block=b69)
l112 = sch.fuse(l107, l108, l109)
sch.parallel(loop=l112)
l113 = sch.fuse(l111)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b114)
b132 = sch.decompose_reduction(block=b114, loop=l117)
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(226, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(226):
                for i4_fused in T.vectorized(3):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(1, 0)
                        i2, i3_1, i4 = T.axis.remap("SSS", [i0_i1_i2_fused, i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3_1, i4])
                        data_pad[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 225 and 1 <= i3_1 and i3_1 < 225, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(8, 2, 8, 56):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_3_init)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused // 4 * 8 + i2_2_init)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused % 4 // 2 * 56 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 8, 1, 2, 3, 1, 1, 1, 8, 1, 56, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_3)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused // 4 * 8 + i2_2)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused % 4 // 2 * 56 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_fused % 2 * 2 + i4_2)
                    ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_0, i7_0])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 8, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 56])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74 = sch.get_loops(block=b67)
l75 = sch.fuse(l70, l71, l72)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b68)
l103 = sch.fuse(l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87)
sch.parallel(loop=l103)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b111)
b128 = sch.decompose_reduction(block=b111, loop=l113)
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(226, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(226):
                for i4_fused in T.vectorized(3):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(1, 0)
                        i2, i3_1, i4 = T.axis.remap("SSS", [i0_i1_i2_fused, i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3_1, i4])
                        data_pad[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 225 and 1 <= i3_1 and i3_1 < 225, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1_1, i4_1 in T.grid(2, 4, 1):
                for i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(7, 2, 4, 4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 4 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 16 * 8 + i2_1 * 4 + i2_3_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 4 * 28 + i3_1_1 * 7 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 // 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 3, 1, 1, 1, 7, 2, 1, 1, 1, 1, 4, 4, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 4 + i1_3)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 16 * 8 + i2_1 * 4 + i2_3)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 // 4 * 28 + i3_1_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 // 2 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 4])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 4, 7, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74 = sch.get_loops(block=b67)
l75 = sch.fuse(l70, l71, l72)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b68)
l103 = sch.fuse(l77, l78, l79, l80, l81, l82, l83)
sch.parallel(loop=l103)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b111)
b132 = sch.decompose_reduction(block=b111, loop=l116)
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #16: GFLOPs: 3.0275. Time: 7.4248 ms. Best GFLOPs: 3.1788
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i5_0 in T.serial(1):
                for i1_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(8, 14, 14, 2):
                    for i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_2_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + i2_3_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 28 + i3_2_init * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i6_0, i7_0 in T.grid(3, 3):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 27, 55):
                        for ax4_fused in T.vectorized(3):
                            with T.block("data_pad"):
                                i0, i1 = T.axis.remap("SS", [ax0, ax1])
                                i2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 8 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + i6_0 + ax2)
                                i3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 112 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 56 + i7_0 + ax3)
                                i4 = T.axis.spatial(3, ax4_fused)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(data_pad[i0, i1, i2, i3, i4])
                                data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 225 and 1 <= i3 and i3 < 225, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 8, 1, 14, 1, 3, 1, 1, 1, 1, 14, 2):
                        for i4_3_fused in T.vectorized(4):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(8, i1_2)
                                oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + i2_3)
                                ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 28 + i3_2 * 2 + i3_3)
                                oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_3_fused, i5_1, i6_0, i7_0])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 14, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, ax1)
                        ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + ax2)
                        ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 28 + ax3)
                        ax4 = T.axis.spatial(4, ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 8, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 1, 14])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 14, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b68)
l89 = sch.fuse(l71, l72, l73, l74, l75, l76, l77, l78, l79, l80)
sch.parallel(loop=l89)
l90 = sch.fuse(l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b69)
l108 = sch.fuse(l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b70)
l115 = sch.fuse(l114)
sch.vectorize(loop=l115)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b116 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b116)
b134 = sch.decompose_reduction(block=b116, loop=l119)
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #18: GFLOPs: 3.4444. Time: 6.5261 ms. Best GFLOPs: 3.4444
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #19: GFLOPs: 0.4884. Time: 46.0215 ms. Best GFLOPs: 3.4444
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #20: GFLOPs: 1.7033. Time: 13.1975 ms. Best GFLOPs: 3.4444
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #21: GFLOPs: 4.2917. Time: 5.2378 ms. Best GFLOPs: 4.2917
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #22: GFLOPs: 2.4304. Time: 9.2492 ms. Best GFLOPs: 4.2917
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 225, 225):
                for ax4_fused in T.vectorized(3):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(1, 0)
                        i2 = T.axis.spatial(226, ax2)
                        i3 = T.axis.spatial(226, ax3)
                        i4 = T.axis.spatial(3, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 225 and 1 <= i3 and i3 < 225, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0 in T.grid(2, 4, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 1, 4, 1, 1, 1):
                    for i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(14, 7, 2, 4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_fused * 2 + i1_3_init)
                            oh = T.axis.spatial(112, i2_0 * 56 + i2_1 * 14 + i2_2_init)
                            ow = T.axis.spatial(112, i3_0 * 28 + i3_2_init * 4 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_0)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 14, 7, 1, 3, 1, 3, 1, 2, 1, 4, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_fused * 2 + i1_3)
                            oh = T.axis.spatial(112, i2_0 * 56 + i2_1 * 14 + i2_2)
                            ow = T.axis.spatial(112, i3_0 * 28 + i3_2 * 4 + i3_3)
                            oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_0, i5_1, i6_0, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 56, 28, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_fused * 2 + ax1)
                        ax2_1 = T.axis.spatial(112, i2_0 * 56 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 * 28 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 1, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 4, 14, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 7, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b68)
l78 = sch.fuse(l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b114)
b140 = sch.decompose_reduction(block=b114, loop=l125)
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #24: GFLOPs: 2.5385. Time: 8.8550 ms. Best GFLOPs: 4.2917
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #25: GFLOPs: 0.9917. Time: 22.6665 ms. Best GFLOPs: 4.2917
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #26: GFLOPs: 2.1677. Time: 10.3700 ms. Best GFLOPs: 4.2917
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #27: GFLOPs: 1.6386. Time: 13.7181 ms. Best GFLOPs: 4.2917
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #28: GFLOPs: 2.5212. Time: 8.9159 ms. Best GFLOPs: 4.2917
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #29: GFLOPs: 2.8798. Time: 7.8056 ms. Best GFLOPs: 4.2917
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #30: GFLOPs: 2.8102. Time: 7.9989 ms. Best GFLOPs: 4.2917
[03:15:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #31: GFLOPs: 2.7337. Time: 8.2230 ms. Best GFLOPs: 4.2917
[03:15:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 36
Total latency (us): 5280.11

[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #0: GFLOPs: 1.9386. Time: 13.6660 ms. Best GFLOPs: 1.9386
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #1: GFLOPs: 1.7584. Time: 15.0668 ms. Best GFLOPs: 1.9386
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #2: GFLOPs: 2.1629. Time: 12.2488 ms. Best GFLOPs: 2.1629
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #3: GFLOPs: 0.4274. Time: 61.9919 ms. Best GFLOPs: 2.1629
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(14, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 1, 8, 2, 1):
                    for i1_3_init in T.serial(4):
                        for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 4 + i1_3_init)
                                oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 8 + i2_2)
                                ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 14 * 28 + i3_1 * 2 + i3_2)
                                oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 7 * 2 + i2_3_i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(32, 1, 1, 1, 4):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 4 + i1_3)
                                oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 8 + i2_2)
                                ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 14 * 28 + i3_1 * 2 + i3_2)
                                oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 7 * 2 + i2_3_i3_3_i4_3_fused)
                                ic = T.axis.reduce(32, i5_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 8, 2):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 4 + ax1)
                            ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 8 + ax2)
                            ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 14 * 28 + i3_1 * 2 + ax3)
                            ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 7 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 8, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 14, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l94)
l95 = sch.fuse(l91, l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b67)
l104 = sch.fuse(l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b105)
b123 = sch.decompose_reduction(block=b105, loop=l117)
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(2, 4, 1):
                for i2_2_init, i3_2_init, i4_2_init, i2_3_init in T.grid(2, 4, 2, 7):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 28 * 28 + i2_1 * 14 + i2_2_init * 7 + i2_3_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 4 * 16 + i3_1 * 4 + i3_2_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(8, 1, 1, 1, 1, 2, 4, 2, 4, 1, 1, 1, 1, 7):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 28 * 28 + i2_1 * 14 + i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 4 * 16 + i3_1 * 4 + i3_2)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 1, 14):
                    for ax3_ax4_fused in T.vectorized(16):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 112 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4)
                            ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 28 * 28 + i2_1 * 14 + ax2)
                            ax3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 4 * 16 + i3_1 * 4 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 4, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l94)
l95 = sch.fuse(l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b67)
l105 = sch.fuse(l103, l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b106)
b126 = sch.decompose_reduction(block=b106, loop=l111)
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #6: GFLOPs: 1.1890. Time: 22.2812 ms. Best GFLOPs: 2.1629
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #7: GFLOPs: 2.1365. Time: 12.4002 ms. Best GFLOPs: 2.1629
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #8: GFLOPs: 2.5049. Time: 10.5763 ms. Best GFLOPs: 2.5049
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #9: GFLOPs: 0.8833. Time: 29.9944 ms. Best GFLOPs: 2.5049
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #10: GFLOPs: 1.7172. Time: 15.4282 ms. Best GFLOPs: 2.5049
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #11: GFLOPs: 1.6021. Time: 16.5364 ms. Best GFLOPs: 2.5049
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #12: GFLOPs: 2.2843. Time: 11.5978 ms. Best GFLOPs: 2.5049
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #13: GFLOPs: 3.0016. Time: 8.8263 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 14, 1, 1):
                for i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(14, 4, 4, 8):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_1 * 4 + i1_3_init)
                        oh = T.axis.spatial(112, i2_1 * 8 + i2_3_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 1, 14, 4, 4, 1, 1, 1, 4, 8, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(112, i2_1 * 8 + i2_3)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 8, 112):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 8])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #15: GFLOPs: 1.0360. Time: 25.5729 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #16: GFLOPs: 2.6988. Time: 9.8165 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #17: GFLOPs: 1.2044. Time: 21.9971 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #18: GFLOPs: 1.7756. Time: 14.9203 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #19: GFLOPs: 0.9817. Time: 26.9856 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #20: GFLOPs: 1.3248. Time: 19.9970 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(7, 2, 4, 2, 2, 28):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 2 + i1_3_init)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 14 + i2_2_init * 2 + i2_3_init)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 56 + i3_2_init * 28 + i3_3_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 7, 2, 4, 4, 1, 1, 1, 2, 2, 28, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 2 + i1_3)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 14 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 56 + i3_2 * 28 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 56):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 2 + ax1)
                        ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 14 + ax2)
                        ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 56 + ax3)
                        ax4 = T.axis.spatial(4, ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #22: GFLOPs: 1.8615. Time: 14.2321 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #23: GFLOPs: 1.1734. Time: 22.5774 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #24: GFLOPs: 2.7174. Time: 9.7493 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #25: GFLOPs: 2.3552. Time: 11.2489 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #26: GFLOPs: 1.2741. Time: 20.7931 ms. Best GFLOPs: 3.0016
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #27: GFLOPs: 3.5384. Time: 7.4872 ms. Best GFLOPs: 3.5384
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #28: GFLOPs: 2.0821. Time: 12.7243 ms. Best GFLOPs: 3.5384
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #29: GFLOPs: 2.0411. Time: 12.9798 ms. Best GFLOPs: 3.5384
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #30: GFLOPs: 2.6988. Time: 9.8164 ms. Best GFLOPs: 3.5384
[03:15:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #31: GFLOPs: 1.6860. Time: 15.7134 ms. Best GFLOPs: 3.5384
[03:15:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 68
Total latency (us): 12767.3

[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #0: GFLOPs: 1.0536. Time: 7.6195 ms. Best GFLOPs: 1.0536
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1 in T.grid(1, 1, 1, 8):
                for ax0, ax1, ax2 in T.grid(1, 8, 3):
                    for ax3_ax4_fused in T.vectorized(36):
                        with T.block("PaddedInput"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 224 // 16 * 8 + i2_1 + ax2)
                            i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 16 * 7 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 2, 1, 1, 1, 2, 1, 7, 1):
                    for i1_3_init in T.serial(4):
                        for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(8, i1_2 * 4 + i1_3_init)
                                oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 16 * 8 + i2_1)
                                ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 * 7 + i3_2)
                                oci = T.axis.spatial(4, i4_1 * 2 + i2_3_i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3 in T.grid(3, 3, 1, 4):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(8, i1_2 * 4 + i1_3)
                                oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 16 * 8 + i2_1)
                                ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 * 7 + i3_2)
                                oci = T.axis.spatial(4, i4_1 * 2 + i2_3_i3_3_i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 8, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[16, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b62)
l78 = sch.fuse(l65, l66, l67, l68)
sch.parallel(loop=l78)
l79 = sch.fuse(l76, l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b63)
l101 = sch.fuse(l98, l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b64)
l107 = sch.fuse(l102, l103, l104)
sch.parallel(loop=l107)
l108 = sch.fuse(l106)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b109)
b129 = sch.decompose_reduction(block=b109, loop=l124)
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 114, 114):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 4 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 4, 1):
                for i5_0 in T.serial(1):
                    for i2_2_init, i3_2_init, i4_2_init, i3_3_init in T.grid(112, 2, 2, 14):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 4 + i1_1)
                            oh = T.axis.spatial(112, i2_2_init)
                            ow = T.axis.spatial(112, i3_1 * 28 + i3_2_init * 14 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 112, 2, 2, 3, 1, 1, 1, 1, 14, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 4 + i1_1)
                            oh = T.axis.spatial(112, i2_2)
                            ow = T.axis.spatial(112, i3_1 * 28 + i3_2 * 14 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 112, 28):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 4 + i1_1)
                            ax2_1 = T.axis.spatial(112, ax2)
                            ax3_1 = T.axis.spatial(112, i3_1 * 28 + ax3)
                            ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 112, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 2, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b63)
l76 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l76)
l77 = sch.fuse(l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b65)
l109 = sch.fuse(l108)
sch.vectorize(loop=l109)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b110 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b110)
b131 = sch.decompose_reduction(block=b110, loop=l118)
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #3: GFLOPs: 1.3389. Time: 5.9960 ms. Best GFLOPs: 1.3389
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 114, 114):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(8, i0_0_i1_0_fused * 4 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 14, 1, 1, 1, 2, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i2_3_init in T.grid(4, 14, 8, 4, 4):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i0_0_i1_0_fused * 4 + i1_2_init)
                        oh = T.axis.spatial(112, i2_1 * 56 + i2_2_init * 4 + i2_3_init)
                        ow = T.axis.spatial(112, i3_0 * 8 + i3_2_init)
                        oci = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 4, 14, 8, 4, 1, 3, 1, 1, 4, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i0_0_i1_0_fused * 4 + i1_2)
                        oh = T.axis.spatial(112, i2_1 * 56 + i2_2 * 4 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 8 + i3_2)
                        oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_0, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 4, 56):
                    for ax3_ax4_fused in T.vectorized(32):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(8, i0_0_i1_0_fused * 4 + ax1)
                            ax2_1 = T.axis.spatial(112, i2_1 * 56 + ax2)
                            ax3 = T.axis.spatial(112, i3_0 * 8 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 14, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 1, 8, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b63)
l73 = sch.fuse(l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b65)
l112 = sch.fuse(l110, l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b113)
b137 = sch.decompose_reduction(block=b113, loop=l123)
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 14, 1, 1):
                for i2_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 56, 4, 4, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_1 * 4 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 56 + i2_1 * 4 + i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 56 + i3_2_init)
                        oci = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 2, 56, 4, 1, 1, 1, 4, 2, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 56 + i2_1 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 56 + i3_2)
                        oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 113 and 1 <= ow + kw and ow + kw < 113, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 56, 56):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, ax1)
                        ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 56 + ax2)
                        ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 56 + ax3)
                        ax4 = T.axis.spatial(4, ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 14, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 56, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
l96 = sch.fuse(l95)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b97)
b118 = sch.decompose_reduction(block=b97, loop=l104)
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1 in T.grid(1, 8):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 58, 30, 1):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_1 + ax1)
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 16 * 56 + ax2)
                        i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 28 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(7, 1, 1, 1, 1, 1, 1, 8, 4, 1):
                    for i3_3_init in T.serial(7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i1_1)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 16 * 56 + i2_1 * 8 + i2_2)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 28 + i3_2 * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 7, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i1_1)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 16 * 56 + i2_1 * 8 + i2_2)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 28 + i3_2 * 7 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 56, 28, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(8, ax1)
                    ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 16 * 56 + ax2)
                    ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 28 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                    T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 8, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 4, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b63)
l78 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l78)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b105)
b126 = sch.decompose_reduction(block=b105, loop=l119)
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #7: GFLOPs: 0.5355. Time: 14.9928 ms. Best GFLOPs: 1.3389
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #8: GFLOPs: 0.7890. Time: 10.1745 ms. Best GFLOPs: 1.3389
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #9: GFLOPs: 0.6362. Time: 12.6183 ms. Best GFLOPs: 1.3389
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #10: GFLOPs: 0.7148. Time: 11.2312 ms. Best GFLOPs: 1.3389
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 114, 114):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2, i3, i4 = T.axis.remap("SSSS", [i0_0_i1_0_fused, ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0 in T.grid(2, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                    for i2_2_init, i3_2_init in T.grid(56, 56):
                        for i4_2_i5_1_i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(8, i0_0_i1_0_fused)
                                oh = T.axis.spatial(112, i2_0 * 56 + i2_2_init)
                                ow = T.axis.spatial(112, i3_0 * 56 + i3_2_init)
                                oci = T.axis.spatial(4, i4_0 * 2 + i4_2_i5_1_i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2 in T.grid(3, 3, 1, 1, 56, 56):
                        for i4_2_i5_1_i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(8, i0_0_i1_0_fused)
                                oh = T.axis.spatial(112, i2_0 * 56 + i2_2)
                                ow = T.axis.spatial(112, i3_0 * 56 + i3_2)
                                oci = T.axis.spatial(4, i4_0 * 2 + i4_2_i5_1_i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 56, 56):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(8, i0_0_i1_0_fused)
                            ax2_1 = T.axis.spatial(112, i2_0 * 56 + ax2)
                            ax3_1 = T.axis.spatial(112, i3_0 * 56 + ax3)
                            ax4 = T.axis.spatial(4, i4_0 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 56, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 56, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b63)
l73 = sch.fuse(l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l90, l91, l92, l93, l94, l95, l96, l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b65)
l108 = sch.fuse(l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b109)
b126 = sch.decompose_reduction(block=b109, loop=l119)
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #12: GFLOPs: 1.6067. Time: 4.9967 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 30, 58):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 4 + ax1)
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 28 + ax2)
                        i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 56 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(4, 2, 8, 2, 14, 7):
                with T.block("DepthwiseConv2d_init"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 4 + i1_2_init)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 28 + i2_2_init * 14 + i2_3_init)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 56 + i3_2_init * 7 + i3_3_init)
                    oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 4, 2, 8, 2, 1, 3, 1, 1, 14, 7, 1):
                with T.block("DepthwiseConv2d_update"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 4 + i1_2)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 28 + i2_2 * 14 + i2_3)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 56 + i3_2 * 7 + i3_3)
                    oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                    T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 8, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b62)
l80 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l80)
l81 = sch.fuse(l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b104)
b120 = sch.decompose_reduction(block=b104, loop=l106)
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 58, 114):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_fused % 2 * 56 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0 in T.grid(1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 1, 2):
                    for i1_2_init, i2_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(2, 7, 16, 4, 7):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(8, i1_1 * 2 + i1_2_init)
                                oh = T.axis.spatial(112, i0_0_i1_0_i2_0_fused * 56 + i2_1 * 28 + i2_2_init * 4 + i2_3_init)
                                ow = T.axis.spatial(112, i3_2_init * 7 + i3_3_init)
                                oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 2, 7, 16, 1, 1, 1, 1, 1, 4, 7):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(8, i1_1 * 2 + i1_2)
                                oh = T.axis.spatial(112, i0_0_i1_0_i2_0_fused * 56 + i2_1 * 28 + i2_2 * 4 + i2_3)
                                ow = T.axis.spatial(112, i3_2 * 7 + i3_3)
                                oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 8, 56, 112):
                    for ax4_fused in T.vectorized(4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(8, ax1)
                            ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_fused * 56 + ax2)
                            ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 16, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
l107 = sch.fuse(l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b108 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b108)
b131 = sch.decompose_reduction(block=b108, loop=l117)
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #15: GFLOPs: 0.6297. Time: 12.7493 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #16: GFLOPs: 0.6485. Time: 12.3801 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #17: GFLOPs: 1.0646. Time: 7.5413 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #18: GFLOPs: 1.0174. Time: 7.8906 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #19: GFLOPs: 1.1056. Time: 7.2613 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #20: GFLOPs: 0.5396. Time: 14.8768 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #21: GFLOPs: 0.6262. Time: 12.8213 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 6, 58):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 112 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 7 * 2 + ax1)
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 28 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 4 + ax2)
                        i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 56 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(2, 1, 1, 1, 1, 1, 4, 7, 1):
                for i1_3_init, i3_3_init in T.grid(2, 4):
                    for i4_3_fused_init in T.vectorized(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 7 * 2 + i1_3_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 28 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 4 + i2_2)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 56 + i3_1 * 28 + i3_2 * 4 + i3_3_init)
                            oci = T.axis.spatial(4, i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 2, 1, 4):
                    for i4_3_fused in T.vectorized(4):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 7 * 2 + i1_3)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 28 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 4 + i2_2)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 56 + i3_1 * 28 + i3_2 * 4 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_3_fused, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b62)
l78 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
l97 = sch.fuse(l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
l103 = sch.fuse(l98, l99, l100)
sch.parallel(loop=l103)
l104 = sch.fuse(l102)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b105)
b123 = sch.decompose_reduction(block=b105, loop=l116)
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #23: GFLOPs: 1.0036. Time: 7.9991 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 8, 30):
                for ax3_ax4_fused in T.vectorized(64):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 32 // 8 * 28 + ax2)
                        i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 28, 1, 4):
                for i1_3_init, i3_3_init in T.grid(4, 14):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_2 * 4 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + i2_2)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i3_3_init)
                        oci = T.axis.spatial(4, i4_2)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 4, 1, 14, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + i2_2)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 28, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[8, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b62)
l74 = sch.fuse(l65, l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l72, l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b104)
b126 = sch.decompose_reduction(block=b104, loop=l119)
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #25: GFLOPs: 1.0034. Time: 8.0006 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #26: GFLOPs: 0.8688. Time: 9.2401 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #27: GFLOPs: 0.5180. Time: 15.4997 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #28: GFLOPs: 0.7936. Time: 10.1167 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #29: GFLOPs: 0.4385. Time: 18.3066 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #30: GFLOPs: 0.2030. Time: 39.5513 ms. Best GFLOPs: 1.6067
[03:15:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"] Trial #31: GFLOPs: 0.5020. Time: 15.9935 ms. Best GFLOPs: 1.6067
[03:15:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 100
Total latency (us): 17764

[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(4, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 4, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i4_2_init, i3_3_init in T.grid(28, 4, 4, 4):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 14 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 28 + i2_2_init)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 16 + i3_2_init * 4 + i3_3_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 28, 4, 4, 4, 1, 1, 1, 1, 1, 4, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 14 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 28 + i2_2)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 16 + i3_2 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 1, 28):
                for ax3_ax4_fused in T.vectorized(64):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28)
                        ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 14 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 28 + ax2)
                        ax3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 16 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 28, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 4, 4])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l98, l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #1: GFLOPs: 1.1779. Time: 11.0750 ms. Best GFLOPs: 1.1779
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(4, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 4, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i2_3_init in T.grid(2, 4, 7, 2, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 128 // 32 * 28 + i2_2_init * 7 + i2_3_init)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 2 * 7 + i3_2_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 128 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 2, 4, 7, 1, 1, 1, 1, 1, 2, 7, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(4, i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 128 // 32 * 28 + i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 2 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 128 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    ic = T.axis.reduce(32, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 7, 1):
                with T.block("T_add"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(4, ax1)
                    ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 128 // 32 * 28 + ax2)
                    ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 2 * 7 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 128 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 4, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #3: GFLOPs: 0.2966. Time: 43.9869 ms. Best GFLOPs: 1.1779
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #4: GFLOPs: 2.1391. Time: 6.0987 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #5: GFLOPs: 0.9990. Time: 13.0593 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #6: GFLOPs: 1.4594. Time: 8.9393 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #7: GFLOPs: 1.1416. Time: 11.4273 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #8: GFLOPs: 0.8509. Time: 15.3311 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #9: GFLOPs: 0.9729. Time: 13.4095 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #10: GFLOPs: 1.8226. Time: 7.1576 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #11: GFLOPs: 1.2842. Time: 10.1587 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #12: GFLOPs: 1.5091. Time: 8.6450 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #13: GFLOPs: 1.9348. Time: 6.7427 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #14: GFLOPs: 0.6424. Time: 20.3065 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #15: GFLOPs: 0.6740. Time: 19.3544 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #16: GFLOPs: 1.3979. Time: 9.3323 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #17: GFLOPs: 0.7190. Time: 18.1449 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(4, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 4, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 4, 1):
                for i2_2_init, i3_2_init, i1_3_init, i2_3_init in T.grid(7, 2, 2, 2):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 2 + i1_3_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 28 * 56 + i2_1 * 14 + i2_2_init * 2 + i2_3_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 8 + i3_1 * 2 + i3_2_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(2, 1, 1, 1, 1, 7, 2, 1, 16, 1, 1, 1, 2, 2):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 2 + i1_3)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 28 * 56 + i2_1 * 14 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 8 + i3_1 * 2 + i3_2)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 56, 8):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 2 + ax1)
                        ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 28 * 56 + ax2)
                        ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 8 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 7, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 4, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[2, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
l94 = sch.fuse(l91, l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b102)
b124 = sch.decompose_reduction(block=b102, loop=l109)
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #19: GFLOPs: 1.1217. Time: 11.6301 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #20: GFLOPs: 0.8543. Time: 15.2706 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #21: GFLOPs: 1.0659. Time: 12.2388 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #22: GFLOPs: 0.8635. Time: 15.1081 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #23: GFLOPs: 0.8698. Time: 14.9988 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #24: GFLOPs: 1.2817. Time: 10.1784 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #25: GFLOPs: 0.3057. Time: 42.6814 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(4, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 4, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 4, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 4, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(128, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i1_3_init, i3_3_init in T.grid(14, 4, 14):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 14 + i2_2_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 * 14 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(4, 1, 1, 1, 1, 14, 1, 1, 8, 1, 1, 1, 4, 1, 14):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(4, i1_3)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 14 + i2_2)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i4_3_fused)
                        ic = T.axis.reduce(32, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [4, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 14):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, ax1)
                        ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 14 + ax2)
                        ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 * 14 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 14, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 1, 14])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #27: GFLOPs: 0.7866. Time: 16.5857 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #28: GFLOPs: 0.3495. Time: 37.3260 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #29: GFLOPs: 0.5300. Time: 24.6145 ms. Best GFLOPs: 2.1391
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #30: GFLOPs: 2.2824. Time: 5.7158 ms. Best GFLOPs: 2.2824
[03:15:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #31: GFLOPs: 1.0554. Time: 12.3609 ms. Best GFLOPs: 2.2824
[03:15:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_contrib_conv2d_NCHWc_add"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 132
Total latency (us): 23479.7

[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #0: GFLOPs: 2.2132. Time: 18.4995 ms. Best GFLOPs: 2.2132
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 2):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init in T.grid(3, 7, 8, 2, 4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16 // 8 * 12 + i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 * 7 + i2_2_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 16 * 8 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 3, 7, 8, 2, 4, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16 // 8 * 12 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 * 7 + i2_2)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 16 * 8 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(16, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 12, 7, 8):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16 // 8 * 12 + ax1)
                            ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 * 7 + ax2)
                            ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 16 * 8 + ax3)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 3, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 8, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 8, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b67)
l103 = sch.fuse(l102)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b104)
b124 = sch.decompose_reduction(block=b104, loop=l108)
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #2: GFLOPs: 2.3395. Time: 17.5012 ms. Best GFLOPs: 2.3395
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #3: GFLOPs: 1.0775. Time: 37.9986 ms. Best GFLOPs: 2.3395
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 12, 14, 16):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 12 + i1_3_init)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 112 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + i2_2_init * 14 + i2_3_init)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 112 // 16 * 16 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 12, 14, 16, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 12 + i1_3)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 112 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + i2_2 * 14 + i2_3)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 112 // 16 * 16 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    ic = T.axis.reduce(16, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 28, 16, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 12 + ax1)
                    ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 112 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + ax2)
                    ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 112 // 16 * 16 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 12])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 16])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #5: GFLOPs: 2.0276. Time: 20.1932 ms. Best GFLOPs: 2.3395
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #6: GFLOPs: 2.9854. Time: 13.7148 ms. Best GFLOPs: 2.9854
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #7: GFLOPs: 3.0500. Time: 13.4242 ms. Best GFLOPs: 3.0500
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #8: GFLOPs: 4.2467. Time: 9.6412 ms. Best GFLOPs: 4.2467
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #9: GFLOPs: 1.7117. Time: 23.9199 ms. Best GFLOPs: 4.2467
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 14, 1):
                for i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(16, 2, 6, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 12 + i1_1 * 6 + i1_3_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 4 * 16 + i2_2_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 28 + i3_1 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(2, 1, 1, 1, 1, 16, 1, 2, 8, 1, 1, 1, 6, 1, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 12 + i1_1 * 6 + i1_3)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 4 * 16 + i2_2)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 28 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(16, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 16, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 12 + ax1)
                        ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 4 * 16 + ax2)
                        ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 28 + ax3)
                        ax4 = T.axis.spatial(4, ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 16, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 14, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=56)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #11: GFLOPs: 1.8107. Time: 22.6115 ms. Best GFLOPs: 4.2467
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #12: GFLOPs: 2.2432. Time: 18.2522 ms. Best GFLOPs: 4.2467
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #13: GFLOPs: 5.1188. Time: 7.9986 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #14: GFLOPs: 2.5594. Time: 15.9977 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #15: GFLOPs: 3.3354. Time: 12.2754 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #16: GFLOPs: 2.8822. Time: 14.2057 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #17: GFLOPs: 2.9243. Time: 14.0014 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #18: GFLOPs: 1.7721. Time: 23.1048 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #19: GFLOPs: 3.9381. Time: 10.3968 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #20: GFLOPs: 2.1553. Time: 18.9969 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #21: GFLOPs: 4.5892. Time: 8.9217 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #22: GFLOPs: 2.2730. Time: 18.0127 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #23: GFLOPs: 4.9634. Time: 8.2491 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #24: GFLOPs: 3.7306. Time: 10.9751 ms. Best GFLOPs: 5.1188
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 4, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(294, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(2, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(2, 8, 4, 2, 4, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 3 * 8 + i1_2_init * 4 + i1_3_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 42 * 16 + i2_1 * 8 + i2_2_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 42 // 3 * 8 + i3_2_init * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(16, 1, 1, 1, 2, 8, 4, 2, 1, 1, 1, 1, 4, 1, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 3 * 8 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 42 * 16 + i2_1 * 8 + i2_2)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 42 // 3 * 8 + i3_2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(16, i5_0)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 4, 112, 112, 4], "float32"], ["TENSOR", [24, 4, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(2688, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(24, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 3, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 8, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 4, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b102)
b123 = sch.decompose_reduction(block=b102, loop=l107)
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #26: GFLOPs: 7.0611. Time: 5.7985 ms. Best GFLOPs: 7.0611
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #27: GFLOPs: 3.5842. Time: 11.4233 ms. Best GFLOPs: 7.0611
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #28: GFLOPs: 1.9418. Time: 21.0858 ms. Best GFLOPs: 7.0611
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #29: GFLOPs: 4.5477. Time: 9.0031 ms. Best GFLOPs: 7.0611
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #30: GFLOPs: 2.7220. Time: 15.0417 ms. Best GFLOPs: 7.0611
[03:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"] Trial #31: GFLOPs: 3.4122. Time: 11.9991 ms. Best GFLOPs: 7.0611
[03:16:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 164
Total latency (us): 29278.2

[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #0: GFLOPs: 0.5018. Time: 11.9996 ms. Best GFLOPs: 0.5018
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #1: GFLOPs: 0.6206. Time: 9.7022 ms. Best GFLOPs: 0.6206
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #2: GFLOPs: 0.6420. Time: 9.3790 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #3: GFLOPs: 0.4301. Time: 13.9984 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #4: GFLOPs: 0.4708. Time: 12.7882 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 6, 113, 113):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(24, i0_0_i1_0_fused * 6 + ax1)
                        i2 = T.axis.spatial(114, ax2)
                        i3 = T.axis.spatial(114, ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(4, 1, 4, 1, 1, 1, 14, 1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 3, 14, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(24, i0_0_i1_0_fused * 6 + i1_2_init * 3 + i1_3_init)
                            oh = T.axis.spatial(56, i2_0 * 14 + i2_3_init)
                            ow = T.axis.spatial(56, i3_1 * 4 + i3_3_init)
                            oci = T.axis.spatial(4, i4_0)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 1, 1, 1, 3, 1, 1, 3, 14, 4, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(24, i0_0_i1_0_fused * 6 + i1_2 * 3 + i1_3)
                            oh = T.axis.spatial(56, i2_0 * 14 + i2_3)
                            ow = T.axis.spatial(56, i3_1 * 4 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_0, i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 14, 4, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(24, i0_0_i1_0_fused * 6 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 14 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 14, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b63)
l73 = sch.fuse(l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b112)
b136 = sch.decompose_reduction(block=b112, loop=l123)
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #6: GFLOPs: 0.2688. Time: 22.3967 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #7: GFLOPs: 0.4675. Time: 12.8799 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #8: GFLOPs: 0.4254. Time: 14.1527 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #9: GFLOPs: 0.1370. Time: 43.9399 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #10: GFLOPs: 0.4215. Time: 14.2839 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #11: GFLOPs: 0.1423. Time: 42.3187 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(2736, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(114):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(24, i0_i1_i2_fused // 114)
                        i2 = T.axis.spatial(114, i0_i1_i2_fused % 114)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1_1, i4_1 in T.grid(1, 1, 1, 56, 1):
                for i1_2_init, i1_3_init, i2_3_init in T.grid(6, 2, 4):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 12 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + i2_3_init)
                            ow = T.axis.spatial(56, i3_1_1)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 1, 1, 6, 1, 1, 1, 1, 3, 1, 2, 4):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 12 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + i2_3)
                            ow = T.axis.spatial(56, i3_1_1)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i3_3_i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 4, 56):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 12 + ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + ax2)
                        ax3_1 = T.axis.spatial(56, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 6, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 56, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77)
sch.parallel(loop=l97)
l98 = sch.fuse(l95, l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b106)
b126 = sch.decompose_reduction(block=b106, loop=l113)
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #13: GFLOPs: 0.5579. Time: 10.7918 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #14: GFLOPs: 0.5019. Time: 11.9978 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1 in T.grid(1, 24, 2, 7):
                for ax0, ax1, ax2 in T.grid(1, 1, 57):
                    for ax3_ax4_fused in T.vectorized(36):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(24, i1_1 + ax1)
                            i2 = T.axis.spatial(114, i2_1 * 56 + ax2)
                            i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 56 + i3_1 * 8 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(2, 1, 1, 1, 1, 2, 2, 1):
                    for i2_3_init, i3_3_init in T.grid(14, 2):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(24, i1_1)
                                oh = T.axis.spatial(56, i2_1 * 28 + i2_2 * 14 + i2_3_init)
                                ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 28 + i3_1 * 4 + i3_2 * 2 + i3_3_init)
                                oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 1, 14, 2):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(24, i1_1)
                                oh = T.axis.spatial(56, i2_1 * 28 + i2_2 * 14 + i2_3)
                                ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 28 + i3_1 * 4 + i3_2 * 2 + i3_3)
                                oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 24, 56, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 28 + ax3)
                        ax4 = T.axis.spatial(4, ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 24, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b63)
l80 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l80)
l81 = sch.fuse(l78, l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b65)
l109 = sch.fuse(l108)
sch.vectorize(loop=l109)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b110 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b110)
b131 = sch.decompose_reduction(block=b110, loop=l124)
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(192, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 57, 29, 1):
                with T.block("PaddedInput"):
                    i0 = T.axis.spatial(1, ax0)
                    i1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 192 // 32 * 4 + ax1)
                    i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 16 * 56 + ax2)
                    i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 28 + ax3)
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 + ax4)
                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 14, 2, 1, 1, 1, 1, 1, 2, 1, 1):
                for i1_3_init, i3_3_init in T.grid(4, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 4 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 16 * 28 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 14 + i3_1 * 7 + i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 4, 1, 7, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 4 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 16 * 28 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 14 + i3_1 * 7 + i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 14, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 4 + ax1)
                    ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 16 * 28 + ax2)
                    ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 4 * 14 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                    T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 1, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b63)
l76 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l76)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b103)
b124 = sch.decompose_reduction(block=b103, loop=l117)
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 4, 1, 1, 2, 1):
                for i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(7, 2, 6, 56, 4):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(24, i1_1 * 6 + i1_3_init)
                        oh = T.axis.spatial(56, i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 28 + i3_2_init * 4 + i3_3_init)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 7, 2, 3, 1, 1, 6, 56, 4, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(24, i1_1 * 6 + i1_3)
                        oh = T.axis.spatial(56, i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 28 + i3_2 * 4 + i3_3)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 113 and 1 <= ow * 2 + kw and ow * 2 + kw < 113, placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 24, 56, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 28 + ax3)
                        ax4 = T.axis.spatial(4, ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 56])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
l96 = sch.fuse(l95)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b97)
b118 = sch.decompose_reduction(block=b97, loop=l105)
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #18: GFLOPs: 0.4140. Time: 14.5435 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(2736, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(114):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(24, i0_i1_i2_fused // 114)
                        i2 = T.axis.spatial(114, i0_i1_i2_fused % 114)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1_1, i4_1 in T.grid(1, 3, 1, 56, 1):
                for i1_2_init, i1_3_init, i2_3_init in T.grid(2, 2, 4):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 12 + i1_1 * 4 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + i2_3_init)
                            ow = T.axis.spatial(56, i3_1_1)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 1, 1, 2, 1, 1, 1, 1, 3, 1, 2, 4):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 12 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + i2_3)
                            ow = T.axis.spatial(56, i3_1_1)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i3_3_i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 4, 56):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 12 + ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + ax2)
                        ax3_1 = T.axis.spatial(56, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 3, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 56, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77)
sch.parallel(loop=l97)
l98 = sch.fuse(l95, l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b106)
b126 = sch.decompose_reduction(block=b106, loop=l113)
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused in T.parallel(64):
            for ax0, ax1, ax2, ax3 in T.grid(1, 3, 57, 57):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 64 // 32 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 16 // 4 * 3 + ax1)
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 4 // 2 * 56 + ax2)
                        i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 32 // 16 * 56 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i5_0 in T.serial(1):
                for i1_2_init, i2_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(3, 14, 2, 2, 28):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused // 32 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 16 // 4 * 3 + i1_2_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 4 // 2 * 28 + i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 32 // 16 * 28 + i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 3, 14, 1, 2, 3, 1, 1, 1, 2, 28, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused // 32 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 16 // 4 * 3 + i1_2)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 4 // 2 * 28 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 32 // 16 * 28 + i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 2 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 3, 28, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused // 32 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 16 // 4 * 3 + ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 4 // 2 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 32 // 16 * 28 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused_fused_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 3, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 14, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b63)
l81 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l81)
l82 = sch.fuse(l80)
sch.vectorize(loop=l82)
l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l83)
sch.parallel(loop=l98)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l99)
sch.parallel(loop=l105)
l106 = sch.fuse(l104)
sch.vectorize(loop=l106)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b107)
b123 = sch.decompose_reduction(block=b107, loop=l110)
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 6, 15, 113):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 32 // 8 * 6 + ax1)
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 2 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 14 + ax2)
                        i3 = T.axis.spatial(114, ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_1, i4_1 in T.grid(7, 1):
                for i1_2_init, i3_2_init, i2_3_init in T.grid(6, 8, 7):
                    for i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 8 * 6 + i1_2_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 2 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 7 + i2_3_init)
                            ow = T.axis.spatial(56, i3_1 * 8 + i3_2_init)
                            oci = T.axis.spatial(4, i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 1, 1, 6, 1, 8, 1, 1, 3, 1, 1, 7):
                    for i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 8 * 6 + i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 2 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 7 + i2_3)
                            ow = T.axis.spatial(56, i3_1 * 8 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i3_3_i4_3_fused, i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1344, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(24, i0_i1_i2_fused // 56)
                        ax2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 6, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 8, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b62)
l78 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
l97 = sch.fuse(l95, l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
l103 = sch.fuse(l98, l99, l100)
sch.parallel(loop=l103)
l104 = sch.fuse(l102)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b105)
b122 = sch.decompose_reduction(block=b105, loop=l109)
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #22: GFLOPs: 0.6212. Time: 9.6930 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(24, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 24, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(2736, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(114):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(24, i0_i1_i2_fused // 114)
                        i2 = T.axis.spatial(114, i0_i1_i2_fused % 114)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0 in T.serial(1):
                for i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 3, 7, 28):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 3 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 2 * 7 + i2_3_init)
                        ow = T.axis.spatial(56, i3_2_init * 28 + i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 2, 1, 3, 1, 1, 3, 7, 28, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 3 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 2 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_2 * 28 + i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 24, 112, 112, 4], "float32"], ["TENSOR", [24, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1344, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(24, i0_i1_i2_fused // 56)
                        ax2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 28])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69 = sch.get_loops(block=b62)
l70 = sch.fuse(l65, l66, l67)
sch.parallel(loop=l70)
l71 = sch.fuse(l69)
sch.vectorize(loop=l71)
sch.annotate(block_or_loop=l70, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l70, ann_key="pragma_unroll_explicit", ann_val=1)
l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b63)
l96 = sch.fuse(l72, l73, l74, l75, l76, l77, l78, l79, l80, l81)
sch.parallel(loop=l96)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b104)
b120 = sch.decompose_reduction(block=b104, loop=l107)
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #24: GFLOPs: 0.3662. Time: 16.4420 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #25: GFLOPs: 0.5711. Time: 10.5438 ms. Best GFLOPs: 0.6420
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #26: GFLOPs: 0.7550. Time: 7.9750 ms. Best GFLOPs: 0.7550
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #27: GFLOPs: 0.4672. Time: 12.8873 ms. Best GFLOPs: 0.7550
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #28: GFLOPs: 0.3943. Time: 15.2714 ms. Best GFLOPs: 0.7550
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #29: GFLOPs: 0.3708. Time: 16.2389 ms. Best GFLOPs: 0.7550
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #30: GFLOPs: 0.5946. Time: 10.1256 ms. Best GFLOPs: 0.7550
[03:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"] Trial #31: GFLOPs: 0.9264. Time: 6.4992 ms. Best GFLOPs: 0.9264
[03:16:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 196
Total latency (us): 35777.4

[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #0: GFLOPs: 1.8852. Time: 7.7052 ms. Best GFLOPs: 1.8852
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #1: GFLOPs: 0.9366. Time: 15.5098 ms. Best GFLOPs: 1.8852
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(7, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 4, 2):
                for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(2, 8, 3, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(6, i1_2_init * 3 + i1_3_init)
                            oh = T.axis.spatial(56, i2_1 * 8 + i2_2_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 8 + i3_1 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [6, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(4, 1, 1, 1, 2, 8, 1, 1, 24, 1, 1, 1, 3, 1, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(6, i1_2 * 3 + i1_3)
                            oh = T.axis.spatial(56, i2_1 * 8 + i2_2)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 8 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                            ic = T.axis.reduce(96, i5_0 * 24 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [6, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 6, 56):
                for ax3_ax4_fused in T.vectorized(32):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 8 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 2, 3])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 7, 8, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 4, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 24])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #3: GFLOPs: 0.6054. Time: 23.9949 ms. Best GFLOPs: 1.8852
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #4: GFLOPs: 0.6690. Time: 21.7114 ms. Best GFLOPs: 1.8852
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 2, 2):
                for i2_2_init, i3_2_init, i1_3_init in T.grid(7, 2, 6):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 28 + i2_1 * 7 + i2_2_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + i3_1 * 2 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [6, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 7, 2, 1, 32, 1, 1, 1, 6, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 28 + i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        ic = T.axis.reduce(96, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [6, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 7, 2, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(6, ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 28 + i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 6])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 2, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[3, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=28)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b66)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b105)
b128 = sch.decompose_reduction(block=b105, loop=l112)
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #6: GFLOPs: 1.1557. Time: 12.5690 ms. Best GFLOPs: 1.8852
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #7: GFLOPs: 0.7894. Time: 18.4013 ms. Best GFLOPs: 1.8852
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #8: GFLOPs: 2.0984. Time: 6.9225 ms. Best GFLOPs: 2.0984
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #9: GFLOPs: 0.8258. Time: 17.5894 ms. Best GFLOPs: 2.0984
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #10: GFLOPs: 0.6286. Time: 23.1090 ms. Best GFLOPs: 2.0984
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #11: GFLOPs: 1.0458. Time: 13.8903 ms. Best GFLOPs: 2.0984
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(196, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(2):
                for i1_2_init, i3_2_init, i4_2_init, i2_3_init in T.grid(3, 4, 2, 8):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 98 * 3 + i1_2_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 8 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 98 // 49 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [6, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 3, 1, 4, 2, 48, 1, 1, 1, 1, 8, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 98 * 3 + i1_2)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 8 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 98 // 49 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(96, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 56, 56, 4], "float32"], ["TENSOR", [6, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 3, 8, 4):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 98 * 3 + ax1)
                            ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 8 + ax2)
                            ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 98 // 49 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + ax3)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 3, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 7, 1, 8])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 4, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[2, 48])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b102)
b121 = sch.decompose_reduction(block=b102, loop=l105)
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #13: GFLOPs: 0.9987. Time: 14.5453 ms. Best GFLOPs: 2.0984
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #14: GFLOPs: 2.1683. Time: 6.6991 ms. Best GFLOPs: 2.1683
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #15: GFLOPs: 1.8765. Time: 7.7409 ms. Best GFLOPs: 2.1683
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #16: GFLOPs: 0.9682. Time: 15.0026 ms. Best GFLOPs: 2.1683
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #17: GFLOPs: 0.4244. Time: 34.2277 ms. Best GFLOPs: 2.1683
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #18: GFLOPs: 1.1761. Time: 12.3511 ms. Best GFLOPs: 2.1683
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #19: GFLOPs: 2.3003. Time: 6.3149 ms. Best GFLOPs: 2.3003
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #20: GFLOPs: 2.7241. Time: 5.3323 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #21: GFLOPs: 2.0958. Time: 6.9310 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #22: GFLOPs: 1.2109. Time: 11.9963 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #23: GFLOPs: 0.5381. Time: 26.9926 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #24: GFLOPs: 1.6283. Time: 8.9209 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #25: GFLOPs: 0.7234. Time: 20.0802 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #26: GFLOPs: 1.7737. Time: 8.1895 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #27: GFLOPs: 0.8812. Time: 16.4850 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #28: GFLOPs: 0.2794. Time: 51.9934 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #29: GFLOPs: 0.2896. Time: 50.1641 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #30: GFLOPs: 0.3016. Time: 48.1616 ms. Best GFLOPs: 2.7241
[03:16:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #31: GFLOPs: 0.3758. Time: 38.6542 ms. Best GFLOPs: 2.7241
[03:16:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 228
Total latency (us): 41109.7

[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #0: GFLOPs: 0.2593. Time: 34.8364 ms. Best GFLOPs: 0.2593
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #1: GFLOPs: 1.4658. Time: 6.1616 ms. Best GFLOPs: 1.4658
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #2: GFLOPs: 0.8566. Time: 10.5433 ms. Best GFLOPs: 1.4658
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #3: GFLOPs: 1.3442. Time: 6.7191 ms. Best GFLOPs: 1.4658
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #4: GFLOPs: 1.5038. Time: 6.0058 ms. Best GFLOPs: 1.5038
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(63, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 4, 58):
                for ax3_ax4_fused in T.vectorized(40):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 63 // 21 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 3 * 4 + ax1)
                        i2 = T.axis.spatial(58, ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 21 // 3 * 8 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_1, i3_1, i4_1 in T.grid(7, 2, 2):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 4, 2, 4, 2):
                    for i2_3_init in T.serial(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 21 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 3 * 4 + i1_2)
                            oh = T.axis.spatial(56, i2_1 * 8 + i2_2 * 4 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 21 // 3 * 8 + i3_1 * 4 + i3_2)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 4, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 21 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 3 * 4 + i1_2)
                            oh = T.axis.spatial(56, i2_1 * 8 + i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 21 // 3 * 8 + i3_1 * 4 + i3_2)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 8, 4):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 21 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 3 * 4 + ax1)
                            ax2_1 = T.axis.spatial(56, i2_1 * 8 + ax2)
                            ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 21 // 3 * 8 + i3_1 * 4 + ax3)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 3, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b63)
l78 = sch.fuse(l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l76, l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
l107 = sch.fuse(l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b108 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b108)
b127 = sch.decompose_reduction(block=b108, loop=l120)
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #6: GFLOPs: 0.8032. Time: 11.2452 ms. Best GFLOPs: 1.5038
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #7: GFLOPs: 0.4356. Time: 20.7350 ms. Best GFLOPs: 1.5038
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(6, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 2, 14, 7, 4, 1, 1, 1, 1, 2, 1, 1):
                for i1_3_init, i2_3_init, i3_3_init in T.grid(3, 2, 8):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 6 + i1_1 * 3 + i1_3_init)
                        oh = T.axis.spatial(56, i2_1 * 4 + i2_2 * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i3_1 * 8 + i3_3_init)
                        oci = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1 in T.grid(3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 3, 2, 8, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 6 * 6 + i1_1 * 3 + ax1)
                            i2 = T.axis.spatial(58, i2_1 * 4 + i2_2 * 2 + i5_1 + ax2)
                            i3 = T.axis.spatial(58, i3_1 * 8 + i6_1 + ax3)
                            i4 = T.axis.spatial(4, i4_1 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 2, 8, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 6 + i1_1 * 3 + i1_3)
                            oh = T.axis.spatial(56, i2_1 * 4 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(56, i3_1 * 8 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 6, 56, 56):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 6 + ax1)
                        ax2_1, ax3_1, ax4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 2, 1, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 8])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=18)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b63)
l90 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l90)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b65)
l117 = sch.fuse(l116)
sch.vectorize(loop=l117)
sch.annotate(block_or_loop=l111, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l111, ann_key="pragma_unroll_explicit", ann_val=1)
b118 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b118)
b139 = sch.decompose_reduction(block=b118, loop=l132)
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #9: GFLOPs: 1.1919. Time: 7.5778 ms. Best GFLOPs: 1.5038
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #10: GFLOPs: 1.2488. Time: 7.2325 ms. Best GFLOPs: 1.5038
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #11: GFLOPs: 0.6022. Time: 14.9974 ms. Best GFLOPs: 1.5038
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #12: GFLOPs: 1.1303. Time: 7.9903 ms. Best GFLOPs: 1.5038
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #13: GFLOPs: 1.5053. Time: 5.9998 ms. Best GFLOPs: 1.5053
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #14: GFLOPs: 1.3413. Time: 6.7334 ms. Best GFLOPs: 1.5053
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #15: GFLOPs: 0.6050. Time: 14.9278 ms. Best GFLOPs: 1.5053
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(3, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 58, 58):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_fused * 12 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 1, 1, 2, 1, 7, 2):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 14, 4, 2, 3, 2, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_fused * 12 + i1_1 * 6 + i1_2_init * 3 + i1_3_init)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i3_1 * 8 + i3_2_init * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 14, 4, 2, 1, 1, 1, 3, 2, 2, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_fused * 12 + i1_1 * 6 + i1_2 * 3 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 8 + i3_2 * 2 + i3_3)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 6, 28, 8):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(36, i0_0_i1_0_fused * 12 + i1_1 * 6 + ax1)
                            ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                            ax3_1 = T.axis.spatial(56, i3_1 * 8 + ax3)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 2, 2, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 14, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 4, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b63)
l73 = sch.fuse(l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b65)
l112 = sch.fuse(l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b113)
b137 = sch.decompose_reduction(block=b113, loop=l123)
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #17: GFLOPs: 0.6846. Time: 13.1924 ms. Best GFLOPs: 1.5053
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3_1 in T.serial(2):
                for ax0, ax1, ax2, ax3 in T.grid(1, 9, 4, 16):
                    for ax4_fused in T.vectorized(2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 448 // 224 * 18 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 9 + ax1)
                            i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 * 2 + ax2)
                            i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 112 * 28 + i3_1 * 14 + ax3)
                            i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 2 + ax4_fused)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1 in T.serial(2):
                    for i1_2_init, i3_2_init, i1_3_init, i2_3_init in T.grid(3, 14, 3, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 18 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 9 + i1_2_init * 3 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 * 2 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 112 * 28 + i3_1 * 14 + i3_2_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 2 + i4_1)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 3, 1, 14, 1, 1, 1, 1, 3, 2, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 18 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 9 + i1_2 * 3 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 * 2 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 112 * 28 + i3_1 * 14 + i3_2)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 9, 2, 14, 1):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 224 * 18 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 9 + ax1)
                            ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 * 2 + ax2)
                            ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 224 // 112 * 28 + i3_1 * 14 + ax3)
                            ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 2 + i4_1)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 3, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b63)
l80 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l80)
l81 = sch.fuse(l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b107)
b125 = sch.decompose_reduction(block=b107, loop=l111)
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(6, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 7, 2):
                for i1_2_init, i2_2_init, i3_2_init in T.grid(6, 28, 8):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 12 + i1_1 * 6 + i1_2_init)
                        oh = T.axis.spatial(56, i2_1 * 28 + i2_2_init)
                        ow = T.axis.spatial(56, i3_1 * 8 + i3_2_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 28, 10, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 6 // 2 * 12 + i1_1 * 6 + ax1)
                            i2 = T.axis.spatial(58, i2_1 * 28 + i5_0 + ax2)
                            i3 = T.axis.spatial(58, i3_1 * 8 + ax3)
                            i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 6, 28, 8, 1, 1, 1, 1, 1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 12 + i1_1 * 6 + i1_2)
                            oh = T.axis.spatial(56, i2_1 * 28 + i2_2)
                            ow = T.axis.spatial(56, i3_1 * 8 + i3_2)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 56, 56):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 12 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 2, 6, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 28, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 8, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b63)
l82 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l82)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b65)
l109 = sch.fuse(l108)
sch.vectorize(loop=l109)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b110 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b110)
b131 = sch.decompose_reduction(block=b110, loop=l117)
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #20: GFLOPs: 0.4324. Time: 20.8854 ms. Best GFLOPs: 1.5053
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #21: GFLOPs: 1.7550. Time: 5.1462 ms. Best GFLOPs: 1.7550
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #22: GFLOPs: 1.7078. Time: 5.2885 ms. Best GFLOPs: 1.7550
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(48, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 6, 30):
                for ax3_ax4_fused in T.vectorized(64):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_fused % 48 // 8 * 6 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 28, 14, 2):
                for i1_2_init in T.serial(6):
                    for i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_fused // 8 * 6 + i1_2_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i2_1)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i3_1)
                            oci = T.axis.spatial(4, i4_1 * 2 + i0_3_i1_3_i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1 in T.grid(3, 1, 1, 6, 1, 1, 1, 1, 3):
                    for i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_fused // 8 * 6 + i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i2_1)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i3_1)
                            oci = T.axis.spatial(4, i4_1 * 2 + i0_3_i1_3_i2_3_i3_3_i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1 in T.grid(1, 6):
                    for ax2_ax3_ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_fused // 8 * 6 + ax1)
                            ax2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i2_1)
                            ax3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i3_1)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax2_ax3_ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 1, 6, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 28, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b63)
l75 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l75)
l76 = sch.fuse(l73, l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l93, l94, l95, l96, l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b65)
l111 = sch.fuse(l108, l109, l110)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b112)
b130 = sch.decompose_reduction(block=b112, loop=l120)
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #24: GFLOPs: 1.6025. Time: 5.6360 ms. Best GFLOPs: 1.7550
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #25: GFLOPs: 0.7632. Time: 11.8333 ms. Best GFLOPs: 1.7550
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #26: GFLOPs: 1.7940. Time: 5.0345 ms. Best GFLOPs: 1.7940
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #27: GFLOPs: 0.6060. Time: 14.9045 ms. Best GFLOPs: 1.7940
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #28: GFLOPs: 0.5264. Time: 17.1582 ms. Best GFLOPs: 1.7940
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #29: GFLOPs: 1.8224. Time: 4.9561 ms. Best GFLOPs: 1.8224
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #30: GFLOPs: 1.2366. Time: 7.3036 ms. Best GFLOPs: 1.8224
[03:16:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 9, 58, 58):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_fused * 9 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(2, 1, 1, 1, 1, 1, 4, 1, 1):
                for i1_2_init, i2_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(9, 4, 4, 7, 14):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_fused * 9 + i1_2_init)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_2_init * 7 + i2_3_init)
                        ow = T.axis.spatial(56, i3_1 * 14 + i3_3_init)
                        oci = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 9, 4, 1, 4, 3, 1, 1, 1, 7, 14, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_fused * 9 + i1_2)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 14 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(2016, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(36, i0_i1_i2_fused // 56)
                        ax2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 9, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71 = sch.get_loops(block=b62)
l72 = sch.fuse(l65, l66)
sch.parallel(loop=l72)
l73 = sch.fuse(l71)
sch.vectorize(loop=l73)
sch.annotate(block_or_loop=l72, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l72, ann_key="pragma_unroll_explicit", ann_val=1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b104)
b128 = sch.decompose_reduction(block=b104, loop=l115)
[03:16:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 260
Total latency (us): 46065.7

[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #0: GFLOPs: 1.0612. Time: 20.5676 ms. Best GFLOPs: 1.0612
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #1: GFLOPs: 0.7277. Time: 29.9929 ms. Best GFLOPs: 1.0612
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #2: GFLOPs: 2.8581. Time: 7.6367 ms. Best GFLOPs: 2.8581
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(96, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                for i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 4, 2, 7, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 2 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 4 * 7 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 14 + i3_2_init * 7 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(24, 1, 1, 1, 1, 1, 2, 4, 6, 1, 1, 1, 2, 7, 7, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 2 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 4 * 7 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 14 + i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(144, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 2, 7):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 32 * 2 + ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 32 // 4 * 7 + ax2)
                        ax3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 14 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #4: GFLOPs: 1.5159. Time: 14.3980 ms. Best GFLOPs: 2.8581
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #5: GFLOPs: 1.9774. Time: 11.0379 ms. Best GFLOPs: 2.8581
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #6: GFLOPs: 3.0974. Time: 7.0466 ms. Best GFLOPs: 3.0974
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #7: GFLOPs: 1.1543. Time: 18.9093 ms. Best GFLOPs: 3.0974
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #8: GFLOPs: 2.4462. Time: 8.9226 ms. Best GFLOPs: 3.0974
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #9: GFLOPs: 1.7848. Time: 12.2294 ms. Best GFLOPs: 3.0974
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #10: GFLOPs: 4.0055. Time: 5.4492 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(392, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(4):
                for i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 3, 4, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 196 * 3 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 196 // 14 * 4 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i3_2_init * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(36, 1, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 3, 4, 2, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 196 * 3 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 196 // 14 * 4 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(144, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(336, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(6, i0_i1_i2_fused // 56)
                        ax2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[36, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b101)
b120 = sch.decompose_reduction(block=b101, loop=l104)
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #12: GFLOPs: 2.4949. Time: 8.7486 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #13: GFLOPs: 0.5953. Time: 36.6629 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #14: GFLOPs: 1.6224. Time: 13.4531 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(128, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_2_init, i1_3_init, i2_3_init in T.grid(2, 14, 3, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(6, i1_2_init * 3 + i1_3_init)
                    oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 64 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 64 // 16 * 7 + i2_3_init)
                    ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 4 * 14 + i3_2_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(18, 1, 1, 1, 2, 1, 14, 1, 8, 1, 1, 1, 3, 7, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(6, i1_2 * 3 + i1_3)
                    oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 64 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 64 // 16 * 7 + i2_3)
                    ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 4 * 14 + i3_2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(144, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 7, 14, 1):
                with T.block("T_add_1"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(6, ax1)
                    ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 64 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 64 // 16 * 7 + ax2)
                    ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 4 * 14 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[18, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(336, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_2_init, i2_3_init, i3_3_init in T.grid(4, 8, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 48 // 8)
                    oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 48 * 8 + i2_3_init)
                    ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 28 + i3_2_init * 7 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(18, 1, 1, 1, 1, 1, 4, 1, 8, 1, 1, 1, 1, 8, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 48 // 8)
                    oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 48 * 8 + i2_3)
                    ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 28 + i3_2 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(144, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(336, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(6, i0_i1_i2_fused // 56)
                        ax2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 6, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 8])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 4, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[18, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #17: GFLOPs: 2.2593. Time: 9.6608 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #18: GFLOPs: 1.5874. Time: 13.7498 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #19: GFLOPs: 0.6966. Time: 31.3312 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #20: GFLOPs: 1.7302. Time: 12.6147 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 4):
                for i3_2_init, i1_3_init, i3_3_init in T.grid(14, 3, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 3 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 28 + i3_2_init * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 1, 14, 1, 18, 1, 1, 1, 3, 1, 2, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 3 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 112 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 112 // 56 * 28 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(144, i5_0 * 18 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(336, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(6, i0_i1_i2_fused // 56)
                        ax2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 28, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 18])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b101)
b121 = sch.decompose_reduction(block=b101, loop=l105)
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #22: GFLOPs: 2.8806. Time: 7.5771 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(168, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i3_3_init in T.grid(2, 4, 2, 14):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 84 // 28 * 2 + i1_2_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 84 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4 * 4 + i2_2_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + i3_2_init * 14 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(48, 1, 1, 1, 2, 4, 2, 1, 3, 1, 1, 1, 1, 1, 14):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 84 // 28 * 2 + i1_2)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 84 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4 * 4 + i2_2)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + i3_2 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(144, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 84 // 28 * 2 + ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 84 * 28 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4 * 4 + ax2)
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 28 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 3, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[48, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b103)
b121 = sch.decompose_reduction(block=b103, loop=l105)
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #24: GFLOPs: 1.4204. Time: 15.3669 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #25: GFLOPs: 0.8688. Time: 25.1224 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(672, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1):
                for i2_2_init, i4_2_init, i2_3_init in T.grid(2, 4, 14):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_fused // 112)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 56 * 28 + i2_2_init * 14 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(24, 1, 1, 1, 1, 2, 1, 4, 6, 1, 1, 1, 1, 14, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_fused // 112)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 56 * 28 + i2_2 * 14 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(144, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 1, 28):
                    for ax3_ax4_fused in T.vectorized(4):
                        with T.block("T_add_1"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(6, i0_0_i1_0_i2_0_i3_0_fused // 112)
                            ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 112 // 56 * 28 + ax2)
                            ax3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[6, 1, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[56, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b67)
l107 = sch.fuse(l105, l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b108 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b108)
b132 = sch.decompose_reduction(block=b108, loop=l116)
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #27: GFLOPs: 1.5788. Time: 13.8248 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(6, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 28, 2, 1):
                for i3_2_init, i1_3_init, i3_3_init in T.grid(14, 6, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(6, i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 28 + i2_1)
                            ow = T.axis.spatial(56, i3_1 * 28 + i3_2_init * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(48, 1, 1, 1, 1, 1, 14, 1, 3, 1, 1, 1, 6, 1, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(6, i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 28 + i2_1)
                            ow = T.axis.spatial(56, i3_1 * 28 + i3_2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(144, i5_0 * 3 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [6, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 6, 28, 56):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(6, ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 28, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[48, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #29: GFLOPs: 2.9894. Time: 7.3013 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #30: GFLOPs: 1.0714. Time: 20.3726 ms. Best GFLOPs: 4.0055
[03:16:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #31: GFLOPs: 1.0090. Time: 21.6328 ms. Best GFLOPs: 4.0055
[03:16:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 292
Total latency (us): 51514.9

[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #0: GFLOPs: 1.3441. Time: 16.7984 ms. Best GFLOPs: 1.3441
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #1: GFLOPs: 1.1683. Time: 19.3267 ms. Best GFLOPs: 1.3441
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #2: GFLOPs: 1.0769. Time: 20.9667 ms. Best GFLOPs: 1.3441
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 3, 1, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 2, 2, 3, 28, 4):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(36, i1_1 * 12 + i1_2_init * 3 + i1_3_init)
                            oh = T.axis.spatial(56, i2_2_init * 28 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 8 + i3_2_init * 4 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(4, 1, 1, 1, 4, 2, 2, 1, 6, 1, 1, 1, 3, 28, 4):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(36, i1_1 * 12 + i1_2 * 3 + i1_3)
                            oh = T.axis.spatial(56, i2_2 * 28 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 8 + i3_2 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(24, i5_0 * 6 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 36, 56, 8):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 8 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 3, 4, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 28])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #4: GFLOPs: 1.7017. Time: 13.2690 ms. Best GFLOPs: 1.7017
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #5: GFLOPs: 1.6693. Time: 13.5263 ms. Best GFLOPs: 1.7017
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #6: GFLOPs: 2.9200. Time: 7.7326 ms. Best GFLOPs: 2.9200
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #7: GFLOPs: 1.3276. Time: 17.0079 ms. Best GFLOPs: 2.9200
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #8: GFLOPs: 0.6515. Time: 34.6561 ms. Best GFLOPs: 2.9200
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #9: GFLOPs: 2.7824. Time: 8.1149 ms. Best GFLOPs: 2.9200
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #10: GFLOPs: 1.8210. Time: 12.3991 ms. Best GFLOPs: 2.9200
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #11: GFLOPs: 1.1116. Time: 20.3130 ms. Best GFLOPs: 2.9200
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #12: GFLOPs: 0.5747. Time: 39.2908 ms. Best GFLOPs: 2.9200
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #13: GFLOPs: 1.2012. Time: 18.7975 ms. Best GFLOPs: 2.9200
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #14: GFLOPs: 2.7261. Time: 8.2826 ms. Best GFLOPs: 2.9200
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #15: GFLOPs: 2.2072. Time: 10.2299 ms. Best GFLOPs: 2.9200
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(42, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 3, 1, 7, 2):
                for i1_2_init, i2_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(4, 2, 4, 2, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 12 + i1_1 * 4 + i1_2_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 4 + i2_2_init * 2 + i2_3_init)
                            ow = T.axis.spatial(56, i3_1 * 8 + i3_2_init * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(12, 1, 1, 1, 4, 2, 4, 1, 2, 1, 1, 1, 1, 2, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 12 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 4 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(56, i3_1 * 8 + i3_2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                            ic = T.axis.reduce(24, i5_0 * 2 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 4, 56):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 12 + ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 4 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 3, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 4, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[12, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(588, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 2):
                for i1_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(4, 8, 2, 3, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 196 * 12 + i1_2_init * 3 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 196 // 49 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 49 // 7 * 8 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(24, 1, 1, 1, 4, 1, 8, 2, 1, 1, 1, 1, 3, 2, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 196 * 12 + i1_2 * 3 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 196 // 49 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 2 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 49 // 7 * 8 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(24, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 12, 2, 8):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 196 * 12 + ax1)
                            ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 196 // 49 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 2 + ax2)
                            ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 49 // 7 * 8 + ax3)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 1, 4, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 7, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 8, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b67)
l103 = sch.fuse(l102)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b104)
b124 = sch.decompose_reduction(block=b104, loop=l108)
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #18: GFLOPs: 3.0645. Time: 7.3680 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #19: GFLOPs: 1.4993. Time: 15.0599 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #20: GFLOPs: 2.4215. Time: 9.3244 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #21: GFLOPs: 2.8230. Time: 7.9984 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #22: GFLOPs: 1.4115. Time: 15.9971 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #23: GFLOPs: 1.8065. Time: 12.4991 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(24, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 1, 1):
                for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(4, 7, 4, 3, 28):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 12 + i1_2_init * 3 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + i2_1 * 7 + i2_2_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 28 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(6, 1, 1, 1, 4, 7, 1, 4, 4, 1, 1, 1, 3, 1, 28, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 12 + i1_2 * 3 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 28 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(24, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 56, 56, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 14, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 12 + ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 14 + ax2)
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 28 + ax3)
                        ax4 = T.axis.spatial(4, ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 1, 4, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #25: GFLOPs: 1.8530. Time: 12.1854 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #26: GFLOPs: 0.6708. Time: 33.6625 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #27: GFLOPs: 2.0217. Time: 11.1685 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #28: GFLOPs: 0.7007. Time: 32.2246 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #29: GFLOPs: 2.0167. Time: 11.1960 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #30: GFLOPs: 1.0585. Time: 21.3304 ms. Best GFLOPs: 3.0645
[03:16:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"] Trial #31: GFLOPs: 1.2351. Time: 18.2820 ms. Best GFLOPs: 3.0645
[03:17:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 324
Total latency (us): 66250.9

[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #0: GFLOPs: 0.0884. Time: 25.5360 ms. Best GFLOPs: 0.0884
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(2088, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(58):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(36, i0_i1_i2_fused // 58)
                        i2 = T.axis.spatial(58, i0_i1_i2_fused % 58)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(48, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 6, 7, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                for i3_3_init in T.serial(28):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 6 + i1_1)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 14 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 28, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 6 + i1_1)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 14 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 14, 28, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 8 * 6 + ax1)
                    ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 14 + ax2)
                    ax3_1 = T.axis.spatial(28, ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                    T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 6, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77)
sch.parallel(loop=l97)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b104)
b125 = sch.decompose_reduction(block=b104, loop=l118)
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #2: GFLOPs: 0.3838. Time: 5.8833 ms. Best GFLOPs: 0.3838
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #3: GFLOPs: 0.3004. Time: 7.5164 ms. Best GFLOPs: 0.3838
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #4: GFLOPs: 0.1711. Time: 13.1930 ms. Best GFLOPs: 0.3838
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 9, 29, 9):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 112 // 28 * 9 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 14 * 28 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 8 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 3, 2, 1, 2):
                for i1_2_init, i2_2_init, i3_2_init, i3_3_init in T.grid(3, 7, 2, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 9 + i1_1 * 3 + i1_2_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 14 * 14 + i2_1 * 7 + i2_2_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 4 + i3_2_init * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 3, 7, 2, 1, 1, 1, 1, 1, 1, 2, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 9 + i1_1 * 3 + i1_2)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 14 * 14 + i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 4 + i3_2 * 2 + i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 9, 14, 4):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 9 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 14 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 4 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 3, 3, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b63)
l76 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l76)
l77 = sch.fuse(l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b65)
l104 = sch.fuse(l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b105)
b126 = sch.decompose_reduction(block=b105, loop=l112)
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #6: GFLOPs: 0.3289. Time: 6.8650 ms. Best GFLOPs: 0.3838
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #7: GFLOPs: 0.3639. Time: 6.2048 ms. Best GFLOPs: 0.3838
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #8: GFLOPs: 0.1730. Time: 13.0491 ms. Best GFLOPs: 0.3838
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #9: GFLOPs: 0.5061. Time: 4.4612 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #10: GFLOPs: 0.2100. Time: 10.7504 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #11: GFLOPs: 0.0605. Time: 37.3512 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #12: GFLOPs: 0.1589. Time: 14.2073 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #13: GFLOPs: 0.3011. Time: 7.4978 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(6, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1 in T.grid(1, 1, 1, 7):
                for ax0, ax1, ax2 in T.grid(1, 12, 57):
                    for ax3_ax4_fused in T.vectorized(20):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 6 // 2 * 12 + ax1)
                            i2 = T.axis.spatial(58, ax2)
                            i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 28 + i3_1 * 4 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1 in T.serial(2):
                    for i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(14, 12, 2, 2):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 12 + i1_3_init)
                                oh = T.axis.spatial(28, i2_2_init * 2 + i2_3_init)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 14 + i3_1 * 2 + i3_3_init)
                                oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 1, 14, 1, 1, 1, 1, 1, 12, 2, 2):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 12 + i1_3)
                                oh = T.axis.spatial(28, i2_2 * 2 + i2_3)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 14 + i3_1 * 2 + i3_3)
                                oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 12, 28):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 12 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 14 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 1, 1, 12])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b63)
l80 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l80)
l81 = sch.fuse(l78, l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b65)
l109 = sch.fuse(l107, l108)
sch.vectorize(loop=l109)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b110 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b110)
b131 = sch.decompose_reduction(block=b110, loop=l117)
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #15: GFLOPs: 0.3576. Time: 6.3139 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 9, 29):
                for ax3_ax4_fused in T.vectorized(20):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 112 // 56 * 18 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 9 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 28 * 28 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 2 * 4 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_1, i3_1, i4_1 in T.grid(2, 1, 4):
                for i1_2_init, i2_2_init, i3_3_init in T.grid(9, 7, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 56 * 18 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 9 + i1_2_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 28 * 14 + i2_1 * 7 + i2_2_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 2 * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 9, 7, 1, 1, 1, 1, 1, 1, 1, 2, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 56 * 18 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 9 + i1_2)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 28 * 14 + i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 2 * 2 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 9, 7, 2, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 56 * 18 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 9 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 28 * 14 + i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 2 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 9, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b63)
l78 = sch.fuse(l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l76, l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b107)
b126 = sch.decompose_reduction(block=b107, loop=l112)
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #17: GFLOPs: 0.1725. Time: 13.0866 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #18: GFLOPs: 0.1089. Time: 20.7376 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #19: GFLOPs: 0.2143. Time: 10.5376 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #20: GFLOPs: 0.1210. Time: 18.6638 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #21: GFLOPs: 0.4188. Time: 5.3917 ms. Best GFLOPs: 0.5061
[03:17:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(168, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 5, 29):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 168 // 56 * 12 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 * 4 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 28 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_1, i4_1 in T.grid(1, 1):
                for i1_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(3, 2, 4, 2, 14):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 12 + i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 * 2 + i2_3_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 14 + i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 3, 1, 1, 2, 1, 3, 1, 4, 2, 14, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 12 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 * 2 + i2_3)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 14 + i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 12, 2, 14):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 56 * 12 + ax1)
                            ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 * 2 + ax2)
                            ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 14 + ax3)
                            ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 1, 3, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b63)
l79 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l79)
l80 = sch.fuse(l78)
sch.vectorize(loop=l80)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b107)
b125 = sch.decompose_reduction(block=b107, loop=l111)
[03:17:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(24, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1 in T.grid(1, 2, 7, 7):
                for ax0, ax1, ax2 in T.grid(1, 3, 3):
                    for ax3_ax4_fused in T.vectorized(36):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 24 // 4 * 6 + i1_1 * 3 + ax1)
                            i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 14 + i2_1 * 2 + ax2)
                            i3 = T.axis.spatial(58, i3_1 * 8 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1, i5_0 in T.grid(1, 1):
                    for i3_2_init, i1_3_init, i3_3_init in T.grid(2, 3, 2):
                        for i4_3_fused_init in T.vectorized(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 6 + i1_1 * 3 + i1_3_init)
                                oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 7 + i2_1)
                                ow = T.axis.spatial(28, i3_1 * 4 + i3_2_init * 2 + i3_3_init)
                                oci = T.axis.spatial(4, i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 1, 1, 2, 1, 3, 1, 1, 3, 1, 2):
                        for i4_3_fused in T.vectorized(4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 6 + i1_1 * 3 + i1_3)
                                oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 7 + i2_1)
                                ow = T.axis.spatial(28, i3_1 * 4 + i3_2 * 2 + i3_3)
                                oci, kh, kw = T.axis.remap("SRR", [i4_3_fused, i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 6, 7, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 6 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 7 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 2, 1, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b63)
l80 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l80)
l81 = sch.fuse(l78, l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b65)
l109 = sch.fuse(l108)
sch.vectorize(loop=l109)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b110 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b110)
b131 = sch.decompose_reduction(block=b110, loop=l118)
[03:17:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #24: GFLOPs: 0.2091. Time: 10.7980 ms. Best GFLOPs: 0.5061
[03:17:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #25: GFLOPs: 0.2169. Time: 10.4085 ms. Best GFLOPs: 0.5061
[03:17:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 36, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(21, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 12, 57):
                for ax3_ax4_fused in T.vectorized(36):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_fused % 21 // 7 * 12 + ax1)
                        i2 = T.axis.spatial(58, ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 3, 2, 2, 2):
                for i5_0 in T.serial(1):
                    for i1_2_init, i2_2_init, i2_3_init, i3_3_init in T.grid(4, 7, 2, 2):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_fused // 7 * 12 + i1_1 * 4 + i1_2_init)
                                oh = T.axis.spatial(28, i2_1 * 14 + i2_2_init * 2 + i2_3_init)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_1 * 2 + i3_3_init)
                                oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 4, 7, 1, 1, 3, 1, 1, 1, 2, 2):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_fused // 7 * 12 + i1_1 * 4 + i1_2)
                                oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 2 + i2_3)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_1 * 2 + i3_3)
                                oci = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 2):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_fused // 7 * 12 + i1_1 * 4 + ax1)
                            ax2_1 = T.axis.spatial(28, i2_1 * 14 + ax2)
                            ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_1 * 2 + ax3)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 3, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b63)
l75 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l75)
l76 = sch.fuse(l73, l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b65)
l111 = sch.fuse(l110)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b112)
b134 = sch.decompose_reduction(block=b112, loop=l121)
[03:17:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #27: GFLOPs: 0.1186. Time: 19.0348 ms. Best GFLOPs: 0.5061
[03:17:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #28: GFLOPs: 0.0718. Time: 31.4339 ms. Best GFLOPs: 0.5061
[03:17:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #29: GFLOPs: 0.3243. Time: 6.9619 ms. Best GFLOPs: 0.5061
[03:17:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #30: GFLOPs: 0.0457. Time: 49.3721 ms. Best GFLOPs: 0.5061
[03:17:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"] Trial #31: GFLOPs: 0.1391. Time: 16.2293 ms. Best GFLOPs: 0.5061
[03:17:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 356
Total latency (us): 70712.1

[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #0: GFLOPs: 1.1871. Time: 6.1076 ms. Best GFLOPs: 1.1871
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #1: GFLOPs: 0.8432. Time: 8.5992 ms. Best GFLOPs: 1.1871
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #2: GFLOPs: 1.4629. Time: 4.9564 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #3: GFLOPs: 0.8851. Time: 8.1920 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #4: GFLOPs: 1.0027. Time: 7.2306 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #5: GFLOPs: 1.0429. Time: 6.9525 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #6: GFLOPs: 0.3496. Time: 20.7365 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #7: GFLOPs: 0.7839. Time: 9.2494 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #8: GFLOPs: 0.7855. Time: 9.2305 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #9: GFLOPs: 1.3430. Time: 5.3986 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #10: GFLOPs: 1.2866. Time: 5.6355 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #11: GFLOPs: 1.4170. Time: 5.1168 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #12: GFLOPs: 0.5885. Time: 12.3200 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #13: GFLOPs: 0.3988. Time: 18.1794 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #14: GFLOPs: 0.8129. Time: 8.9194 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #15: GFLOPs: 0.4621. Time: 15.6915 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #16: GFLOPs: 0.7364. Time: 9.8456 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #17: GFLOPs: 1.0027. Time: 7.2309 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #18: GFLOPs: 0.9354. Time: 7.7509 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #19: GFLOPs: 0.7833. Time: 9.2563 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #20: GFLOPs: 0.7061. Time: 10.2679 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #21: GFLOPs: 0.6747. Time: 10.7462 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #22: GFLOPs: 1.2737. Time: 5.6923 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #23: GFLOPs: 0.3341. Time: 21.6992 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #24: GFLOPs: 0.9506. Time: 7.6275 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #25: GFLOPs: 0.5099. Time: 14.2207 ms. Best GFLOPs: 1.4629
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #26: GFLOPs: 1.6952. Time: 4.2771 ms. Best GFLOPs: 1.6952
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #27: GFLOPs: 0.7251. Time: 9.9986 ms. Best GFLOPs: 1.6952
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #28: GFLOPs: 1.4057. Time: 5.1579 ms. Best GFLOPs: 1.6952
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(196, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 2, 4):
                for i1_2_init, i3_2_init, i2_3_init in T.grid(2, 2, 4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 2 + i1_2_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 28 * 4 + i2_3_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 4 * 4 + i3_1 * 2 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 28, 28, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 2, 1, 2, 1, 48, 1, 1, 1, 1, 4, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 2 + i1_2)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 28 * 4 + i2_3)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 4 * 4 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(144, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 28, 28, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 4, 2, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 28 * 4 + ax2)
                        ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 4 * 4 + i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 4, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 1, 4])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[3, 48])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b66)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b103)
b124 = sch.decompose_reduction(block=b103, loop=l108)
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #30: GFLOPs: 0.9630. Time: 7.5289 ms. Best GFLOPs: 1.6952
[03:17:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #31: GFLOPs: 0.6017. Time: 12.0493 ms. Best GFLOPs: 1.6952
[03:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 388
Total latency (us): 74989.1

[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #0: GFLOPs: 1.0067. Time: 9.6198 ms. Best GFLOPs: 1.0067
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #1: GFLOPs: 2.1419. Time: 4.5213 ms. Best GFLOPs: 2.1419
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #2: GFLOPs: 0.9933. Time: 9.7488 ms. Best GFLOPs: 2.1419
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #3: GFLOPs: 2.2896. Time: 4.2295 ms. Best GFLOPs: 2.2896
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 2, 2):
                for i3_2_init, i2_3_init, i3_3_init in T.grid(2, 2, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 4 + i1_1)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 4 + i2_1 * 2 + i2_3_init)
                        ow = T.axis.spatial(28, i3_1 * 14 + i3_2_init * 7 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(96, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 7, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 4 + i1_1)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        ic = T.axis.reduce(192, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 4 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 4 + ax2)
                        ax3_1 = T.axis.spatial(28, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 2, 2):
                for i2_2_init, i3_3_init in T.grid(14, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 4 + i1_1)
                            oh = T.axis.spatial(28, i2_1 * 14 + i2_2_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 4 + i3_1 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(6, 1, 1, 1, 1, 14, 1, 1, 32, 1, 1, 1, 1, 1, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 4 + i1_1)
                            oh = T.axis.spatial(28, i2_1 * 14 + i2_2)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 4 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                            ic = T.axis.reduce(192, i5_0 * 32 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 4, 28):
                for ax3_ax4_fused in T.vectorized(16):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 4 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 4 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #6: GFLOPs: 1.0149. Time: 9.5417 ms. Best GFLOPs: 2.2896
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 14, 1):
                for i3_2_init, i4_2_init in T.grid(2, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 4 + i1_1)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 2)
                        ow = T.axis.spatial(28, i3_1 * 2 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 1, 1, 2, 2, 16, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 4 + i1_1)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 2)
                        ow = T.axis.spatial(28, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                        ic = T.axis.reduce(192, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 4 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 2)
                        ax3_1 = T.axis.spatial(28, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[28, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[12, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=56)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i3_3_init in T.grid(4, 7, 2, 14):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 4 + i1_2_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + i2_2_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 14 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(6, 1, 1, 1, 4, 7, 1, 2, 32, 1, 1, 1, 1, 1, 14):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 4 + i1_2)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + i2_2)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(192, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #9: GFLOPs: 2.3402. Time: 4.1381 ms. Best GFLOPs: 2.3402
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #10: GFLOPs: 0.8477. Time: 11.4232 ms. Best GFLOPs: 2.3402
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #11: GFLOPs: 0.5732. Time: 16.8955 ms. Best GFLOPs: 2.3402
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #12: GFLOPs: 5.7644. Time: 1.6800 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #13: GFLOPs: 1.0814. Time: 8.9547 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #14: GFLOPs: 1.2285. Time: 7.8826 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #15: GFLOPs: 1.6171. Time: 5.9885 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #16: GFLOPs: 0.8878. Time: 10.9081 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #17: GFLOPs: 1.5956. Time: 6.0690 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #18: GFLOPs: 0.8611. Time: 11.2467 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #19: GFLOPs: 2.0178. Time: 4.7993 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #20: GFLOPs: 0.7009. Time: 13.8166 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #21: GFLOPs: 0.7262. Time: 13.3344 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(784, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(2, 2):
                for i1_2_init, i2_2_init, i3_2_init in T.grid(2, 2, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 7 * 2 + i1_2_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 392 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 2 + i2_2_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 392 // 56 * 4 + i3_1 * 2 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(6, 1, 1, 1, 2, 2, 2, 1, 32, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 7 * 2 + i1_2)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 392 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7 * 2 + i2_2)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 392 // 56 * 4 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 56 // 28 * 2 + i4_1)
                        ic = T.axis.reduce(192, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b101)
b121 = sch.decompose_reduction(block=b101, loop=l105)
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #23: GFLOPs: 2.2913. Time: 4.2264 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #24: GFLOPs: 1.2111. Time: 7.9962 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #25: GFLOPs: 1.0753. Time: 9.0061 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 8, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(8, 14, 7, 2, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_2_init)
                    oh = T.axis.spatial(28, i2_2_init * 2 + i2_3_init)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 14 + i3_2_init * 2 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 8, 14, 7, 1, 16, 1, 1, 1, 1, 2, 2, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_2)
                    oh = T.axis.spatial(28, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 14 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(192, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 14, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[12, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #27: GFLOPs: 1.2499. Time: 7.7477 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #28: GFLOPs: 1.4100. Time: 6.8679 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #29: GFLOPs: 2.2484. Time: 4.3070 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #30: GFLOPs: 1.0630. Time: 9.1098 ms. Best GFLOPs: 5.7644
[03:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #31: GFLOPs: 2.0044. Time: 4.8313 ms. Best GFLOPs: 5.7644
[03:18:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 420
Total latency (us): 78349.1

[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #0: GFLOPs: 1.8389. Time: 5.4027 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #1: GFLOPs: 1.3447. Time: 7.3880 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #2: GFLOPs: 0.9707. Time: 10.2349 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #3: GFLOPs: 0.6196. Time: 16.0334 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #4: GFLOPs: 1.1836. Time: 8.3936 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #5: GFLOPs: 1.4781. Time: 6.7215 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #6: GFLOPs: 0.7142. Time: 13.9106 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #7: GFLOPs: 0.5883. Time: 16.8860 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #8: GFLOPs: 0.7588. Time: 13.0932 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #9: GFLOPs: 0.8134. Time: 12.2136 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #10: GFLOPs: 1.4933. Time: 6.6531 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 2, 1):
                for i1_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(6, 4, 2, 7, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i2_1 * 7 + i2_3_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_1 * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 28, 28, 4], "float32"], ["TENSOR", [48, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 6, 1, 1, 4, 1, 1, 1, 1, 2, 7, 2, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_1 * 2 + i3_3)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 28, 28, 4], "float32"], ["TENSOR", [48, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(48, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(28, 28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 6, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=8)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b101)
b125 = sch.decompose_reduction(block=b101, loop=l109)
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #12: GFLOPs: 0.9441. Time: 10.5228 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #13: GFLOPs: 1.1674. Time: 8.5103 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #14: GFLOPs: 0.5845. Time: 16.9972 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #15: GFLOPs: 1.7736. Time: 5.6015 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #16: GFLOPs: 0.9784. Time: 10.1537 ms. Best GFLOPs: 1.8389
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #17: GFLOPs: 1.8967. Time: 5.2379 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #18: GFLOPs: 1.0764. Time: 9.2297 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #19: GFLOPs: 0.4470. Time: 22.2262 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #20: GFLOPs: 0.4807. Time: 20.6659 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #21: GFLOPs: 0.9991. Time: 9.9437 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #22: GFLOPs: 1.1900. Time: 8.3486 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #23: GFLOPs: 1.7957. Time: 5.5326 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #24: GFLOPs: 1.0976. Time: 9.0511 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #25: GFLOPs: 0.6458. Time: 15.3832 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #26: GFLOPs: 1.1758. Time: 8.4492 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #27: GFLOPs: 0.8216. Time: 12.0920 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #28: GFLOPs: 1.3146. Time: 7.5575 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #29: GFLOPs: 1.4896. Time: 6.6695 ms. Best GFLOPs: 1.8967
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #30: GFLOPs: 2.3071. Time: 4.3062 ms. Best GFLOPs: 2.3071
[03:18:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"] Trial #31: GFLOPs: 0.6018. Time: 16.5082 ms. Best GFLOPs: 2.3071
[03:18:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 452
Total latency (us): 91267.7

[03:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 48, 4, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_fused % 14 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 2, 1, 28, 1):
                for i5_0 in T.serial(1):
                    for i1_3_init, i2_3_init in T.grid(24, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i1_1 * 24 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused * 2 + i2_3_init)
                            ow, oci = T.axis.remap("SS", [i3_1, i4_0])
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 1, 3, 1, 1, 24, 2, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i1_1 * 24 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused * 2 + i2_3)
                            ow, oci, kh, kw = T.axis.remap("SSRR", [i3_1, i4_0, i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 24, 2, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(48, i1_1 * 24 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_fused * 2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [i3_1, i4_0])
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 24])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b111)
b134 = sch.decompose_reduction(block=b111, loop=l121)
[03:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #1: GFLOPs: 0.2855. Time: 10.5442 ms. Best GFLOPs: 0.2855
[03:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #2: GFLOPs: 0.5802. Time: 5.1891 ms. Best GFLOPs: 0.5802
[03:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #3: GFLOPs: 0.6001. Time: 5.0170 ms. Best GFLOPs: 0.6001
[03:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #4: GFLOPs: 0.5977. Time: 5.0365 ms. Best GFLOPs: 0.6001
[03:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #5: GFLOPs: 0.2626. Time: 11.4627 ms. Best GFLOPs: 0.6001
[03:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(42, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 16, 16):
                for ax3_ax4_fused in T.vectorized(24):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 6 // 2 * 16 + ax1)
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 14 + ax2)
                        i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 42 // 6 * 4 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_1, i4_1 in T.grid(1, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 16, 2, 4, 4):
                    for i2_3_init in T.serial(7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 6 // 2 * 16 + i1_2)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 14 + i2_2 * 7 + i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 6 * 4 + i3_2)
                            oci = T.axis.spatial(4, i4_2)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 7, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 6 // 2 * 16 + i1_2)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 14 + i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 6 * 4 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 16, 14):
                    for ax3_ax4_fused in T.vectorized(16):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 6 // 2 * 16 + ax1)
                            ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 14 + ax2)
                            ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 6 * 4 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 3, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b63)
l79 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l79)
l80 = sch.fuse(l77, l78)
sch.vectorize(loop=l80)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b107)
b125 = sch.decompose_reduction(block=b107, loop=l118)
[03:18:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1440, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(30):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(48, i0_i1_i2_fused // 30)
                        i2 = T.axis.spatial(30, i0_i1_i2_fused % 30)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(384, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1_1, i4_1 in T.grid(1, 7, 1):
                for i2_2_init, i3_2_init, i2_3_init in T.grid(7, 4, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 64 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 32 * 14 + i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(28, i3_1_1 * 4 + i3_2_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 // 8)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 7, 4, 1, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 64 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 32 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_1_1 * 4 + i3_2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 // 8)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 14, 4, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 64 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 32 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_1_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 // 8)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 8, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77, l78, l79)
sch.parallel(loop=l97)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b107)
b126 = sch.decompose_reduction(block=b107, loop=l112)
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #8: GFLOPs: 0.2796. Time: 10.7680 ms. Best GFLOPs: 0.6001
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #9: GFLOPs: 0.1210. Time: 24.8871 ms. Best GFLOPs: 0.6001
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #10: GFLOPs: 0.5386. Time: 5.5893 ms. Best GFLOPs: 0.6001
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #11: GFLOPs: 0.6774. Time: 4.4442 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #12: GFLOPs: 0.4040. Time: 7.4527 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 6, 30, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(48, i0_0_i1_0_fused * 6 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0 in T.grid(1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 1, 14, 2, 1, 1):
                    for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(2, 2, 3, 14):
                        for i4_3_fused_init in T.vectorized(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(48, i0_0_i1_0_fused * 6 + i1_2_init * 3 + i1_3_init)
                                oh = T.axis.spatial(28, i2_1 * 2 + i2_2_init)
                                ow = T.axis.spatial(28, i3_1 * 14 + i3_3_init)
                                oci = T.axis.spatial(4, i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 2, 2, 1, 1, 3, 1, 1, 3, 1, 14):
                        for i4_3_fused in T.vectorized(4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(48, i0_0_i1_0_fused * 6 + i1_2 * 3 + i1_3)
                                oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                                ow = T.axis.spatial(28, i3_1 * 14 + i3_3)
                                oci, kh, kw = T.axis.remap("SRR", [i4_3_fused, i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 6, 28, 28):
                    for ax4_fused in T.vectorized(4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(48, i0_0_i1_0_fused * 6 + ax1)
                            ax2_1, ax3_1, ax4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 2, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b63)
l73 = sch.fuse(l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b65)
l108 = sch.fuse(l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b109)
b133 = sch.decompose_reduction(block=b109, loop=l120)
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #14: GFLOPs: 0.3073. Time: 9.7967 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #15: GFLOPs: 0.0645. Time: 46.6587 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1 in T.grid(1, 6):
                for ax0, ax1, ax2 in T.grid(1, 8, 30):
                    for ax3_ax4_fused in T.vectorized(12):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(48, i1_1 * 8 + ax1)
                            i2 = T.axis.spatial(30, ax2)
                            i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                    for i1_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(4, 2, 2, 28):
                        for i3_3_i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(48, i1_1 * 8 + i1_2_init * 2 + i1_3_init)
                                oh, ow = T.axis.remap("SS", [i2_3_init, i0_0_i1_0_i2_0_i3_0_i4_0_fused])
                                oci = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 1, 1, 4, 1, 1, 2, 1, 3, 1, 2, 28):
                        for i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(48, i1_1 * 8 + i1_2 * 2 + i1_3)
                                oh, ow = T.axis.remap("SS", [i2_3, i0_0_i1_0_i2_0_i3_0_i4_0_fused])
                                oci = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 48, 28):
                for ax3_ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3, ax4 = T.axis.remap("SSSS", [ax1, ax2, i0_0_i1_0_i2_0_i3_0_i4_0_fused, ax3_ax4_fused])
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 6, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[28, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b63)
l78 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l78)
l79 = sch.fuse(l76, l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b64)
l100 = sch.fuse(l98, l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
l107 = sch.fuse(l105, l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l101, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l101, ann_key="pragma_unroll_explicit", ann_val=1)
b108 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b108)
b128 = sch.decompose_reduction(block=b108, loop=l115)
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #17: GFLOPs: 0.2895. Time: 10.3976 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #18: GFLOPs: 0.6117. Time: 4.9219 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(294, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 16, 4):
                for ax3_ax4_fused in T.vectorized(24):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused % 294 // 98 * 16 + ax1)
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 2 + ax2)
                        i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_0 in T.serial(1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1):
                    for i1_3_init, i2_3_init in T.grid(4, 2):
                        for i3_3_i4_3_fused_init in T.vectorized(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 98 * 16 + i1_2 * 4 + i1_3_init)
                                oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 2 + i2_3_init)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_1)
                                oci = T.axis.spatial(4, i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 3, 1, 4, 2):
                        for i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 98 * 16 + i1_2 * 4 + i1_3)
                                oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 2 + i2_3)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_1)
                                oci, kh, kw = T.axis.remap("SRR", [i3_3_i4_3_fused, i5_1, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 16, 2):
                    for ax3_ax4_fused in T.vectorized(16):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 98 * 16 + ax1)
                            ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 2 + ax2)
                            ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 1, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 4, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b63)
l75 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l75)
l76 = sch.fuse(l73, l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l96, l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b107)
b128 = sch.decompose_reduction(block=b107, loop=l122)
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #20: GFLOPs: 0.5928. Time: 5.0783 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(392, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 12, 1, 1, 1, 1):
                    for i1_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(2, 4, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 196 * 24 + i1_1 * 2 + i1_2_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 2 + i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_3_init)
                            oci = T.axis.spatial(4, i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0 in T.serial(3):
                        for ax0, ax1, ax2 in T.grid(1, 2, 4):
                            for ax3_ax4_fused in T.vectorized(8):
                                with T.block("PaddedInput"):
                                    i0 = T.axis.spatial(1, ax0)
                                    i1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused % 392 // 196 * 24 + i1_1 * 2 + ax1)
                                    i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 2 + ax2)
                                    i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i6_0 + ax3_ax4_fused // 4)
                                    i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                        for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 4, 3, 1, 1, 1, 2, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 196 * 24 + i1_1 * 2 + i1_2)
                                oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 2 + i2_3)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_3)
                                oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 24, 2):
                    for ax3_ax4_fused in T.vectorized(8):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 196 * 24 + ax1)
                            ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 196 // 14 * 2 + ax2)
                            ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 12, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b63)
l83 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l83)
l84 = sch.fuse(l81, l82)
sch.vectorize(loop=l84)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b65)
l113 = sch.fuse(l111, l112)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l106, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l106, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b114)
b136 = sch.decompose_reduction(block=b114, loop=l123)
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 48, 16, 30):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 14 + ax2)
                        i3 = T.axis.spatial(30, ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 6, 1, 1, 1):
                for i2_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 28, 2, 8, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(48, i1_1 * 8 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 14 + i2_2_init * 7 + i2_3_init)
                        ow = T.axis.spatial(28, i3_2_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 2, 28, 2, 1, 1, 1, 8, 7, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(48, i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 14 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 48, 14, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(48, ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 6, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 28, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b63)
l76 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l76)
l77 = sch.fuse(l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b65)
l104 = sch.fuse(l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b105)
b126 = sch.decompose_reduction(block=b105, loop=l112)
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #23: GFLOPs: 0.1673. Time: 17.9986 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #24: GFLOPs: 0.4642. Time: 6.4860 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #25: GFLOPs: 0.4233. Time: 7.1113 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #26: GFLOPs: 0.4470. Time: 6.7343 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #27: GFLOPs: 0.3972. Time: 7.5794 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #28: GFLOPs: 0.0612. Time: 49.1669 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #29: GFLOPs: 0.3024. Time: 9.9553 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #30: GFLOPs: 0.2015. Time: 14.9382 ms. Best GFLOPs: 0.6774
[03:18:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 24, 6, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i0_0_i1_0_i2_0_fused % 14 // 7 * 24 + ax1)
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_fused % 7 * 4 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0 in T.grid(7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 4):
                    for i3_2_init, i1_3_init, i2_3_init in T.grid(4, 12, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_fused // 7 * 24 + i1_1 * 12 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 7 * 4 + i2_3_init)
                            ow = T.axis.spatial(28, i3_0 * 4 + i3_2_init)
                            oci = T.axis.spatial(4, i4_1)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 4, 1, 1, 3, 1, 12, 4, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_fused // 7 * 24 + i1_1 * 12 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 7 * 4 + i2_3)
                            ow = T.axis.spatial(28, i3_0 * 4 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 24, 4):
                    for ax3_ax4_fused in T.vectorized(16):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(48, i0_0_i1_0_i2_0_fused // 7 * 24 + ax1)
                            ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 7 * 4 + ax2)
                            ax3 = T.axis.spatial(28, i3_0 * 4 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 12])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b107)
b130 = sch.decompose_reduction(block=b107, loop=l116)
[03:18:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 484
Total latency (us): 104600

[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #0: GFLOPs: 1.3480. Time: 14.3312 ms. Best GFLOPs: 1.3480
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #1: GFLOPs: 1.4024. Time: 13.7752 ms. Best GFLOPs: 1.4024
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #2: GFLOPs: 0.5913. Time: 32.6691 ms. Best GFLOPs: 1.4024
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #3: GFLOPs: 1.1805. Time: 16.3639 ms. Best GFLOPs: 1.4024
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i2_3_init in T.grid(16, 7, 14, 2, 4):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_2_init)
                            oh = T.axis.spatial(28, i2_2_init * 4 + i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + i3_2_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(24, 1, 1, 1, 16, 7, 14, 2, 8, 1, 1, 1, 1, 4):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_2)
                            oh = T.axis.spatial(28, i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + i3_2)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(192, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 16, 28):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 16, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[24, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
l94 = sch.fuse(l91, l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b102)
b124 = sch.decompose_reduction(block=b102, loop=l109)
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #5: GFLOPs: 1.1233. Time: 17.1979 ms. Best GFLOPs: 1.4024
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #6: GFLOPs: 0.6586. Time: 29.3314 ms. Best GFLOPs: 1.4024
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #7: GFLOPs: 1.8978. Time: 10.1791 ms. Best GFLOPs: 1.8978
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #8: GFLOPs: 2.2652. Time: 8.5282 ms. Best GFLOPs: 2.2652
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #9: GFLOPs: 1.2265. Time: 15.7498 ms. Best GFLOPs: 2.2652
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #10: GFLOPs: 1.4491. Time: 13.3311 ms. Best GFLOPs: 2.2652
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(8, 4, 2, 7, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i1_2_init)
                    oh = T.axis.spatial(28, i2_2_init * 7 + i2_3_init)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(24, 1, 1, 1, 8, 4, 1, 2, 8, 1, 1, 1, 1, 7, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i1_2)
                    oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(192, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 28, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 7 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 8, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[24, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #12: GFLOPs: 1.5455. Time: 12.4990 ms. Best GFLOPs: 2.2652
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #13: GFLOPs: 1.4080. Time: 13.7197 ms. Best GFLOPs: 2.2652
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #14: GFLOPs: 0.9139. Time: 21.1385 ms. Best GFLOPs: 2.2652
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #15: GFLOPs: 2.5655. Time: 7.5298 ms. Best GFLOPs: 2.5655
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #16: GFLOPs: 1.2650. Time: 15.2709 ms. Best GFLOPs: 2.5655
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #17: GFLOPs: 1.1325. Time: 17.0582 ms. Best GFLOPs: 2.5655
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #18: GFLOPs: 0.9655. Time: 20.0082 ms. Best GFLOPs: 2.5655
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(2, 7, 7, 2, 4):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 4 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 7 + i2_2_init)
                    ow = T.axis.spatial(28, i3_2_init * 4 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(24, 1, 1, 1, 2, 7, 7, 1, 8, 1, 1, 1, 2, 1, 4, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 7 + i2_2)
                    ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 4)
                    ic = T.axis.reduce(192, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 28, 1):
                with T.block("T_add"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 4 + ax1)
                    ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 7 + ax2)
                    ax3_1 = T.axis.spatial(28, ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 4)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[24, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #20: GFLOPs: 6.1981. Time: 3.1167 ms. Best GFLOPs: 6.1981
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 4, 4, 7, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 8 + i1_2_init * 4 + i1_3_init)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 7 + i2_3_init)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4 * 4 + i3_2_init * 2 + i3_3_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 2, 1, 2, 4, 16, 1, 1, 1, 4, 7, 2, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 8 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 7 + i2_3)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4 * 4 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(192, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[12, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[03:18:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i4_2_init, i1_3_init in T.grid(2, 14, 4, 16):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_3_init)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 2 + i2_2_init)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 14 + i3_2_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 1, 2, 14, 4, 16, 1, 1, 1, 16, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_3)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 14 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 2 + i2_2)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 14 + i3_2)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(192, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 16])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 7, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[12, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[03:18:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_2_init, i4_2_init, i1_3_init in T.grid(2, 14, 4, 4):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 8 + i1_2_init * 4 + i1_3_init)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 4 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + i3_2_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 2, 1, 14, 4, 6, 1, 1, 1, 4, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 8 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56 // 4 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + i3_2)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(192, i5_0 * 6 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 6])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[03:18:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #24: GFLOPs: 0.9994. Time: 19.3300 ms. Best GFLOPs: 6.1981
[03:18:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #25: GFLOPs: 1.4950. Time: 12.9214 ms. Best GFLOPs: 6.1981
[03:18:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #26: GFLOPs: 1.6100. Time: 11.9986 ms. Best GFLOPs: 6.1981
[03:18:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i2_3_init in T.grid(4, 2, 4, 2, 7):
                for i3_3_i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 8 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 14 + i2_2_init * 7 + i2_3_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 4 + i3_2_init)
                        oc_block = T.axis.spatial(4, i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(64, 1, 1, 1, 4, 2, 4, 1, 3, 1, 1, 1, 2, 7):
                for i3_3_i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 14 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i3_3_i4_3_fused)
                        ic = T.axis.reduce(192, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [16, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 4, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 3])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
l93 = sch.fuse(l90, l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b101)
b118 = sch.decompose_reduction(block=b101, loop=l103)
[03:18:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #28: GFLOPs: 2.8979. Time: 6.6661 ms. Best GFLOPs: 6.1981
[03:18:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #29: GFLOPs: 0.9943. Time: 19.4287 ms. Best GFLOPs: 6.1981
[03:18:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #30: GFLOPs: 1.5749. Time: 12.2659 ms. Best GFLOPs: 6.1981
[03:18:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #31: GFLOPs: 1.6905. Time: 11.4271 ms. Best GFLOPs: 6.1981
[03:19:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 516
Total latency (us): 107717

[03:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #0: GFLOPs: 0.4856. Time: 12.3985 ms. Best GFLOPs: 0.4856
[03:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #1: GFLOPs: 0.1290. Time: 46.6613 ms. Best GFLOPs: 0.4856
[03:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(336, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 2):
                for i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 2, 4, 28):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + i1_3_init)
                        oh = T.axis.spatial(28, i2_3_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_2_init)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 28, 4):
                        for ax4_fused in T.vectorized(2):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused % 336 // 14 * 4 + ax1)
                                i2 = T.axis.spatial(30, i5_0 + ax2)
                                i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ax3)
                                i4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 2, 2, 1, 1, 1, 4, 28, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + i1_3)
                            oh = T.axis.spatial(28, i2_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_2)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 28, 2):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + ax1)
                            ax2_1 = T.axis.spatial(28, ax2)
                            ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ax3)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[24, 1, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b63)
l82 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l82)
l83 = sch.fuse(l81)
sch.vectorize(loop=l83)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b65)
l117 = sch.fuse(l116)
sch.vectorize(loop=l117)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b118 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b118)
b140 = sch.decompose_reduction(block=b118, loop=l126)
[03:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(336, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1 in T.grid(2, 1, 2, 2):
                for ax0, ax1, ax2, ax3 in T.grid(1, 2, 9, 6):
                    for ax4_fused in T.vectorized(2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused % 336 // 14 * 4 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i2_1 * 7 + ax2)
                            i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4_fused)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1 in T.grid(1, 1):
                    for i2_2_init, i1_3_init, i3_3_init in T.grid(7, 2, 4):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + i1_1 * 2 + i1_3_init)
                                oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i2_1 * 7 + i2_2_init)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_3_init)
                                oci = T.axis.spatial(4, i4_0 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 1, 1, 7, 1, 1, 1, 3, 1, 2, 1, 4):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + i1_1 * 2 + i1_3)
                                oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i2_1 * 7 + i2_2)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_3)
                                oci = T.axis.spatial(4, i4_0 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(2688, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(96, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[24, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b62)
l78 = sch.fuse(l65, l66, l67, l68)
sch.parallel(loop=l78)
l79 = sch.fuse(l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b63)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b64)
l107 = sch.fuse(l102, l103, l104)
sch.parallel(loop=l107)
l108 = sch.fuse(l106)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b109)
b131 = sch.decompose_reduction(block=b109, loop=l117)
[03:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #4: GFLOPs: 0.5645. Time: 10.6654 ms. Best GFLOPs: 0.5645
[03:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 30, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(96, i0_0_i1_0_fused * 12 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(4, 1, 2, 1, 3, 1, 1, 1):
                for i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(7, 2, 4, 7, 4):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i0_0_i1_0_fused * 12 + i1_1 * 4 + i1_3_init)
                        oh = T.axis.spatial(28, i2_0 * 7 + i2_3_init)
                        ow = T.axis.spatial(28, i3_2_init * 4 + i3_3_init)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 7, 2, 1, 3, 1, 4, 7, 4, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i0_0_i1_0_fused * 12 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(2688, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(96, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 3, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71 = sch.get_loops(block=b62)
l72 = sch.fuse(l65, l66)
sch.parallel(loop=l72)
l73 = sch.fuse(l71)
sch.vectorize(loop=l73)
sch.annotate(block_or_loop=l72, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l72, ann_key="pragma_unroll_explicit", ann_val=1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b104)
b128 = sch.decompose_reduction(block=b104, loop=l114)
[03:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1 in T.grid(1, 48, 1, 2):
                for ax0, ax1, ax2 in T.grid(1, 1, 9):
                    for ax3_ax4_fused in T.vectorized(12):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 112 // 56 * 48 + i1_1 + ax1)
                            i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 14 * 7 + ax2)
                            i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 2 + i3_1 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 1, 7, 1, 4):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 48 + i1_1)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 14 * 7 + i2_2)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 2 + i3_1)
                        oci = T.axis.spatial(4, i4_2)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 48 + i1_1)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 14 * 7 + i2_2)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 2 + i3_1)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 48, 7):
                for ax3_ax4_fused in T.vectorized(8):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 56 * 48 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 14 * 7 + ax2)
                        ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 * 2 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 48, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b63)
l80 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l80)
l81 = sch.fuse(l78, l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b65)
l108 = sch.fuse(l106, l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b109)
b130 = sch.decompose_reduction(block=b109, loop=l123)
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 12, 7, 2, 4, 1, 1, 1, 2, 1, 1, 1):
                for i2_3_init, i3_3_init in T.grid(4, 14):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 24 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_1 * 4 + i2_3_init)
                        ow = T.axis.spatial(28, i3_1 * 14 + i3_3_init)
                        oci = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 4, 14, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 24 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_1 * 4 + i2_3)
                        ow = T.axis.spatial(28, i3_1 * 14 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 29 and 1 <= ow + kw and ow + kw < 29, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 24, 28, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 24 + ax1)
                        ax2_1, ax3_1, ax4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 12, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
l96 = sch.fuse(l95)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b97)
b118 = sch.decompose_reduction(block=b97, loop=l111)
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #8: GFLOPs: 0.8778. Time: 6.8590 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 48, 9, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_fused % 8 // 4 * 48 + ax1)
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_fused % 4 * 7 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 1, 12, 1, 1, 2):
                for i5_0 in T.serial(1):
                    for i1_2_init, i3_2_init, i4_2_init, i2_3_init in T.grid(4, 14, 2, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_fused // 4 * 48 + i1_1 * 4 + i1_2_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 4 * 7 + i2_3_init)
                            ow = T.axis.spatial(28, i3_0 * 14 + i3_2_init)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 4, 1, 14, 2, 3, 1, 1, 1, 7, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_fused // 4 * 48 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 4 * 7 + i2_3)
                            ow = T.axis.spatial(28, i3_0 * 14 + i3_2)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 14):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_fused // 4 * 48 + i1_1 * 4 + ax1)
                            ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 4 * 7 + ax2)
                            ax3_1 = T.axis.spatial(28, i3_0 * 14 + ax3)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 12, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)
l74 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b65)
l111 = sch.fuse(l110)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b112)
b135 = sch.decompose_reduction(block=b112, loop=l122)
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(2880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(30):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(96, i0_i1_i2_fused // 30)
                        i2 = T.axis.spatial(30, i0_i1_i2_fused % 30)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(168, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1_1, i4_1 in T.grid(1, 2, 4):
                for i5_0 in T.serial(1):
                    for i1_2_init, i2_2_init, i3_2_init, i3_3_init in T.grid(8, 2, 2, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 56 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 8 + i1_2_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 4 * 2 + i2_2_init)
                            ow = T.axis.spatial(28, i3_1_1 * 14 + i3_2_init * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i4_1)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 8, 2, 2, 1, 3, 1, 1, 1, 1, 7, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 56 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 8 + i1_2)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 4 * 2 + i2_2)
                            ow = T.axis.spatial(28, i3_1_1 * 14 + i3_2 * 7 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 14, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 56 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 4 * 2 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_1_1 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 4, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77, l78, l79)
sch.parallel(loop=l97)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b107)
b126 = sch.decompose_reduction(block=b107, loop=l113)
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #11: GFLOPs: 0.5351. Time: 11.2529 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(2):
                for ax0, ax1, ax2, ax3 in T.grid(1, 48, 30, 4):
                    for ax4_fused in T.vectorized(2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 48 + ax1)
                            i2 = T.axis.spatial(30, ax2)
                            i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4_fused)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 4, 1, 2):
                    for i3_2_init, i1_3_init, i2_3_init in T.grid(2, 12, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + i1_1 * 12 + i1_3_init)
                            oh = T.axis.spatial(28, i2_1 * 7 + i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_2_init)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 2, 1, 1, 3, 1, 12, 7, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + i1_1 * 12 + i1_3)
                            oh = T.axis.spatial(28, i2_1 * 7 + i2_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_2)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 48, 28, 2):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + ax1)
                            ax2_1 = T.axis.spatial(28, ax2)
                            ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ax3)
                            ax4 = T.axis.spatial(4, i4_0 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 1, 12])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b63)
l76 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l76)
l77 = sch.fuse(l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b107)
b129 = sch.decompose_reduction(block=b107, loop=l115)
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #13: GFLOPs: 0.6177. Time: 9.7469 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(2880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(30):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(96, i0_i1_i2_fused // 30)
                        i2 = T.axis.spatial(30, i0_i1_i2_fused % 30)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1_1, i4_1, i5_0 in T.grid(1, 3, 1, 14, 1, 1):
                for i1_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 4, 16, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i1_1 * 32 + i1_2_init * 16 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 7 + i2_3_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 14 + i3_1_1)
                        oci = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 1, 1, 4, 3, 1, 1, 16, 7, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i1_1 * 32 + i1_2 * 16 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 14 + i3_1_1)
                        oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 96, 7):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(96, ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 7 + ax2)
                        ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 14 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 3, 2, 16])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77)
sch.parallel(loop=l97)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b65)
l104 = sch.fuse(l102, l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b105)
b126 = sch.decompose_reduction(block=b105, loop=l113)
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(98, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 3, 1, 1, 4):
                for i1_2_init, i2_3_init, i3_3_init in T.grid(32, 2, 4):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i1_1 * 32 + i1_2_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 2 + i2_3_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 4 + i3_3_init)
                        oci = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 32, 1, 1, 1, 1, 1, 1, 1, 2, 4, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i1_1 * 32 + i1_2)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 2 + i2_3)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 4 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 29 and 1 <= ow + kw and ow + kw < 29, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 96, 2):
                for ax3_ax4_fused in T.vectorized(16):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(96, ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 2 + ax2)
                        ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 4 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 3, 32, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
l96 = sch.fuse(l94, l95)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b97)
b118 = sch.decompose_reduction(block=b97, loop=l104)
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #16: GFLOPs: 0.6224. Time: 9.6745 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(192, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 16, 30, 1):
                with T.block("PaddedInput"):
                    i0 = T.axis.spatial(1, ax0)
                    i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 192 // 24 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 12 // 4 * 4 + ax1)
                    i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 24 // 12 * 14 + ax2)
                    i3 = T.axis.spatial(30, ax3)
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 + ax4)
                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i1_2_init, i2_2_init, i3_2_init, i1_3_init in T.grid(2, 14, 28, 2):
                with T.block("DepthwiseConv2d_init"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 24 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 12 // 4 * 4 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 24 // 12 * 14 + i2_2_init)
                    ow = T.axis.spatial(28, i3_2_init)
                    oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 14, 28, 1, 1, 1, 1, 2, 1, 1, 1):
                with T.block("DepthwiseConv2d_update"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 24 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 12 // 4 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 24 // 12 * 14 + i2_2)
                    ow = T.axis.spatial(28, i3_2)
                    oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                    T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 28, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 24 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 12 // 4 * 4 + ax1)
                    ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 24 // 12 * 14 + ax2)
                    ax3_1 = T.axis.spatial(28, ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 3, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 28, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b63)
l81 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l81)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b103)
b119 = sch.decompose_reduction(block=b103, loop=l105)
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #18: GFLOPs: 0.4871. Time: 12.3615 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #19: GFLOPs: 0.5211. Time: 11.5545 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #20: GFLOPs: 0.2604. Time: 23.1220 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #21: GFLOPs: 0.6157. Time: 9.7793 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_i6_0_i0_2_i1_2_fused in T.parallel(336, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2, i3_2, i4_2 in T.grid(4, 1, 2):
                for i1_3_init, i2_3_init, i3_3_init in T.grid(4, 7, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_i6_0_i0_2_i1_2_fused // 168 * 48 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_i6_0_i0_2_i1_2_fused % 12 * 4 + i1_3_init)
                            oh = T.axis.spatial(28, i2_2 * 7 + i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_i6_0_i0_2_i1_2_fused % 168 // 12 * 2 + i3_3_init)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1 in T.serial(3):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 4):
                        for ax4_fused in T.vectorized(2):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_i6_0_i0_2_i1_2_fused % 336 // 168 * 48 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_i6_0_i0_2_i1_2_fused % 12 * 4 + ax1)
                                i2 = T.axis.spatial(30, i2_2 * 7 + i5_1 + ax2)
                                i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_i6_0_i0_2_i1_2_fused % 168 // 12 * 2 + ax3)
                                i4 = T.axis.spatial(4, i4_2 * 2 + ax4_fused)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 4, 7, 2):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_i6_0_i0_2_i1_2_fused // 168 * 48 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_i6_0_i0_2_i1_2_fused % 12 * 4 + i1_3)
                                oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_i5_0_i6_0_i0_2_i1_2_fused % 168 // 12 * 2 + i3_3)
                                oci = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(2688, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(96, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 12, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 14, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=17)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b62)
l88 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78)
sch.parallel(loop=l88)
l89 = sch.fuse(l87)
sch.vectorize(loop=l89)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b63)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b64)
l107 = sch.fuse(l102, l103, l104)
sch.parallel(loop=l107)
l108 = sch.fuse(l106)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b109)
b121 = sch.decompose_reduction(block=b109, loop=l114)
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #23: GFLOPs: 0.1737. Time: 34.6608 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #24: GFLOPs: 0.4601. Time: 13.0873 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #25: GFLOPs: 0.3981. Time: 15.1263 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #26: GFLOPs: 0.1650. Time: 36.4996 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #27: GFLOPs: 0.2108. Time: 28.5677 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #28: GFLOPs: 0.1506. Time: 39.9906 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #29: GFLOPs: 0.3193. Time: 18.8555 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #30: GFLOPs: 0.5474. Time: 10.9988 ms. Best GFLOPs: 0.8778
[03:19:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"] Trial #31: GFLOPs: 0.6186. Time: 9.7327 ms. Best GFLOPs: 0.8778
[03:19:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 548
Total latency (us): 128294

[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #0: GFLOPs: 1.6903. Time: 22.8568 ms. Best GFLOPs: 1.6903
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #1: GFLOPs: 0.7428. Time: 52.0150 ms. Best GFLOPs: 1.6903
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #2: GFLOPs: 3.3577. Time: 11.5067 ms. Best GFLOPs: 3.3577
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #3: GFLOPs: 2.3501. Time: 16.4398 ms. Best GFLOPs: 3.3577
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused_fused in T.parallel(8):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 2):
                for i1_2_init, i2_2_init, i3_2_init, i2_3_init in T.grid(4, 7, 28, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused_fused // 4 * 8 + i1_1 * 4 + i1_2_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused_fused % 4 // 2 * 14 + i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(28, i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(96, 1, 1, 1, 4, 7, 28, 1, 4, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused_fused // 4 * 8 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused_fused % 4 // 2 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused_fused % 2 * 2 + i4_1)
                        ic = T.axis.reduce(384, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 14, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused_fused // 4 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused_fused % 4 // 2 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 28, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l95)
sch.parallel(loop=l101)
l102 = sch.fuse(l100)
sch.vectorize(loop=l102)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #5: GFLOPs: 2.3475. Time: 16.4579 ms. Best GFLOPs: 3.3577
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #6: GFLOPs: 3.2203. Time: 11.9976 ms. Best GFLOPs: 3.3577
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #7: GFLOPs: 1.9127. Time: 20.1997 ms. Best GFLOPs: 3.3577
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #8: GFLOPs: 1.3479. Time: 28.6629 ms. Best GFLOPs: 3.3577
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i2_3_init in T.grid(14, 28, 2):
                for i3_3_i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8)
                        oh = T.axis.spatial(28, i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(28, i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(64, 1, 1, 1, 1, 14, 28, 1, 6, 1, 1, 1, 1, 2):
                for i3_3_i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8)
                        oh = T.axis.spatial(28, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i3_3_i4_3_fused)
                        ic = T.axis.reduce(384, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 8, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 14, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 28, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l91, l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b102)
b119 = sch.decompose_reduction(block=b102, loop=l104)
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #10: GFLOPs: 2.0700. Time: 18.6643 ms. Best GFLOPs: 3.3577
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #11: GFLOPs: 2.1470. Time: 17.9954 ms. Best GFLOPs: 3.3577
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #12: GFLOPs: 3.7566. Time: 10.2847 ms. Best GFLOPs: 3.7566
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #13: GFLOPs: 2.8283. Time: 13.6602 ms. Best GFLOPs: 3.7566
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #14: GFLOPs: 3.1055. Time: 12.4409 ms. Best GFLOPs: 3.7566
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 7, 1):
                for i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(28, 2, 8, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_3_init)
                        oh = T.axis.spatial(28, i2_2_init)
                        ow = T.axis.spatial(28, i3_1 * 4 + i3_2_init * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(24, 1, 1, 1, 1, 28, 2, 1, 16, 1, 1, 1, 8, 1, 2, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(28, i2_2)
                        ow = T.axis.spatial(28, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused)
                        ic = T.axis.reduce(384, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 28, 28, 1):
                with T.block("T_add_1"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSS", [ax1, ax2, ax3, i0_0_i1_0_i2_0_i3_0_i4_0_fused])
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 28, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[24, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b101)
b124 = sch.decompose_reduction(block=b101, loop=l108)
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #16: GFLOPs: 1.6801. Time: 22.9965 ms. Best GFLOPs: 3.7566
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #17: GFLOPs: 4.6976. Time: 8.2245 ms. Best GFLOPs: 4.6976
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i4_2_init in T.grid(16, 14, 7, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_2_init)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + i2_2_init)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 7 + i3_2_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 16, 14, 7, 2, 24, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_2)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + i2_2)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(384, i5_0 * 24 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 28)
                        ax2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 24])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #19: GFLOPs: 1.8026. Time: 21.4334 ms. Best GFLOPs: 4.6976
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #20: GFLOPs: 2.7964. Time: 13.8161 ms. Best GFLOPs: 4.6976
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #21: GFLOPs: 2.6818. Time: 14.4068 ms. Best GFLOPs: 4.6976
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #22: GFLOPs: 0.6606. Time: 58.4850 ms. Best GFLOPs: 4.6976
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(16, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 16, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                    for i3_2_init, i4_2_init, i2_3_init in T.grid(14, 2, 4):
                        for i3_3_i4_3_fused_init in T.vectorized(2):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 14)
                                oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i2_3_init)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_2_init)
                                oc_block = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(6, 1, 1, 1, 1, 1, 14, 2, 64, 1, 1, 1, 1, 4):
                        for i3_3_i4_3_fused in T.vectorized(2):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 14)
                                oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i2_3)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_2)
                                oc_block = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                                ic = T.axis.reduce(384, i5_0 * 64 + i5_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [16, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 1, 4):
                    for ax3_ax4_fused in T.vectorized(56):
                        with T.block("T_add_1"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 14)
                            ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + ax2)
                            ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71)
sch.parallel(loop=l94)
l95 = sch.fuse(l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b67)
l103 = sch.fuse(l101, l102)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b104)
b127 = sch.decompose_reduction(block=b104, loop=l112)
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #24: GFLOPs: 1.8435. Time: 20.9579 ms. Best GFLOPs: 4.6976
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #25: GFLOPs: 3.4507. Time: 11.1965 ms. Best GFLOPs: 4.6976
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #26: GFLOPs: 2.8625. Time: 13.4970 ms. Best GFLOPs: 4.6976
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #27: GFLOPs: 5.4349. Time: 7.1088 ms. Best GFLOPs: 5.4349
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #28: GFLOPs: 2.7647. Time: 13.9746 ms. Best GFLOPs: 5.4349
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #29: GFLOPs: 4.3166. Time: 8.9505 ms. Best GFLOPs: 5.4349
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #30: GFLOPs: 2.0876. Time: 18.5071 ms. Best GFLOPs: 5.4349
[03:19:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #31: GFLOPs: 4.4525. Time: 8.6772 ms. Best GFLOPs: 5.4349
[03:20:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 580
Total latency (us): 149620

[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #0: GFLOPs: 2.9092. Time: 13.4528 ms. Best GFLOPs: 2.9092
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #1: GFLOPs: 1.1514. Time: 33.9905 ms. Best GFLOPs: 2.9092
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #2: GFLOPs: 4.3487. Time: 8.9998 ms. Best GFLOPs: 4.3487
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #3: GFLOPs: 0.2767. Time: 141.4511 ms. Best GFLOPs: 4.3487
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #4: GFLOPs: 4.5355. Time: 8.6291 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #5: GFLOPs: 4.4458. Time: 8.8033 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #6: GFLOPs: 1.3986. Time: 27.9840 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #7: GFLOPs: 2.5262. Time: 15.4923 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #8: GFLOPs: 1.5428. Time: 25.3677 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #9: GFLOPs: 2.5391. Time: 15.4140 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #10: GFLOPs: 3.7792. Time: 10.3559 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #11: GFLOPs: 0.8713. Time: 44.9174 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #12: GFLOPs: 1.0125. Time: 38.6545 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #13: GFLOPs: 3.7934. Time: 10.3173 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #14: GFLOPs: 1.3369. Time: 29.2740 ms. Best GFLOPs: 4.5355
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #15: GFLOPs: 4.8930. Time: 7.9987 ms. Best GFLOPs: 4.8930
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #16: GFLOPs: 2.8790. Time: 13.5940 ms. Best GFLOPs: 4.8930
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #17: GFLOPs: 7.0789. Time: 5.5288 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #18: GFLOPs: 3.7197. Time: 10.5217 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #19: GFLOPs: 3.3953. Time: 11.5268 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #20: GFLOPs: 1.4313. Time: 27.3435 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #21: GFLOPs: 1.4806. Time: 26.4340 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #22: GFLOPs: 1.1960. Time: 32.7227 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #23: GFLOPs: 3.7136. Time: 10.5388 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #24: GFLOPs: 2.5570. Time: 15.3058 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #25: GFLOPs: 6.0659. Time: 6.4520 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(1344, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(2, 7, 1):
                for i2_2_init, i1_3_init, i3_3_init in T.grid(2, 2, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 448 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 * 2 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 448 // 64 * 4 + i2_1 * 2 + i2_2_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 32 * 14 + i3_1 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 // 16 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 28, 28, 4], "float32"], ["TENSOR", [96, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(8, 1, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 2, 1, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 448 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 * 2 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 448 // 64 * 4 + i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 32 * 14 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 // 16 * 2 + i4_3_fused)
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 28, 28, 4], "float32"], ["TENSOR", [96, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 448 * 32 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 16 * 2 + ax1)
                            ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 448 // 64 * 4 + i2_1 * 2 + ax2)
                            ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 32 * 14 + i3_1 * 2 + ax3)
                            ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 32 // 16 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 16, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b67)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b106)
b127 = sch.decompose_reduction(block=b106, loop=l111)
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #27: GFLOPs: 2.5224. Time: 15.5160 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #28: GFLOPs: 3.3748. Time: 11.5968 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #29: GFLOPs: 2.5036. Time: 15.6325 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #30: GFLOPs: 1.2254. Time: 31.9391 ms. Best GFLOPs: 7.0789
[03:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 96, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(8, 7, 12, 2, 14):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(96, i1_2_init * 12 + i1_3_init)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 14 + i2_2_init * 2 + i2_3_init)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 28, 28, 4], "float32"], ["TENSOR", [96, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 8, 7, 1, 1, 2, 1, 1, 1, 12, 2, 14, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(96, i1_2 * 12 + i1_3)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 14 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 28, 28, 4], "float32"], ["TENSOR", [96, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 96, 14, 14, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(96, ax1)
                    ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 14 + ax2)
                    ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 14 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 8, 12])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:20:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 612
Total latency (us): 171735

[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #0: GFLOPs: 0.3046. Time: 4.9413 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #1: GFLOPs: 0.2162. Time: 6.9618 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(2880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(30):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(96, i0_i1_i2_fused // 30)
                        i2 = T.axis.spatial(30, i0_i1_i2_fused % 30)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(84, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1_1, i4_1, i5_0 in T.grid(1, 2, 1, 1, 2, 1):
                for i1_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(4, 7, 4, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 32 + i1_1 * 16 + i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                        ow = T.axis.spatial(14, i3_2_init * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 4, 1, 7, 1, 3, 1, 1, 4, 1, 2, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 32 + i1_1 * 16 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                        ow = T.axis.spatial(14, i3_2 * 2 + i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 14):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 32 + ax1)
                        ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 2, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77)
sch.parallel(loop=l97)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b65)
l104 = sch.fuse(l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b105)
b126 = sch.decompose_reduction(block=b105, loop=l113)
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #3: GFLOPs: 0.1627. Time: 9.2514 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #4: GFLOPs: 0.1635. Time: 9.2075 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #5: GFLOPs: 0.1204. Time: 12.4982 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #6: GFLOPs: 0.2175. Time: 6.9219 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #7: GFLOPs: 0.1505. Time: 9.9990 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #8: GFLOPs: 0.2466. Time: 6.1041 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #9: GFLOPs: 0.2791. Time: 5.3934 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1 in T.grid(1, 2, 2):
                for ax0, ax1, ax2, ax3 in T.grid(1, 24, 15, 29):
                    for ax4_fused in T.vectorized(2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 48 + i1_1 * 24 + ax1)
                            i2 = T.axis.spatial(30, i2_1 * 14 + ax2)
                            i3 = T.axis.spatial(30, ax3)
                            i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1, i5_0 in T.grid(7, 1, 1):
                    for i1_2_init, i2_2_init, i1_3_init, i3_3_init in T.grid(4, 7, 6, 2):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 48 + i1_1 * 24 + i1_2_init * 6 + i1_3_init)
                                oh = T.axis.spatial(14, i2_1 * 7 + i2_2_init)
                                ow = T.axis.spatial(14, i3_1 * 2 + i3_3_init)
                                oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 4, 7, 1, 1, 3, 1, 1, 6, 1, 2):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 48 + i1_1 * 24 + i1_2 * 6 + i1_3)
                                oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                                ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                                oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 48, 14, 14):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 48 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 4, 6])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b63)
l79 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l79)
l80 = sch.fuse(l78)
sch.vectorize(loop=l80)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b64)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b65)
l108 = sch.fuse(l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b109)
b130 = sch.decompose_reduction(block=b109, loop=l117)
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #11: GFLOPs: 0.2308. Time: 6.5228 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #12: GFLOPs: 0.1992. Time: 7.5549 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #13: GFLOPs: 0.1302. Time: 11.5645 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #14: GFLOPs: 0.1419. Time: 10.6114 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #15: GFLOPs: 0.1357. Time: 11.0932 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #16: GFLOPs: 0.1561. Time: 9.6400 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #17: GFLOPs: 0.2555. Time: 5.8912 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #18: GFLOPs: 0.1777. Time: 8.4687 ms. Best GFLOPs: 0.3046
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #19: GFLOPs: 0.3212. Time: 4.6867 ms. Best GFLOPs: 0.3212
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #20: GFLOPs: 0.1641. Time: 9.1722 ms. Best GFLOPs: 0.3212
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(196, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                    for i1_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(8, 2, 4, 3, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 49 * 24 + i1_2_init * 3 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + i2_3_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_2_init)
                            oci = T.axis.spatial(4, i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0 in T.serial(3):
                        for ax0, ax1, ax2 in T.grid(1, 24, 3):
                            for ax3_ax4_fused in T.vectorized(20):
                                with T.block("PaddedInput"):
                                    i0 = T.axis.spatial(1, ax0)
                                    i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused % 196 // 49 * 24 + ax1)
                                    i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 4 + i5_0 + ax2)
                                    i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax3_ax4_fused // 4)
                                    i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                        for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 8, 1, 2, 4, 1, 1, 1, 3, 2, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 49 * 24 + i1_2 * 3 + i1_3)
                                oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + i2_3)
                                ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_2)
                                oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_0, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 24, 2):
                    for ax3_ax4_fused in T.vectorized(8):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 49 * 24 + ax1)
                            ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + ax2)
                            ax3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 8, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b63)
l82 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l82)
l83 = sch.fuse(l80, l81)
sch.vectorize(loop=l83)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b65)
l112 = sch.fuse(l110, l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b113)
b135 = sch.decompose_reduction(block=b113, loop=l121)
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(24, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 7, 4):
                for i1_2_init, i2_2_init, i3_3_init in T.grid(4, 14, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + i1_2_init)
                        oh = T.axis.spatial(14, i2_2_init)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 27, 5, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 24 * 4 + ax1)
                            i2 = T.axis.spatial(30, i5_0 + ax2)
                            i3 = T.axis.spatial(30, i3_1 * 4 + ax3)
                            i4 = T.axis.spatial(4, i4_1 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 14, 1, 1, 1, 3, 1, 1, 1, 2, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + i1_2)
                            oh = T.axis.spatial(14, i2_2)
                            ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 4, 14):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3 = T.axis.spatial(14, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[24, 1, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b63)
l82 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l82)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b65)
l109 = sch.fuse(l107, l108)
sch.vectorize(loop=l109)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b110 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b110)
b131 = sch.decompose_reduction(block=b110, loop=l117)
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(2880, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(30):
                for i4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(96, i0_i1_i2_fused // 30)
                        i2 = T.axis.spatial(30, i0_i1_i2_fused % 30)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3_1, i4])
                        PaddedInput[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(7, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1_1, i4_1, i5_0 in T.grid(1, 8, 2, 1, 1, 1):
                for i1_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(3, 2, 4, 4, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i1_1 * 12 + i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_3_init)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i3_2_init)
                        oci = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 3, 1, 2, 4, 3, 1, 1, 4, 7, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(96, i1_1 * 12 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i3_2)
                        oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 96, 14):
                for ax3_ax4_fused in T.vectorized(8):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 3, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l71)
l72 = sch.fuse(l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77)
sch.parallel(loop=l97)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b65)
l104 = sch.fuse(l102, l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b105)
b126 = sch.decompose_reduction(block=b105, loop=l113)
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #24: GFLOPs: 0.2851. Time: 5.2793 ms. Best GFLOPs: 0.3212
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #25: GFLOPs: 0.3484. Time: 4.3206 ms. Best GFLOPs: 0.3484
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #26: GFLOPs: 0.0516. Time: 29.1962 ms. Best GFLOPs: 0.3484
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #27: GFLOPs: 0.2185. Time: 6.8901 ms. Best GFLOPs: 0.3484
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #28: GFLOPs: 0.2509. Time: 6.0000 ms. Best GFLOPs: 0.3484
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 24, 15, 29):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 24 + ax1)
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 8 // 4 * 14 + ax2)
                        i3 = T.axis.spatial(30, ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 14, 1):
                for i1_3_init, i2_3_init in T.grid(24, 7):
                    for i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 24 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 4 * 7 + i2_3_init)
                            ow, oci = T.axis.remap("SS", [i3_2, i3_3_i4_3_fused_init])
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 3, 1, 24, 7):
                    for i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 24 + i1_3)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 4 * 7 + i2_3)
                            ow, oci, kh, kw = T.axis.remap("SSRR", [i3_2, i3_3_i4_3_fused, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 96, 28, 28, 4], "float32"], ["TENSOR", [96, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(1344, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(96, i0_i1_i2_fused // 14)
                    ax2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    ax3 = T.axis.spatial(14, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 24])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b62)
l77 = sch.fuse(l65, l66, l67, l68, l69, l70, l71)
sch.parallel(loop=l77)
l78 = sch.fuse(l76)
sch.vectorize(loop=l78)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
l97 = sch.fuse(l95, l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b64)
l103 = sch.fuse(l98, l99, l100)
sch.parallel(loop=l103)
l104 = sch.fuse(l101, l102)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b105)
b123 = sch.decompose_reduction(block=b105, loop=l117)
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #30: GFLOPs: 0.6022. Time: 2.4997 ms. Best GFLOPs: 0.6022
[03:20:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"] Trial #31: GFLOPs: 0.1567. Time: 9.6076 ms. Best GFLOPs: 0.6022
[03:20:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 644
Total latency (us): 174235

[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #0: GFLOPs: 1.3851. Time: 10.4469 ms. Best GFLOPs: 1.3851
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #1: GFLOPs: 0.8441. Time: 17.1420 ms. Best GFLOPs: 1.3851
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #2: GFLOPs: 1.7607. Time: 8.2179 ms. Best GFLOPs: 1.7607
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #3: GFLOPs: 1.7108. Time: 8.4576 ms. Best GFLOPs: 1.7607
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #4: GFLOPs: 0.6578. Time: 21.9980 ms. Best GFLOPs: 1.7607
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #5: GFLOPs: 2.0873. Time: 6.9322 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #6: GFLOPs: 0.8269. Time: 17.4977 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #7: GFLOPs: 1.2661. Time: 11.4280 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #8: GFLOPs: 1.6164. Time: 8.9516 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 1, 1):
                for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(2, 2, 4, 6, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 12 + i1_2_init * 6 + i1_3_init)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_2_init)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 7 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 2, 2, 1, 4, 32, 1, 1, 1, 6, 1, 7, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 12 + i1_2 * 6 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(384, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 12, 14):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 12 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 7 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 2, 6])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[12, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l98, l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b101)
b124 = sch.decompose_reduction(block=b101, loop=l108)
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #10: GFLOPs: 1.3657. Time: 10.5952 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i3_3_init in T.grid(12, 2, 2, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 12 + i1_2_init)
                    oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 2 + i2_2_init)
                    ow = T.axis.spatial(14, i3_2_init * 7 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 12, 2, 2, 1, 48, 1, 1, 1, 1, 1, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 12 + i1_2)
                    oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 2 + i2_2)
                    ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(384, i5_0 * 48 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(336, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(24, i0_i1_i2_fused // 14)
                    ax2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    ax3 = T.axis.spatial(14, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 2, 12, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 48])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l96, l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #12: GFLOPs: 1.5255. Time: 9.4850 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #13: GFLOPs: 1.6380. Time: 8.8336 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #14: GFLOPs: 1.7189. Time: 8.4180 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #15: GFLOPs: 2.0218. Time: 7.1567 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i4_2_init, i1_3_init in T.grid(2, 7, 4, 12):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 12 + i1_3_init)
                    oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2 * 2 + i2_2_init)
                    ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 7 + i3_2_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 1, 2, 7, 4, 32, 1, 1, 1, 12, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 12 + i1_3)
                    oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2 * 2 + i2_2)
                    ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(384, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(336, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(24, i0_i1_i2_fused // 14)
                    ax2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    ax3 = T.axis.spatial(14, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 12])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[12, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l96, l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #17: GFLOPs: 1.3103. Time: 11.0430 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #18: GFLOPs: 1.1934. Time: 12.1247 ms. Best GFLOPs: 2.0873
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #19: GFLOPs: 2.1207. Time: 6.8231 ms. Best GFLOPs: 2.1207
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(168, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(2):
                for i1_2_init, i4_2_init, i2_3_init in T.grid(4, 2, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 56 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 4 + i1_2_init)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i2_3_init)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 28 * 7 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(6, 1, 1, 1, 4, 1, 1, 2, 64, 1, 1, 1, 1, 7, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 56 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 4 + i1_2)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i2_3)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 28 * 7 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(384, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 4, 7):
                    for ax3_ax4_fused in T.vectorized(2):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 56 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 4 + ax1)
                            ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + ax2)
                            ax3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 28 * 7 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[3, 2, 4, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[6, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b102)
b121 = sch.decompose_reduction(block=b102, loop=l105)
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #21: GFLOPs: 0.9949. Time: 14.5431 ms. Best GFLOPs: 2.1207
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #22: GFLOPs: 0.6704. Time: 21.5832 ms. Best GFLOPs: 2.1207
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #23: GFLOPs: 2.5459. Time: 5.6834 ms. Best GFLOPs: 2.5459
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #24: GFLOPs: 1.9661. Time: 7.3593 ms. Best GFLOPs: 2.5459
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #25: GFLOPs: 2.3064. Time: 6.2735 ms. Best GFLOPs: 2.5459
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #26: GFLOPs: 1.8424. Time: 7.8536 ms. Best GFLOPs: 2.5459
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #27: GFLOPs: 0.4830. Time: 29.9560 ms. Best GFLOPs: 2.5459
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #28: GFLOPs: 1.4142. Time: 10.2317 ms. Best GFLOPs: 2.5459
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(588, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 1, 1, 1, 1):
                for i4_2_init, i1_3_init, i2_3_init in T.grid(2, 4, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 98 * 4 + i1_3_init)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 2 + i2_3_init)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 1, 1, 1, 2, 32, 1, 1, 1, 4, 2, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 98 * 4 + i1_3)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 2 + i2_3)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(384, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 96, 14, 14, 4], "float32"], ["TENSOR", [24, 96, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 4, 2):
                    for ax3_ax4_fused in T.vectorized(2):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 98 * 4 + ax1)
                            ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 2 + ax2)
                            ax3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                            ax4 = T.axis.spatial(4, i4_0 * 2 + ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[6, 1, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[12, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b66)
l106 = sch.fuse(l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b107)
b131 = sch.decompose_reduction(block=b107, loop=l115)
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #30: GFLOPs: 0.9046. Time: 15.9951 ms. Best GFLOPs: 2.5459
[03:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #31: GFLOPs: 2.4117. Time: 5.9996 ms. Best GFLOPs: 2.5459
[03:21:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 676
Total latency (us): 179918

[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 36, 3, 16):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 112 // 28 * 36 + ax1)
                        i2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 + ax2)
                        i3 = T.axis.spatial(16, ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 12, 1, 1, 1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i3_3_init in T.grid(3, 14):
                        for i4_3_fused_init in T.vectorized(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 36 + i1_1 * 3 + i1_2_init)
                                oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                                ow = T.axis.spatial(14, i3_3_init)
                                oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 14):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 36 + i1_1 * 3 + i1_2)
                                oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                                ow = T.axis.spatial(14, i3_3)
                                oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                                kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 14):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 36 + i1_1 * 3 + ax1)
                            ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                            ax3_1 = T.axis.spatial(14, ax3)
                            ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 12, 3, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b63)
l76 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l76)
l77 = sch.fuse(l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b65)
l110 = sch.fuse(l109)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b111)
b132 = sch.decompose_reduction(block=b111, loop=l119)
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #1: GFLOPs: 0.4675. Time: 4.8294 ms. Best GFLOPs: 0.4675
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #2: GFLOPs: 0.1883. Time: 11.9924 ms. Best GFLOPs: 0.4675
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(84, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1 in T.grid(1, 2):
                for ax0, ax1, ax2 in T.grid(1, 6, 4):
                    for ax3_ax4_fused in T.vectorized(64):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 84 // 7 * 12 + i1_1 * 6 + ax1)
                            i2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 2 + ax2)
                            i3 = T.axis.spatial(16, ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 4, 1, 1, 1, 6, 2, 7, 1):
                    for i3_3_init in T.serial(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 12 + i1_1 * 6 + i1_2)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 2 + i2_2)
                            ow = T.axis.spatial(14, i3_2 * 2 + i3_3_init)
                            oci = T.axis.spatial(4, i4_1)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 2, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 12 + i1_1 * 6 + i1_2)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 2 + i2_2)
                            ow = T.axis.spatial(14, i3_2 * 2 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 12, 2):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 12 + ax1)
                        ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 2 + ax2)
                        ax3 = T.axis.spatial(14, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[12, 2, 6, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b63)
l78 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l78)
l79 = sch.fuse(l76, l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b107)
b128 = sch.decompose_reduction(block=b107, loop=l121)
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #4: GFLOPs: 0.1737. Time: 12.9993 ms. Best GFLOPs: 0.4675
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #5: GFLOPs: 0.4234. Time: 5.3324 ms. Best GFLOPs: 0.4675
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #6: GFLOPs: 0.2589. Time: 8.7206 ms. Best GFLOPs: 0.4675
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #7: GFLOPs: 0.3279. Time: 6.8864 ms. Best GFLOPs: 0.4675
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #8: GFLOPs: 0.1656. Time: 13.6355 ms. Best GFLOPs: 0.4675
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #9: GFLOPs: 0.2555. Time: 8.8375 ms. Best GFLOPs: 0.4675
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 72, 16, 3):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 28 * 72 + ax1)
                        i2 = T.axis.spatial(16, ax2)
                        i3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 24, 1, 1, 1):
                for i1_3_init, i2_3_init in T.grid(3, 14):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 72 + i1_2 * 3 + i1_3_init)
                        oh = T.axis.spatial(14, i2_3_init)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 3, 14, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 72 + i1_2 * 3 + i1_3)
                        oh = T.axis.spatial(14, i2_3)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_fused in T.parallel(144, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(14):
                for i3_i4_fused in T.vectorized(56):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(14, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 24, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b62)
l75 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98)
sch.parallel(loop=l102)
l103 = sch.fuse(l100, l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b104)
b125 = sch.decompose_reduction(block=b104, loop=l118)
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #11: GFLOPs: 0.3495. Time: 6.4597 ms. Best GFLOPs: 0.4675
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #12: GFLOPs: 0.1060. Time: 21.2975 ms. Best GFLOPs: 0.4675
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #13: GFLOPs: 0.5031. Time: 4.4883 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_i1_fused in T.parallel(144, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(16):
                for i3_i4_fused in T.vectorized(64):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(16, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[i0, i1, i2_1 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2_1, i3, i4])
                        PaddedInput[i0, i1, i2_1, i3, i4] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2_1 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(147, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_0 in T.serial(1):
                for i0_1, i1_1, i2_1_1, i3_1, i4_1, i5_0 in T.grid(1, 6, 2, 1, 1, 1):
                    for i3_2_init, i1_3_init in T.grid(2, 8):
                        for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + i1_1 * 8 + i1_3_init)
                                oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + i2_1_1)
                                ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_2_init)
                                oci = T.axis.spatial(4, i2_3_i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 1, 1, 1, 2, 1, 3, 1, 1, 8):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + i1_1 * 8 + i1_3)
                                oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + i2_1_1)
                                ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_2)
                                oci, kh, kw = T.axis.remap("SRR", [i2_3_i3_3_i4_3_fused, i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 48, 2):
                    for ax3_ax4_fused in T.vectorized(8):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + ax1)
                            ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + ax2)
                            ax3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 6, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67)
sch.parallel(loop=l71)
l72 = sch.fuse(l69, l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76)
sch.parallel(loop=l97)
l98 = sch.fuse(l94, l95, l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b107)
b127 = sch.decompose_reduction(block=b107, loop=l116)
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #15: GFLOPs: 0.2533. Time: 8.9128 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #16: GFLOPs: 0.3972. Time: 5.6847 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1 in T.grid(1, 12, 14):
                for ax0, ax1, ax2 in T.grid(1, 3, 3):
                    for ax3_ax4_fused in T.vectorized(64):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 * 36 + i1_1 * 3 + ax1)
                            i2 = T.axis.spatial(16, i2_1 + ax2)
                            i3 = T.axis.spatial(16, ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1 in T.grid(14, 1):
                    for i1_2_init in T.serial(3):
                        for i2_2_i3_2_i4_2_i5_1_i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 36 + i1_1 * 3 + i1_2_init)
                                oh, ow, oci = T.axis.remap("SSS", [i2_1, i3_1, i2_2_i3_2_i4_2_i5_1_i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused_init])
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2 in T.grid(3, 3, 1, 3):
                        for i2_2_i3_2_i4_2_i5_1_i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 36 + i1_1 * 3 + i1_2)
                                oh, ow, oci, kh, kw = T.axis.remap("SSSRR", [i2_1, i3_1, i2_2_i3_2_i4_2_i5_1_i6_1_i0_3_i1_3_i2_3_i3_3_i4_3_fused, i5_0, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 36, 14):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 36 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3 = T.axis.spatial(14, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 12, 3, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b63)
l79 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l79)
l80 = sch.fuse(l77, l78)
sch.vectorize(loop=l80)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b64)
l101 = sch.fuse(l91, l92, l93, l94, l95, l96, l97, l98, l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b65)
l108 = sch.fuse(l106, l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b109)
b121 = sch.decompose_reduction(block=b109, loop=l116)
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #18: GFLOPs: 0.2202. Time: 10.2542 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #19: GFLOPs: 0.1048. Time: 21.5435 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #20: GFLOPs: 0.0693. Time: 32.5693 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_i1_fused in T.parallel(144, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(16):
                for i3_i4_fused in T.vectorized(64):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(16, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[i0, i1, i2_1 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2_1, i3, i4])
                        PaddedInput[i0, i1, i2_1, i3, i4] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2_1 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1_1, i3_1, i4_1, i5_0 in T.grid(1, 1, 1, 2, 1, 1):
                for i1_2_init, i1_3_init, i3_3_init in T.grid(9, 4, 7):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 36 + i1_2_init * 4 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                            ow = T.axis.spatial(14, i3_1 * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 9, 1, 1, 1, 3, 1, 1, 4, 1, 7):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 36 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                            ow = T.axis.spatial(14, i3_1 * 7 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 36, 1, 14):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 36 + ax1)
                        ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 9, 4])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67)
sch.parallel(loop=l71)
l72 = sch.fuse(l69, l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77)
sch.parallel(loop=l97)
l98 = sch.fuse(l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b106)
b127 = sch.decompose_reduction(block=b106, loop=l114)
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #22: GFLOPs: 0.2689. Time: 8.3976 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #23: GFLOPs: 0.4237. Time: 5.3294 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(42, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_0, i4_0, i0_1, i1_1 in T.grid(1, 1, 1, 8):
                for ax0, ax1, ax2 in T.grid(1, 6, 3):
                    for ax3_ax4_fused in T.vectorized(64):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(144, i0_0_i1_0_i2_0_fused % 42 // 14 * 48 + i1_1 * 6 + ax1)
                            i2 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused % 14 + ax2)
                            i3 = T.axis.spatial(16, ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 1, 4):
                    for i1_2_init, i1_3_init, i3_3_init in T.grid(3, 2, 14):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(144, i0_0_i1_0_i2_0_fused // 14 * 48 + i1_1 * 6 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_fused % 14)
                            ow, oci = T.axis.remap("SS", [i3_3_init, i4_1])
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 3, 1, 1, 1, 1, 3, 1, 2, 1, 14, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(144, i0_0_i1_0_i2_0_fused // 14 * 48 + i1_1 * 6 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_fused % 14)
                            ow, oci, kh, kw = T.axis.remap("SSRR", [i3_3, i4_1, i5_0, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 1, 14, 1):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_fused // 14 * 48 + i1_1 * 6 + ax1)
                            ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_fused % 14)
                            ax3_1, ax4_1 = T.axis.remap("SS", [ax3, i4_1])
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 8, 3, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b63)
l78 = sch.fuse(l66, l67, l68)
sch.parallel(loop=l78)
l79 = sch.fuse(l76, l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b115 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137 = sch.get_loops(block=b115)
b138 = sch.decompose_reduction(block=b115, loop=l124)
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #25: GFLOPs: 0.0584. Time: 38.6739 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #26: GFLOPs: 0.2823. Time: 7.9994 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #27: GFLOPs: 0.2769. Time: 8.1544 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #28: GFLOPs: 0.1839. Time: 12.2811 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #29: GFLOPs: 0.1152. Time: 19.5976 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #30: GFLOPs: 0.1004. Time: 22.4842 ms. Best GFLOPs: 0.5031
[03:21:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"] Trial #31: GFLOPs: 0.1328. Time: 16.9980 ms. Best GFLOPs: 0.5031
[03:21:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 708
Total latency (us): 188895

[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #0: GFLOPs: 1.5406. Time: 14.0942 ms. Best GFLOPs: 1.5406
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #1: GFLOPs: 1.7451. Time: 12.4426 ms. Best GFLOPs: 1.7451
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #2: GFLOPs: 1.3281. Time: 16.3497 ms. Best GFLOPs: 1.7451
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #3: GFLOPs: 3.2574. Time: 6.6660 ms. Best GFLOPs: 3.2574
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #4: GFLOPs: 2.6323. Time: 8.2490 ms. Best GFLOPs: 3.2574
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #5: GFLOPs: 1.2534. Time: 17.3237 ms. Best GFLOPs: 3.2574
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 24, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(84, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 14, 2, 2):
                for i1_3_init in T.serial(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 4 + i1_3_init)
                        oh = T.axis.spatial(14, i2_1)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 1, 1, 1, 1, 18, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_1)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        ic = T.axis.reduce(576, i5_0 * 18 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 2):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 4 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 2 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[6, 1, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 18])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #7: GFLOPs: 3.4649. Time: 6.2668 ms. Best GFLOPs: 3.4649
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #8: GFLOPs: 2.9356. Time: 7.3966 ms. Best GFLOPs: 3.4649
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #9: GFLOPs: 0.9321. Time: 23.2950 ms. Best GFLOPs: 3.4649
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #10: GFLOPs: 1.6459. Time: 13.1923 ms. Best GFLOPs: 3.4649
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #11: GFLOPs: 2.8653. Time: 7.5781 ms. Best GFLOPs: 3.4649
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 24, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(84, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_3_init, i2_3_init, i3_3_init in T.grid(4, 2, 14):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 42 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 3 * 4 + i1_3_init)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 42 // 6 * 2 + i2_3_init)
                        ow = T.axis.spatial(14, i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 6 // 3 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(48, 1, 1, 1, 1, 1, 1, 1, 12, 1, 1, 1, 4, 2, 14):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 42 * 12 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 3 * 4 + i1_3)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 42 // 6 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 6 // 3 * 2 + i4_3_fused)
                        ic = T.axis.reduce(576, i5_0 * 12 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(336, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("T_add_1"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(24, i0_i1_i2_fused // 14)
                    ax2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    ax3 = T.axis.spatial(14, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 3, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[48, 12])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l98, l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #13: GFLOPs: 2.6202. Time: 8.2871 ms. Best GFLOPs: 3.4649
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #14: GFLOPs: 0.5340. Time: 40.6652 ms. Best GFLOPs: 3.4649
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #15: GFLOPs: 1.3271. Time: 16.3615 ms. Best GFLOPs: 3.4649
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #16: GFLOPs: 3.6112. Time: 6.0128 ms. Best GFLOPs: 3.6112
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 24, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i1_3_init in T.grid(3, 7, 7, 4):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 12 + i1_2_init * 4 + i1_3_init)
                    oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 7 + i2_2_init)
                    ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 7 + i3_2_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 3, 7, 7, 1, 9, 1, 1, 1, 4, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 12 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 7 + i2_2)
                    ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    ic = T.axis.reduce(576, i5_0 * 9 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 7, 7, 1):
                with T.block("T_add_1"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 12 + ax1)
                    ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 7 + ax2)
                    ax3_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 7 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 3, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 9])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #18: GFLOPs: 0.9526. Time: 22.7935 ms. Best GFLOPs: 3.6112
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #19: GFLOPs: 2.8080. Time: 7.7328 ms. Best GFLOPs: 3.6112
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #20: GFLOPs: 1.7564. Time: 12.3623 ms. Best GFLOPs: 3.6112
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #21: GFLOPs: 2.2610. Time: 9.6034 ms. Best GFLOPs: 3.6112
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #22: GFLOPs: 4.3908. Time: 4.9452 ms. Best GFLOPs: 4.3908
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #23: GFLOPs: 1.2880. Time: 16.8588 ms. Best GFLOPs: 4.3908
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #24: GFLOPs: 2.2073. Time: 9.8373 ms. Best GFLOPs: 4.3908
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #25: GFLOPs: 4.0223. Time: 5.3983 ms. Best GFLOPs: 4.3908
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 24, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 24, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(784, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(2):
                for i1_2_init, i2_2_init, i3_2_init in T.grid(3, 2, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 98 * 3 + i1_2_init)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 98 // 14 * 2 + i2_2_init)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 3, 2, 2, 1, 48, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 98 * 3 + i1_2)
                        oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 98 // 14 * 2 + i2_2)
                        ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 2 + i4_1)
                        ic = T.axis.reduce(576, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [24, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 3, 2, 2, 1):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 98 * 3 + ax1)
                        ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 98 // 14 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 2 + i4_1)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 3, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[12, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b102)
b121 = sch.decompose_reduction(block=b102, loop=l105)
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #27: GFLOPs: 3.5631. Time: 6.0940 ms. Best GFLOPs: 4.3908
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #28: GFLOPs: 2.5489. Time: 8.5188 ms. Best GFLOPs: 4.3908
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #29: GFLOPs: 0.8194. Time: 26.4989 ms. Best GFLOPs: 4.3908
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #30: GFLOPs: 1.4588. Time: 14.8848 ms. Best GFLOPs: 4.3908
[03:22:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #31: GFLOPs: 1.3263. Time: 16.3712 ms. Best GFLOPs: 4.3908
[03:22:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 740
Total latency (us): 198786

[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #0: GFLOPs: 2.1713. Time: 10.0870 ms. Best GFLOPs: 2.1713
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #1: GFLOPs: 1.1155. Time: 19.6336 ms. Best GFLOPs: 2.1713
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #2: GFLOPs: 1.9930. Time: 10.9896 ms. Best GFLOPs: 2.1713
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #3: GFLOPs: 0.4056. Time: 53.9981 ms. Best GFLOPs: 2.1713
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #4: GFLOPs: 1.2958. Time: 16.9024 ms. Best GFLOPs: 2.1713
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #5: GFLOPs: 2.1630. Time: 10.1256 ms. Best GFLOPs: 2.1713
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(18, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(14, 2, 2, 16, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 9 * 16 + i1_3_init)
                    oh = T.axis.spatial(14, i2_2_init)
                    ow = T.axis.spatial(14, i3_2_init * 7 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 9 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 14, 2, 2, 48, 1, 1, 1, 16, 1, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 9 * 16 + i1_3)
                    oh = T.axis.spatial(14, i2_2)
                    ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 9 * 2 + i4_2)
                    ic = T.axis.reduce(96, i5_0 * 48 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(144, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(14):
                for i3_i4_fused in T.vectorized(56):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(14, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 9, 1, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 48])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #7: GFLOPs: 2.7388. Time: 7.9969 ms. Best GFLOPs: 2.7388
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #8: GFLOPs: 1.1865. Time: 18.4586 ms. Best GFLOPs: 2.7388
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #9: GFLOPs: 2.8213. Time: 7.7631 ms. Best GFLOPs: 2.8213
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #10: GFLOPs: 1.1840. Time: 18.4983 ms. Best GFLOPs: 2.8213
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #11: GFLOPs: 1.2652. Time: 17.3114 ms. Best GFLOPs: 2.8213
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #12: GFLOPs: 1.9915. Time: 10.9977 ms. Best GFLOPs: 2.8213
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #13: GFLOPs: 12.0904. Time: 1.8115 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #14: GFLOPs: 0.2282. Time: 95.9899 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(168, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(2, 1):
                for i1_2_init, i2_2_init, i1_3_init in T.grid(6, 7, 2):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 14 * 12 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 7 + i2_2_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 2 * 2 + i3_1)
                            oc_block = T.axis.spatial(4, i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(3, 1, 1, 1, 6, 7, 1, 1, 32, 1, 1, 1, 2):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 14 * 12 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 7 + i2_2)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 2 * 2 + i3_1)
                            oc_block = T.axis.spatial(4, i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(96, i5_0 * 32 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 12, 7):
                    for ax3_ax4_fused in T.vectorized(4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 14 * 12 + ax1)
                            ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 2 * 7 + ax2)
                            ax3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 2 * 2 + i3_1)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[12, 1, 6, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l94)
l95 = sch.fuse(l91, l92, l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b67)
l104 = sch.fuse(l102, l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b105)
b123 = sch.decompose_reduction(block=b105, loop=l109)
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #16: GFLOPs: 1.3691. Time: 15.9976 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #17: GFLOPs: 1.2814. Time: 17.0915 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #18: GFLOPs: 1.5446. Time: 14.1798 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(42, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 7, 1):
                for i1_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(3, 2, 4, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 24 + i1_1 * 12 + i1_2_init * 4 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 2 + i2_1)
                            ow = T.axis.spatial(14, i3_1 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(16, 1, 1, 1, 3, 1, 1, 2, 6, 1, 1, 1, 4, 1, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 24 + i1_1 * 12 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 2 + i2_1)
                            ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(96, i5_0 * 6 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 24, 2):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7 * 24 + ax1)
                        ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 * 2 + ax2)
                        ax3 = T.axis.spatial(14, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[6, 2, 3, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l100, l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #20: GFLOPs: 0.9780. Time: 22.3939 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #21: GFLOPs: 2.0722. Time: 10.5693 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #22: GFLOPs: 1.6429. Time: 13.3311 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #23: GFLOPs: 3.0162. Time: 7.2614 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #24: GFLOPs: 1.5058. Time: 14.5446 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 144, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(48, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 14, 1):
                for i1_2_init, i1_3_init, i2_3_init in T.grid(3, 4, 14):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 12 + i1_2_init * 4 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_3_init, i3_1])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(48, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 4, 14, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 12 + i1_2 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_1])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        ic = T.axis.reduce(96, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 24, 14, 14, 4], "float32"], ["TENSOR", [144, 24, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 14, 14, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 12 + ax1)
                    ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[12, 1, 3, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[48, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b101)
b124 = sch.decompose_reduction(block=b101, loop=l108)
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #26: GFLOPs: 0.6319. Time: 34.6598 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #27: GFLOPs: 1.2169. Time: 17.9987 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #28: GFLOPs: 2.1639. Time: 10.1215 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #29: GFLOPs: 1.2461. Time: 17.5764 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #30: GFLOPs: 0.8533. Time: 25.6666 ms. Best GFLOPs: 12.0904
[03:22:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"] Trial #31: GFLOPs: 1.8824. Time: 11.6348 ms. Best GFLOPs: 12.0904
[03:22:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 772
Total latency (us): 204220

[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #0: GFLOPs: 0.0501. Time: 11.2782 ms. Best GFLOPs: 0.0501
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #1: GFLOPs: 0.0376. Time: 14.9978 ms. Best GFLOPs: 0.0501
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #2: GFLOPs: 0.0992. Time: 5.6918 ms. Best GFLOPs: 0.0992
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #3: GFLOPs: 0.0410. Time: 13.7805 ms. Best GFLOPs: 0.0992
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #4: GFLOPs: 0.0096. Time: 58.6489 ms. Best GFLOPs: 0.0992
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #5: GFLOPs: 6.8149. Time: 0.0828 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #6: GFLOPs: 0.0460. Time: 12.2650 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #7: GFLOPs: 0.0728. Time: 7.7505 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #8: GFLOPs: 0.0750. Time: 7.5219 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 72, 15, 15):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 72 + ax1)
                        i2 = T.axis.spatial(16, ax2)
                        i3 = T.axis.spatial(16, ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 24, 7, 7, 1):
                for i4_2_init, i1_3_init in T.grid(2, 3):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 72 + i1_1 * 3 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 72 + i1_1 * 3 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 72, 7, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 72 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 24, 1, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b63)
l76 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l76)
l77 = sch.fuse(l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b65)
l104 = sch.fuse(l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b105)
b126 = sch.decompose_reduction(block=b105, loop=l112)
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #10: GFLOPs: 0.0378. Time: 14.9231 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #11: GFLOPs: 4.6612. Time: 0.1211 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #12: GFLOPs: 0.1214. Time: 4.6483 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(168, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 3, 15):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 168 // 28 * 24 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 12 + ax1)
                        i2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 4 * 2 + ax2)
                        i3 = T.axis.spatial(16, ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 // 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                for i1_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(2, 2, 6, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 28 * 24 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 12 + i1_2_init * 6 + i1_3_init)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 4)
                        ow = T.axis.spatial(7, i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 // 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 1, 1, 2, 1, 1, 1, 6, 1, 7, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 28 * 24 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2 * 12 + i1_2 * 6 + i1_3)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 28 // 4)
                        ow = T.axis.spatial(7, i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 // 2 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_fused in T.parallel(144, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 2, 2, 6])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b62)
l77 = sch.fuse(l65, l66, l67, l68, l69, l70, l71)
sch.parallel(loop=l77)
l78 = sch.fuse(l76)
sch.vectorize(loop=l78)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98)
sch.parallel(loop=l102)
l103 = sch.fuse(l100, l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b104)
b123 = sch.decompose_reduction(block=b104, loop=l109)
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #14: GFLOPs: 0.0524. Time: 10.7684 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0 in T.serial(1):
                for i2_2_init, i4_2_init, i1_3_init in T.grid(7, 2, 18):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 18 + i1_3_init)
                        oh = T.axis.spatial(7, i2_2_init)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 7, 1, 2, 3, 1, 1, 18, 1, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 18 + i1_3)
                        oh = T.axis.spatial(7, i2_2)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 15 and 1 <= ow * 2 + kw and ow * 2 + kw < 15, placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 18, 7):
                for ax3_ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 18 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax3_ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 18])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
l96 = sch.fuse(l94, l95)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b97)
b113 = sch.decompose_reduction(block=b97, loop=l100)
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #16: GFLOPs: 0.0560. Time: 10.0713 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #17: GFLOPs: 0.0532. Time: 10.6113 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #18: GFLOPs: 0.0483. Time: 11.6891 ms. Best GFLOPs: 6.8149
[03:22:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 6, 7, 7, 1):
                for i1_3_init in T.serial(12):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 72 + i1_2 * 12 + i1_3_init)
                            oh, ow, oci = T.axis.remap("SSS", [i2_2, i3_2, i2_3_i3_3_i4_3_fused_init])
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1 in T.grid(3, 3):
                    for ax0, ax1, ax2 in T.grid(1, 12, 1):
                        for ax3_ax4_fused in T.vectorized(4):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 72 + i1_2 * 12 + ax1)
                                i2 = T.axis.spatial(16, i2_2 * 2 + i5_1 + ax2)
                                i3 = T.axis.spatial(16, i3_2 * 2 + i6_1 + 0)
                                i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_3, i1_3 in T.grid(1, 12):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 72 + i1_2 * 12 + i1_3)
                                oh, ow, oci, kh, kw = T.axis.remap("SSSRR", [i2_2, i3_2, i2_3_i3_3_i4_3_fused, i5_1, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2 in T.grid(1, 72, 7):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 72 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 6, 12])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=18)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b63)
l90 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l90)
l91 = sch.fuse(l88, l89)
sch.vectorize(loop=l91)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b64)
l107 = sch.fuse(l104, l105, l106)
sch.vectorize(loop=l107)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b65)
l114 = sch.fuse(l112, l113)
sch.vectorize(loop=l114)
sch.annotate(block_or_loop=l108, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l108, ann_key="pragma_unroll_explicit", ann_val=1)
b115 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b115)
b129 = sch.decompose_reduction(block=b115, loop=l124)
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #20: GFLOPs: 0.0807. Time: 6.9990 ms. Best GFLOPs: 6.8149
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #21: GFLOPs: 0.1176. Time: 4.7993 ms. Best GFLOPs: 6.8149
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #22: GFLOPs: 0.1224. Time: 4.6101 ms. Best GFLOPs: 6.8149
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 144, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 36, 15):
                for ax3_ax4_fused in T.vectorized(60):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(144, i0_0_i1_0_fused * 36 + ax1)
                        i2 = T.axis.spatial(16, ax2)
                        i3 = T.axis.spatial(16, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 7, 1, 1, 6, 1, 1, 1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i1_3_init, i2_3_init in T.grid(3, 2, 7):
                        for i3_3_i4_3_fused_init in T.vectorized(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(144, i0_0_i1_0_fused * 36 + i1_1 * 6 + i1_2_init * 2 + i1_3_init)
                                oh, ow, oci = T.axis.remap("SSS", [i2_3_init, i3_0, i3_3_i4_3_fused_init])
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3 in T.grid(3, 1, 3, 1, 1, 1, 3, 1, 1, 2, 7):
                        for i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(144, i0_0_i1_0_fused * 36 + i1_1 * 6 + i1_2 * 2 + i1_3)
                                oh, ow, oci, kh, kw = T.axis.remap("SSSRR", [i2_3, i3_0, i3_3_i4_3_fused, i5_1, i6_0])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 144, 14, 14, 4], "float32"], ["TENSOR", [144, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 6, 7):
                    for ax3_ax4_fused in T.vectorized(4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(144, i0_0_i1_0_fused * 36 + i1_1 * 6 + ax1)
                            ax2_1, ax3, ax4 = T.axis.remap("SSS", [ax2, i3_0, ax3_ax4_fused])
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 6, 3, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b63)
l73 = sch.fuse(l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l71, l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l96, l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b65)
l113 = sch.fuse(l111, l112)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b114)
b137 = sch.decompose_reduction(block=b114, loop=l125)
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #24: GFLOPs: 0.0975. Time: 5.7879 ms. Best GFLOPs: 6.8149
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #25: GFLOPs: 0.0820. Time: 6.8858 ms. Best GFLOPs: 6.8149
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #26: GFLOPs: 0.0379. Time: 14.9066 ms. Best GFLOPs: 6.8149
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #27: GFLOPs: 0.1310. Time: 4.3092 ms. Best GFLOPs: 6.8149
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #28: GFLOPs: 0.1308. Time: 4.3159 ms. Best GFLOPs: 6.8149
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #29: GFLOPs: 0.0817. Time: 6.9094 ms. Best GFLOPs: 6.8149
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #30: GFLOPs: 0.0686. Time: 8.2285 ms. Best GFLOPs: 6.8149
[03:22:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"] Trial #31: GFLOPs: 0.0295. Time: 19.1407 ms. Best GFLOPs: 6.8149
[03:23:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 804
Total latency (us): 204303

[03:23:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(140, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                for i1_2_init, i4_2_init, i2_3_init in T.grid(2, 4, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 35 * 10 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 5 * 2 + i1_2_init)
                        oh = T.axis.spatial(7, i2_3_init)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 35 // 5)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(12, 1, 1, 1, 2, 1, 1, 4, 48, 1, 1, 1, 1, 7, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 35 * 10 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 5 * 2 + i1_2)
                        oh = T.axis.spatial(7, i2_3)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 35 // 5)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(576, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 2, 7):
                    for ax3_ax4_fused in T.vectorized(4):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(40, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 35 * 10 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 5 * 2 + ax1)
                            ax2_1 = T.axis.spatial(7, ax2)
                            ax3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 35 // 5)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 5, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[12, 48])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b66)
l103 = sch.fuse(l101, l102)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b104)
b125 = sch.decompose_reduction(block=b104, loop=l109)
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #1: GFLOPs: 0.2256. Time: 40.0738 ms. Best GFLOPs: 0.2256
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #2: GFLOPs: 1.4850. Time: 6.0874 ms. Best GFLOPs: 1.4850
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #3: GFLOPs: 0.7388. Time: 12.2352 ms. Best GFLOPs: 1.4850
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #4: GFLOPs: 1.8300. Time: 4.9397 ms. Best GFLOPs: 1.8300
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #5: GFLOPs: 2.3388. Time: 3.8651 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #6: GFLOPs: 1.5809. Time: 5.7180 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #7: GFLOPs: 1.8723. Time: 4.8279 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #8: GFLOPs: 1.8524. Time: 4.8798 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #9: GFLOPs: 1.1447. Time: 7.8969 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #10: GFLOPs: 1.4255. Time: 6.3411 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #11: GFLOPs: 0.1884. Time: 47.9805 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(7, 2, 40, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk, oh, ow = T.axis.remap("SSS", [i1_3_init, i2_3_init, i3_2_init])
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(36, 1, 1, 1, 1, 1, 7, 2, 16, 1, 1, 1, 40, 7, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk, oh, ow = T.axis.remap("SSS", [i1_3, i2_3, i3_2])
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 2 + i4_2)
                    ic = T.axis.reduce(576, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 40, 7, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1 = T.axis.remap("SSS", [ax1, ax2, ax3])
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 40])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[36, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #13: GFLOPs: 1.0110. Time: 8.9409 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #14: GFLOPs: 1.2137. Time: 7.4482 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #15: GFLOPs: 0.6950. Time: 13.0062 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #16: GFLOPs: 0.1111. Time: 81.3705 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #17: GFLOPs: 0.8693. Time: 10.3985 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(70, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_3_init in T.grid(8, 7):
                for i3_3_i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 5 * 8 + i1_2_init)
                        oh = T.axis.spatial(7, i2_3_init)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 10)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 10 // 5 * 2 + i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(12, 1, 1, 1, 8, 1, 1, 1, 48, 1, 1, 1, 1, 7):
                for i3_3_i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 5 * 8 + i1_2)
                        oh = T.axis.spatial(7, i2_3)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 10)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 10 // 5 * 2 + i3_3_i4_3_fused)
                        ic = T.axis.reduce(576, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(280, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(28):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(40, i0_i1_i2_fused // 7)
                    ax2 = T.axis.spatial(7, i0_i1_i2_fused % 7)
                    ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 5, 8, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[12, 48])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
l93 = sch.fuse(l90, l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b101)
b118 = sch.decompose_reduction(block=b101, loop=l103)
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #19: GFLOPs: 1.5311. Time: 5.9040 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #20: GFLOPs: 1.9160. Time: 4.7179 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #21: GFLOPs: 0.5756. Time: 15.7034 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #22: GFLOPs: 1.8739. Time: 4.8239 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(5, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 1):
                for i1_2_init, i3_2_init, i4_2_init, i2_3_init in T.grid(2, 7, 2, 7):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(40, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 8 + i1_1 * 2 + i1_2_init)
                            oh, ow = T.axis.remap("SS", [i2_3_init, i3_2_init])
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(32, 1, 1, 1, 2, 1, 7, 2, 18, 1, 1, 1, 1, 7):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(40, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 8 + i1_1 * 2 + i1_2)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(576, i5_0 * 18 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 8, 7):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(40, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 8 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[5, 4, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 18])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
l94 = sch.fuse(l91, l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b102)
b124 = sch.decompose_reduction(block=b102, loop=l109)
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #24: GFLOPs: 1.7671. Time: 5.1155 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #25: GFLOPs: 1.6961. Time: 5.3294 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 144, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i4_2_init, i1_3_init in T.grid(7, 2, 40):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk, oh = T.axis.remap("SS", [i1_3_init, i2_2_init])
                    ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(48, 1, 1, 1, 1, 7, 1, 2, 12, 1, 1, 1, 40, 1, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk, oh = T.axis.remap("SS", [i1_3, i2_2])
                    ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(576, i5_0 * 12 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 144, 7, 7, 4], "float32"], ["TENSOR", [40, 144, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(280, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(28):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(40, i0_i1_i2_fused // 7)
                    ax2 = T.axis.spatial(7, i0_i1_i2_fused % 7)
                    ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 40])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[48, 12])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l96, l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #27: GFLOPs: 1.0282. Time: 8.7919 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #28: GFLOPs: 0.6080. Time: 14.8679 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #29: GFLOPs: 1.4205. Time: 6.3638 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #30: GFLOPs: 0.3308. Time: 27.3296 ms. Best GFLOPs: 2.3388
[03:23:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #31: GFLOPs: 1.0550. Time: 8.5679 ms. Best GFLOPs: 2.3388
[03:24:06] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 836
Total latency (us): 208168

[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #0: GFLOPs: 0.8359. Time: 18.0266 ms. Best GFLOPs: 0.8359
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #1: GFLOPs: 1.1634. Time: 12.9516 ms. Best GFLOPs: 1.1634
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #2: GFLOPs: 0.3385. Time: 44.5120 ms. Best GFLOPs: 1.1634
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #3: GFLOPs: 2.5090. Time: 6.0059 ms. Best GFLOPs: 2.5090
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #4: GFLOPs: 0.2307. Time: 65.3189 ms. Best GFLOPs: 2.5090
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #5: GFLOPs: 0.5023. Time: 29.9998 ms. Best GFLOPs: 2.5090
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #6: GFLOPs: 2.1093. Time: 7.1439 ms. Best GFLOPs: 2.5090
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #7: GFLOPs: 0.9688. Time: 15.5537 ms. Best GFLOPs: 2.5090
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #8: GFLOPs: 0.2547. Time: 59.1684 ms. Best GFLOPs: 2.5090
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #9: GFLOPs: 1.1027. Time: 13.6652 ms. Best GFLOPs: 2.5090
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #10: GFLOPs: 2.8536. Time: 5.2805 ms. Best GFLOPs: 2.8536
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #11: GFLOPs: 1.4194. Time: 10.6160 ms. Best GFLOPs: 2.8536
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #12: GFLOPs: 1.4423. Time: 10.4472 ms. Best GFLOPs: 2.8536
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #13: GFLOPs: 2.2297. Time: 6.7581 ms. Best GFLOPs: 2.8536
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #14: GFLOPs: 0.7477. Time: 20.1530 ms. Best GFLOPs: 2.8536
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #15: GFLOPs: 1.0960. Time: 13.7489 ms. Best GFLOPs: 2.8536
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #16: GFLOPs: 0.3768. Time: 39.9885 ms. Best GFLOPs: 2.8536
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #17: GFLOPs: 2.9615. Time: 5.0881 ms. Best GFLOPs: 2.9615
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #18: GFLOPs: 8.0876. Time: 1.8632 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #19: GFLOPs: 1.6390. Time: 9.1935 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #20: GFLOPs: 3.0195. Time: 4.9903 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #21: GFLOPs: 1.8144. Time: 8.3048 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #22: GFLOPs: 1.6597. Time: 9.0791 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #23: GFLOPs: 0.5651. Time: 26.6640 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #24: GFLOPs: 0.9419. Time: 15.9984 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #25: GFLOPs: 2.0251. Time: 7.4408 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #26: GFLOPs: 2.4294. Time: 6.2026 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #27: GFLOPs: 0.3863. Time: 39.0111 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #28: GFLOPs: 1.2560. Time: 11.9973 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #29: GFLOPs: 2.5575. Time: 5.8920 ms. Best GFLOPs: 8.0876
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(40, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 40, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 40, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(10, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i1_3_init in T.grid(4, 7, 7, 2):
                for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 8 + i1_2_init * 2 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_2_init, i3_2_init])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i2_3_i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [40, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(48, 1, 1, 1, 4, 7, 7, 1, 20, 1, 1, 1, 2):
                for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 8 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i2_3_i3_3_i4_3_fused)
                        ic = T.axis.reduce(960, i5_0 * 20 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [40, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(280, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(28):
                with T.block("T_add_1"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(40, i0_i1_i2_fused // 7)
                    ax2 = T.axis.spatial(7, i0_i1_i2_fused % 7)
                    ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 5, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[48, 20])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l90, l91, l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96, l97)
sch.parallel(loop=l100)
l101 = sch.fuse(l98, l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b102)
b118 = sch.decompose_reduction(block=b102, loop=l104)
[03:24:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #31: GFLOPs: 0.8793. Time: 17.1364 ms. Best GFLOPs: 8.0876
[03:24:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 868
Total latency (us): 211894

[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #0: GFLOPs: 0.9467. Time: 15.9992 ms. Best GFLOPs: 0.9467
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #1: GFLOPs: 1.1831. Time: 12.8032 ms. Best GFLOPs: 1.1831
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 7, 1):
                for i1_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 4, 15, 7):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 30 + i1_2_init * 15 + i1_3_init)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3_init, i3_1, i4_2_init])
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 7, 7, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(40, 1, 1, 1, 2, 1, 1, 4, 4, 1, 1, 1, 15, 7, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 30 + i1_2 * 15 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_1, i4_2])
                        ic = T.axis.reduce(160, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 7, 7, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 30, 7):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 30 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 2, 15])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[40, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #3: GFLOPs: 1.5146. Time: 10.0008 ms. Best GFLOPs: 1.5146
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(196, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 1):
                for i1_2_init, i1_3_init in T.grid(24, 5):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 7 * 120 + i1_2_init * 5 + i1_3_init)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 28)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 2 + i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 7, 7, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(80, 1, 1, 1, 24, 1, 1, 1, 2, 1, 1, 1, 5):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 14 // 7 * 120 + i1_2 * 5 + i1_3)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 28)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 28 // 14 * 2 + i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(160, i5_0 * 2 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 7, 7, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(240, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 24, 5])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[80, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l93)
l94 = sch.fuse(l90, l91, l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l95, l96)
sch.parallel(loop=l100)
l101 = sch.fuse(l98, l99)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l106)
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #5: GFLOPs: 0.4887. Time: 30.9947 ms. Best GFLOPs: 1.5146
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #6: GFLOPs: 1.3115. Time: 11.5492 ms. Best GFLOPs: 1.5146
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #7: GFLOPs: 0.1222. Time: 123.9911 ms. Best GFLOPs: 1.5146
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #8: GFLOPs: 2.0561. Time: 7.3668 ms. Best GFLOPs: 2.0561
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #9: GFLOPs: 2.1363. Time: 7.0904 ms. Best GFLOPs: 2.1363
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #10: GFLOPs: 1.9307. Time: 7.8452 ms. Best GFLOPs: 2.1363
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #11: GFLOPs: 2.4889. Time: 6.0858 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #12: GFLOPs: 0.8285. Time: 18.2824 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #13: GFLOPs: 1.0682. Time: 14.1805 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #14: GFLOPs: 0.6183. Time: 24.4968 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #15: GFLOPs: 1.6709. Time: 9.0653 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #16: GFLOPs: 1.6069. Time: 9.4261 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #17: GFLOPs: 1.3772. Time: 10.9982 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #18: GFLOPs: 1.3543. Time: 11.1841 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #19: GFLOPs: 1.5988. Time: 9.4737 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #20: GFLOPs: 1.3377. Time: 11.3230 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #21: GFLOPs: 2.3140. Time: 6.5459 ms. Best GFLOPs: 2.4889
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #22: GFLOPs: 2.6844. Time: 5.6425 ms. Best GFLOPs: 2.6844
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #23: GFLOPs: 0.8011. Time: 18.9083 ms. Best GFLOPs: 2.6844
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #24: GFLOPs: 2.0179. Time: 7.5062 ms. Best GFLOPs: 2.6844
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #25: GFLOPs: 2.3019. Time: 6.5803 ms. Best GFLOPs: 2.6844
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #26: GFLOPs: 0.7102. Time: 21.3282 ms. Best GFLOPs: 2.6844
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #27: GFLOPs: 1.1139. Time: 13.5976 ms. Best GFLOPs: 2.6844
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #28: GFLOPs: 0.8584. Time: 17.6463 ms. Best GFLOPs: 2.6844
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #29: GFLOPs: 1.4878. Time: 10.1805 ms. Best GFLOPs: 2.6844
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #30: GFLOPs: 2.6519. Time: 5.7117 ms. Best GFLOPs: 2.6844
[03:24:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"] Trial #31: GFLOPs: 1.8937. Time: 7.9987 ms. Best GFLOPs: 2.6844
[03:24:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 900
Total latency (us): 228822

[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #0: GFLOPs: 0.1502. Time: 6.2631 ms. Best GFLOPs: 0.1502
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #1: GFLOPs: 0.0588. Time: 15.9988 ms. Best GFLOPs: 0.1502
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_i1_fused in T.parallel(240, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(9):
                for i3_i4_fused in T.vectorized(36):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(9, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[i0, i1, i2_1 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2_1, i3, i4])
                        PaddedInput[i0, i1, i2_1, i3, i4] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2_1 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 5, 7, 1, 1):
                for i1_3_init in T.serial(48):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i1_2 * 48 + i1_3_init)
                        oh = T.axis.spatial(7, i2_2)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 48, 1, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i1_2 * 48 + i1_3)
                        oh = T.axis.spatial(7, i2_2)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 240, 7, 1, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                    ax3_1 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 5, 48])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67)
sch.parallel(loop=l71)
l72 = sch.fuse(l69, l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77, l78, l79, l80, l81, l82)
sch.parallel(loop=l97)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b104)
b120 = sch.decompose_reduction(block=b104, loop=l113)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_0 in T.serial(1, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1):
                for i0_1, i1_1, i2_1 in T.grid(1, 8, 7):
                    for ax0, ax1, ax2 in T.grid(1, 30, 3):
                        for ax3_ax4_fused in T.vectorized(36):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(240, i1_1 * 30 + ax1)
                                i2 = T.axis.spatial(9, i2_1 + ax2)
                                i3 = T.axis.spatial(9, ax3_ax4_fused // 4)
                                i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i3_1, i4_1 in T.grid(1, 1):
                        for i1_2_init, i3_2_init, i1_3_init in T.grid(15, 7, 2):
                            for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    oco = T.axis.spatial(240, i1_1 * 30 + i1_2_init * 2 + i1_3_init)
                                    oh, ow, oci = T.axis.remap("SSS", [i2_1, i3_2_init, i2_3_i3_3_i4_3_fused_init])
                                    T.reads()
                                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 3, 1, 15, 1, 7, 1, 1, 1, 1, 2):
                            for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    oco = T.axis.spatial(240, i1_1 * 30 + i1_2 * 2 + i1_3)
                                    oh, ow, oci, kh, kw = T.axis.remap("SSSRR", [i2_1, i3_2, i2_3_i3_3_i4_3_fused, i5_0, i6_0])
                                    T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 240, 7):
                    for ax3_ax4_fused in T.vectorized(28):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                            ax3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 15, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b63)
l79 = sch.fuse(l77, l78)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l66, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l66, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b64)
l104 = sch.fuse(l101, l102, l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b65)
l115 = sch.fuse(l113, l114)
sch.vectorize(loop=l115)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b116 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b116)
b139 = sch.decompose_reduction(block=b116, loop=l127)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #4: GFLOPs: 0.0588. Time: 15.9976 ms. Best GFLOPs: 0.1502
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #5: GFLOPs: 0.1359. Time: 6.9220 ms. Best GFLOPs: 0.1502
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #6: GFLOPs: 0.0784. Time: 11.9995 ms. Best GFLOPs: 0.1502
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #7: GFLOPs: 0.1699. Time: 5.5385 ms. Best GFLOPs: 0.1699
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #8: GFLOPs: 0.1076. Time: 8.7464 ms. Best GFLOPs: 0.1699
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #9: GFLOPs: 0.0818. Time: 11.4973 ms. Best GFLOPs: 0.1699
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(192, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 5, 9, 9, 1):
                with T.block("PaddedInput"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 48 * 60 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 12 * 5 + ax1)
                    i2, i3 = T.axis.remap("SS", [ax2, ax3])
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 48 // 12)
                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_1, i3_1, i4_1 in T.grid(1, 7, 1):
                for i1_3_init, i2_3_init in T.grid(5, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 48 * 60 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 12 * 5 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_3_init, i3_1])
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 48 // 12)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 5, 7, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 48 * 60 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 12 * 5 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_1])
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 48 // 12)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 5, 7, 1, 1):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 48 * 60 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 12 * 5 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, i3_1])
                        ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 48 // 12)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 12, 1, 5])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b63)
l78 = sch.fuse(l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l78)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b106)
b125 = sch.decompose_reduction(block=b106, loop=l111)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #11: GFLOPs: 0.1020. Time: 9.2277 ms. Best GFLOPs: 0.1699
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #12: GFLOPs: 0.2028. Time: 4.6392 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #13: GFLOPs: 0.1390. Time: 6.7689 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #14: GFLOPs: 0.1337. Time: 7.0381 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #15: GFLOPs: 0.1481. Time: 6.3518 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #16: GFLOPs: 0.1465. Time: 6.4207 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(98, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 120, 3):
                for ax3_ax4_fused in T.vectorized(12):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_fused % 98 // 49 * 120 + ax1)
                        i2 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 + ax2)
                        i3 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_0 in T.serial(1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 20, 1, 1, 2, 1):
                    for i1_2_init, i4_2_init in T.grid(6, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_fused // 49 * 120 + i1_1 * 6 + i1_2_init)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 6, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_fused // 49 * 120 + i1_1 * 6 + i1_2)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1 in T.grid(1, 120):
                    for ax2_ax3_ax4_fused in T.vectorized(4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_fused // 49 * 120 + ax1)
                            ax2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7)
                            ax3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            ax4 = T.axis.spatial(4, ax2_ax3_ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 20, 6, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b63)
l75 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l75)
l76 = sch.fuse(l73, l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l102, l103, l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b106)
b128 = sch.decompose_reduction(block=b106, loop=l115)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #18: GFLOPs: 0.1051. Time: 8.9550 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_i1_fused in T.parallel(240, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(9):
                for i3_i4_fused in T.vectorized(36):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(9, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[i0, i1, i2_1 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2_1, i3, i4])
                        PaddedInput[i0, i1, i2_1, i3, i4] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2_1 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(245, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 2, 1, 1, 1):
                    for i1_3_init in T.serial(24):
                        for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 49 * 48 + i1_2 * 24 + i1_3_init)
                                oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                                ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7)
                                oci = T.axis.spatial(4, i2_3_i3_3_i4_3_fused_init)
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3 in T.grid(3, 3, 1, 24):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 49 * 48 + i1_2 * 24 + i1_3)
                                oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                                ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7)
                                oci, kh, kw = T.axis.remap("SRR", [i2_3_i3_3_i4_3_fused, i5_1, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1 in T.grid(1, 48):
                    for ax2_ax3_ax4_fused in T.vectorized(4):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 49 * 48 + ax1)
                            ax2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                            ax3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7)
                            ax4 = T.axis.spatial(4, ax2_ax3_ax4_fused)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 5, 2, 24])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70 = sch.get_loops(block=b63)
l71 = sch.fuse(l66, l67)
sch.parallel(loop=l71)
l72 = sch.fuse(l69, l70)
sch.vectorize(loop=l72)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l73, l74, l75, l76, l77, l78, l79, l80, l81)
sch.parallel(loop=l97)
l98 = sch.fuse(l94, l95, l96)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b65)
l106 = sch.fuse(l103, l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b107 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b107)
b122 = sch.decompose_reduction(block=b107, loop=l117)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #20: GFLOPs: 0.0965. Time: 9.7484 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i1_3_init in T.grid(7, 7, 30):
                for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 30 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_2_init, i3_2_init])
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i2_3_i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3 in T.grid(3, 3, 1, 1, 7, 7, 1, 1, 1, 1, 30):
                for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 30 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i2_3_i3_3_i4_3_fused)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 8 and 1 <= ow + kw and ow + kw < 8, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 30, 7, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 30 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 1, 30])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l89)
l90 = sch.fuse(l86, l87, l88)
sch.vectorize(loop=l90)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b64)
l97 = sch.fuse(l96)
sch.vectorize(loop=l97)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
b98 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b98)
b112 = sch.decompose_reduction(block=b98, loop=l100)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(70, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(2, 4, 12, 7):
                with T.block("DepthwiseConv2d_init"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 48 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 24 + i1_2_init * 12 + i1_3_init)
                    oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                    ow, oci = T.axis.remap("SS", [i3_3_init, i4_2_init])
                    T.reads()
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 2, 1, 1, 4, 1, 3, 1, 12, 1, 7, 1):
                with T.block("DepthwiseConv2d_update"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 48 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 24 + i1_2 * 12 + i1_3)
                    oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                    ow, oci, kh, kw = T.axis.remap("SSRR", [i3_3, i4_2, i5_0, i6_1])
                    T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 8 and 1 <= ow + kw and ow + kw < 8, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1 in T.grid(1, 24):
                for ax2_ax3_ax4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 48 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 7 * 24 + ax1)
                        ax2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                        ax3 = T.axis.spatial(7, ax2_ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax2_ax3_ax4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[5, 2, 2, 12])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64 = sch.get_child_blocks(b62)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b63)
l89 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b64)
l96 = sch.fuse(l93, l94, l95)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
b97 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b97)
b113 = sch.decompose_reduction(block=b97, loop=l99)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(49, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2 in T.grid(1, 1, 1, 1, 4, 1, 1, 1, 5):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 3, 3, 1):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(240, i1_2 * 48 + ax1)
                        i2 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 49 // 7 + ax2)
                        i3 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7 + ax3)
                        i4 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_2, i3_2, i4_2 in T.grid(1, 1, 1):
                    for i1_3_init in T.serial(48):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(240, i1_2 * 48 + i1_3_init)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7)
                            oci = T.axis.spatial(4, i4_1)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 48, 1, 1, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(240, i1_2 * 48 + i1_3)
                            oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1 in T.grid(1, 240):
                for ax2_ax3_ax4_fused in T.vectorized(4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(240, ax1)
                        ax2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 7)
                        ax3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 7)
                        ax4 = T.axis.spatial(4, ax2_ax3_ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_relu[ax0_1, ax1_1, ax2, ax3, ax4])
                        T_relu[ax0_1, ax1_1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 5, 48])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=13)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b63)
l85 = sch.fuse(l66, l67, l68, l69, l70)
sch.parallel(loop=l85)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l86, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l86, ann_key="pragma_unroll_explicit", ann_val=1)
l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b65)
l112 = sch.fuse(l109, l110, l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l106, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l106, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b113)
b134 = sch.decompose_reduction(block=b113, loop=l127)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(3, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 80, 9):
                for ax3_ax4_fused in T.vectorized(36):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(240, i0_0_i1_0_fused * 80 + ax1)
                        i2 = T.axis.spatial(9, ax2)
                        i3 = T.axis.spatial(9, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0 in T.grid(1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 7, 1, 1):
                    for i1_2_init, i1_3_init, i3_3_init in T.grid(8, 10, 7):
                        for i4_3_fused_init in T.vectorized(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(240, i0_0_i1_0_fused * 80 + i1_2_init * 10 + i1_3_init)
                                oh, ow, oci = T.axis.remap("SSS", [i2_1, i3_3_init, i4_3_fused_init])
                                T.reads()
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 1, 8, 1, 1, 1, 1, 3, 1, 10, 1, 7):
                        for i4_3_fused in T.vectorized(4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                oco = T.axis.spatial(240, i0_0_i1_0_fused * 80 + i1_2 * 10 + i1_3)
                                oh, ow, oci, kh, kw = T.axis.remap("SSSRR", [i2_1, i3_3, i4_3_fused, i5_0, i6_1])
                                T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2 in T.grid(1, 80, 7):
                    for ax3_ax4_fused in T.vectorized(28):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(240, i0_0_i1_0_fused * 80 + ax1)
                            ax2_1 = T.axis.spatial(7, ax2)
                            ax3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.max(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 1, 8, 10])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b63)
l73 = sch.fuse(l66, l67)
sch.parallel(loop=l73)
l74 = sch.fuse(l71, l72)
sch.vectorize(loop=l74)
sch.annotate(block_or_loop=l73, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l73, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
l98 = sch.fuse(l97)
sch.vectorize(loop=l98)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b65)
l108 = sch.fuse(l106, l107)
sch.vectorize(loop=l108)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b109 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b109)
b133 = sch.decompose_reduction(block=b109, loop=l119)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #25: GFLOPs: 0.1411. Time: 6.6671 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_i1_fused in T.parallel(240, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(9):
                for i3_i4_fused in T.vectorized(36):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2_1 = T.axis.remap("SS", [i0_i1_fused, i2])
                        i3 = T.axis.spatial(9, i3_i4_fused // 4)
                        i4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(placeholder[i0, i1, i2_1 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2_1, i3, i4])
                        PaddedInput[i0, i1, i2_1, i3, i4] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2_1 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(560, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 1, 1, 4, 1, 7, 1):
                for i1_3_init in T.serial(3):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 280 * 120 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 70 // 7 * 12 + i1_2 * 3 + i1_3_init)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7)
                        ow = T.axis.spatial(7, i3_2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 280 // 70)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 3, 1, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 280 * 120 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 70 // 7 * 12 + i1_2 * 3 + i1_3)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 7)
                        ow = T.axis.spatial(7, i3_2)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 280 // 70)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_fused in T.parallel(240, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 10, 4, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69 = sch.get_loops(block=b62)
l70 = sch.fuse(l65, l66)
sch.parallel(loop=l70)
l71 = sch.fuse(l68, l69)
sch.vectorize(loop=l71)
sch.annotate(block_or_loop=l70, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l70, ann_key="pragma_unroll_explicit", ann_val=1)
l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b63)
l96 = sch.fuse(l72, l73, l74, l75, l76, l77, l78, l79)
sch.parallel(loop=l96)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98)
sch.parallel(loop=l102)
l103 = sch.fuse(l100, l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b104)
b122 = sch.decompose_reduction(block=b104, loop=l115)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #27: GFLOPs: 0.0784. Time: 11.9977 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #28: GFLOPs: 0.1547. Time: 6.0795 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #29: GFLOPs: 0.1541. Time: 6.1062 ms. Best GFLOPs: 0.2028
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(280, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 24, 3, 9, 1):
                with T.block("PaddedInput"):
                    i0 = T.axis.spatial(1, ax0)
                    i1 = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 280 // 28 * 24 + ax1)
                    i2 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4 + ax2)
                    i3 = T.axis.spatial(9, ax3)
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 + ax4)
                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                    T.writes(PaddedInput[i0, i1, i2, i3, i4])
                    PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i5_0 in T.serial(1):
                for i1_2_init, i1_3_init, i3_3_init in T.grid(2, 12, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 24 + i1_2_init * 12 + i1_3_init)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4)
                        ow = T.axis.spatial(7, i3_3_init)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 1, 1, 1, 3, 1, 1, 12, 1, 7, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 24 + i1_2 * 12 + i1_3)
                        oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4)
                        ow = T.axis.spatial(7, i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [240, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_fused in T.parallel(240, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[10, 1, 2, 12])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b62)
l80 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73, l74)
sch.parallel(loop=l80)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100 = sch.get_loops(block=b64)
l101 = sch.fuse(l96, l97)
sch.parallel(loop=l101)
l102 = sch.fuse(l99, l100)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l101, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l101, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b103)
b119 = sch.decompose_reduction(block=b103, loop=l106)
[03:25:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"] Trial #31: GFLOPs: 0.1199. Time: 7.8477 ms. Best GFLOPs: 0.2028
[03:25:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 932
Total latency (us): 242739

[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #0: GFLOPs: 1.7473. Time: 17.2391 ms. Best GFLOPs: 1.7473
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #1: GFLOPs: 1.7264. Time: 17.4473 ms. Best GFLOPs: 1.7473
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #2: GFLOPs: 3.0128. Time: 9.9979 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #3: GFLOPs: 2.2153. Time: 13.5972 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #4: GFLOPs: 1.6374. Time: 18.3964 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #5: GFLOPs: 1.3437. Time: 22.4171 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #6: GFLOPs: 2.9817. Time: 10.1022 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(80, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 80, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 80, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i4_2_init, i3_3_init in T.grid(10, 4, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(80, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 * 10 + i1_2_init)
                    oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8)
                    ow, oc_block = T.axis.remap("SS", [i3_3_init, i4_2_init])
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(96, 1, 1, 1, 10, 1, 1, 4, 10, 1, 1, 1, 1, 1, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(80, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 * 10 + i1_2)
                    oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8)
                    ow, oc_block = T.axis.remap("SS", [i3_3, i4_2])
                    ic = T.axis.reduce(960, i5_0 * 10 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(560, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(28):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(80, i0_i1_i2_fused // 7)
                    ax2 = T.axis.spatial(7, i0_i1_i2_fused % 7)
                    ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 8, 10, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[96, 10])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l96, l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #8: GFLOPs: 1.9431. Time: 15.5014 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #9: GFLOPs: 2.9116. Time: 10.3454 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(80, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 80, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 80, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(7, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 10, 1, 1, 4):
                for i1_2_init, i2_2_init, i1_3_init in T.grid(4, 7, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(80, i1_1 * 8 + i1_2_init * 2 + i1_3_init)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2_init, i0_0_i1_0_i2_0_i3_0_i4_0_fused, i4_1])
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(120, 1, 1, 1, 4, 7, 1, 1, 8, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(80, i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i0_0_i1_0_i2_0_i3_0_i4_0_fused, i4_1])
                        ic = T.axis.reduce(960, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 80, 7):
                for ax3_ax4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3, ax4 = T.axis.remap("SSSS", [ax1, ax2, i0_0_i1_0_i2_0_i3_0_i4_0_fused, ax3_ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 10, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[120, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l98, l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b101)
b124 = sch.decompose_reduction(block=b101, loop=l108)
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #11: GFLOPs: 1.0038. Time: 30.0066 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #12: GFLOPs: 1.4067. Time: 21.4130 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #13: GFLOPs: 0.8095. Time: 37.2112 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #14: GFLOPs: 2.0089. Time: 14.9942 ms. Best GFLOPs: 3.0128
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #15: GFLOPs: 3.7657. Time: 7.9988 ms. Best GFLOPs: 3.7657
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #16: GFLOPs: 0.6852. Time: 43.9569 ms. Best GFLOPs: 3.7657
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #17: GFLOPs: 3.9126. Time: 7.6986 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #18: GFLOPs: 0.8380. Time: 35.9427 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #19: GFLOPs: 1.8324. Time: 16.4385 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #20: GFLOPs: 1.8342. Time: 16.4222 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #21: GFLOPs: 1.8369. Time: 16.3982 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #22: GFLOPs: 2.1966. Time: 13.7125 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #23: GFLOPs: 0.9119. Time: 33.0318 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(80, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 80, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 80, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(160, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_3_init in T.grid(7, 7):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(80, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                        oh, ow = T.axis.remap("SS", [i2_2_init, i3_3_init])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(160, 1, 1, 1, 1, 7, 1, 1, 6, 1, 1, 1, 1, 1, 7):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(80, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(960, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(560, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(28):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(80, i0_i1_i2_fused // 7)
                    ax2 = T.axis.spatial(7, i0_i1_i2_fused % 7)
                    ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 80, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[160, 6])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
l93 = sch.fuse(l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(80, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 80, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 80, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 80, 7, 7, 4], dtype="float32")
        for i0_0 in T.serial(1, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1):
                    for i1_2_init, i2_3_init, i3_3_init in T.grid(80, 7, 7):
                        for i4_3_fused_init in T.vectorized(4):
                            with T.block("conv2d_NCHWc_init"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk, oh, ow, oc_block = T.axis.remap("SSSS", [i1_2_init, i2_3_init, i3_3_init, i4_3_fused_init])
                                T.reads()
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(192, 1, 1, 1, 80, 1, 1, 1, 5, 1, 1, 1, 1, 7, 7):
                        for i4_3_fused in T.vectorized(4):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk, oh, ow, oc_block = T.axis.remap("SSSS", [i1_2, i2_3, i3_3, i4_3_fused])
                                ic = T.axis.reduce(960, i5_0 * 5 + i5_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [80, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 80, 7):
                    for ax3_ax4_fused in T.vectorized(28):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                            ax3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 80, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[192, 5])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l92)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l67, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l67, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b66)
l104 = sch.fuse(l102, l103)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b105)
b132 = sch.decompose_reduction(block=b105, loop=l116)
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #26: GFLOPs: 2.1009. Time: 14.3374 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #27: GFLOPs: 2.8604. Time: 10.5304 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #28: GFLOPs: 1.8282. Time: 16.4758 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #29: GFLOPs: 0.6285. Time: 47.9262 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #30: GFLOPs: 2.2666. Time: 13.2895 ms. Best GFLOPs: 3.9126
[03:25:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #31: GFLOPs: 2.8567. Time: 10.5439 ms. Best GFLOPs: 3.9126
[03:26:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 964
Total latency (us): 250438

[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #0: GFLOPs: 4.7125. Time: 8.5445 ms. Best GFLOPs: 4.7125
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #1: GFLOPs: 2.5597. Time: 15.7308 ms. Best GFLOPs: 4.7125
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #2: GFLOPs: 1.8396. Time: 21.8888 ms. Best GFLOPs: 4.7125
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #3: GFLOPs: 4.4107. Time: 9.1291 ms. Best GFLOPs: 4.7125
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #4: GFLOPs: 4.5108. Time: 8.9266 ms. Best GFLOPs: 4.7125
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #5: GFLOPs: 6.4047. Time: 6.2870 ms. Best GFLOPs: 6.4047
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #6: GFLOPs: 1.8992. Time: 21.2020 ms. Best GFLOPs: 6.4047
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #7: GFLOPs: 3.1793. Time: 12.6650 ms. Best GFLOPs: 6.4047
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #8: GFLOPs: 4.5357. Time: 8.8776 ms. Best GFLOPs: 6.4047
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #9: GFLOPs: 0.7823. Time: 51.4696 ms. Best GFLOPs: 6.4047
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #10: GFLOPs: 1.9271. Time: 20.8947 ms. Best GFLOPs: 6.4047
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #11: GFLOPs: 6.7441. Time: 5.9706 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #12: GFLOPs: 3.7593. Time: 10.7111 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #13: GFLOPs: 2.5200. Time: 15.9785 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #14: GFLOPs: 3.4424. Time: 11.6970 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #15: GFLOPs: 2.3032. Time: 17.4830 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #16: GFLOPs: 3.3621. Time: 11.9765 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #17: GFLOPs: 2.8415. Time: 14.1708 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #18: GFLOPs: 2.4477. Time: 16.4506 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #19: GFLOPs: 3.0682. Time: 13.1239 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #20: GFLOPs: 2.1573. Time: 18.6655 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #21: GFLOPs: 2.2750. Time: 17.6997 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 80, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(320, 80, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 320, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(10, 7, 4, 8, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(320, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 80 + i1_2_init * 8 + i1_3_init)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_2_init, i3_3_init, i4_2_init])
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 7, 7, 4], "float32"], ["TENSOR", [320, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 10, 7, 1, 4, 5, 1, 1, 1, 8, 1, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(320, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 80 + i1_2 * 8 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_3, i4_2])
                    ic = T.axis.reduce(320, i5_0 * 5 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 80, 7, 7, 4], "float32"], ["TENSOR", [320, 80, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(320, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for i3_i4_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(7, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                        T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 10, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 5])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #23: GFLOPs: 2.7716. Time: 14.5282 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #24: GFLOPs: 1.8855. Time: 21.3557 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #25: GFLOPs: 1.7738. Time: 22.7000 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #26: GFLOPs: 0.8966. Time: 44.9096 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #27: GFLOPs: 1.1824. Time: 34.0545 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #28: GFLOPs: 1.9090. Time: 21.0930 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #29: GFLOPs: 3.3555. Time: 12.0001 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #30: GFLOPs: 1.9359. Time: 20.7996 ms. Best GFLOPs: 6.7441
[03:26:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"] Trial #31: GFLOPs: 1.2368. Time: 32.5569 ms. Best GFLOPs: 6.7441
[03:27:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 996
Total latency (us): 256409

[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_nn_global_avg_pool2d"] Trial #0: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 320, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 320, 1, 1, 4], dtype="float32")
        tensor_rf = T.alloc_buffer([1, 320, 1, 1, 4, 49], dtype="float32")
        for i0_i1_fused in T.parallel(320, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2, i3, i4 in T.grid(1, 1, 4):
                for i5_i6_fused_0_i5_i6_fused_1_fused in T.vectorized(49):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_0 = T.axis.spatial(49, i5_i6_fused_0_i5_i6_fused_1_fused)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(320, i0_i1_fused)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        ax4 = T.axis.spatial(4, i4)
                        T.reads(placeholder[ax0, ax1, ax2 * 7 + vi5_i6_fused_0 // 7, ax3 * 7 + vi5_i6_fused_0 % 7, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = placeholder[ax0, ax1, ax2 * 7 + vi5_i6_fused_0 // 7, ax3 * 7 + vi5_i6_fused_0 % 7, ax4]
        for i0_i1_fused in T.parallel(320, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax5_init in T.serial(4):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(320, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, ax5_init)
                    T.reads()
                    T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
            for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(49, 1, 1, 1, 1, 4):
                with T.block("tensor_update"):
                    vi5_i6_fused_0 = T.axis.reduce(49, ax0)
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(320, i0_i1_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    ax4_1 = T.axis.spatial(4, ax5)
                    T.reads(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0])
                    T.writes(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0]
            for i2_i3_i4_fused in T.vectorized(4):
                with T.block("tensor_1"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(320, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, i2_i3_i4_fused)
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[49, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=-1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
sch.enter_postproc()
b19 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.unroll_explicit")
b20, b21, b22 = sch.get_child_blocks(b19)
l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b20)
l30 = sch.fuse(l23, l24)
sch.parallel(loop=l30)
l31 = sch.fuse(l28, l29)
sch.vectorize(loop=l31)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
l32, l33, l34, l35, l36, l37, l38, l39 = sch.get_loops(block=b21)
l40 = sch.fuse(l32, l33)
sch.parallel(loop=l40)
sch.annotate(block_or_loop=l40, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l40, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42, l43, l44 = sch.get_loops(block=b22)
l45 = sch.fuse(l42, l43, l44)
sch.vectorize(loop=l45)
sch.annotate(block_or_loop=l41, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l41, ann_key="pragma_unroll_explicit", ann_val=1)
b46 = sch.get_block(name="tensor", func_name="main")
l47, l48, l49, l50, l51, l52, l53 = sch.get_loops(block=b46)
b54 = sch.decompose_reduction(block=b46, loop=l48)
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #1: GFLOPs: 0.0077. Time: 8.2769 ms. Best GFLOPs: 0.0077
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #2: GFLOPs: 0.0055. Time: 11.6741 ms. Best GFLOPs: 0.0077
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #3: GFLOPs: 0.0030. Time: 21.4975 ms. Best GFLOPs: 0.0077
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #4: GFLOPs: 0.0160. Time: 3.9997 ms. Best GFLOPs: 0.0160
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #5: GFLOPs: 0.0105. Time: 6.0789 ms. Best GFLOPs: 0.0160
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #6: GFLOPs: 0.0309. Time: 2.0684 ms. Best GFLOPs: 0.0309
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #7: GFLOPs: 0.0190. Time: 3.3636 ms. Best GFLOPs: 0.0309
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #8: GFLOPs: 0.0504. Time: 1.2692 ms. Best GFLOPs: 0.0504
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #9: GFLOPs: 0.0214. Time: 2.9840 ms. Best GFLOPs: 0.0504
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #10: GFLOPs: 0.0199. Time: 3.2091 ms. Best GFLOPs: 0.0504
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #11: GFLOPs: 0.0139. Time: 4.6175 ms. Best GFLOPs: 0.0504
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #12: GFLOPs: 0.0135. Time: 4.7302 ms. Best GFLOPs: 0.0504
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #13: GFLOPs: 0.0149. Time: 4.3068 ms. Best GFLOPs: 0.0504
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #14: GFLOPs: 0.0065. Time: 9.8796 ms. Best GFLOPs: 0.0504
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #15: GFLOPs: 0.0288. Time: 2.2236 ms. Best GFLOPs: 0.0504
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #16: GFLOPs: 0.4130. Time: 0.1550 ms. Best GFLOPs: 0.4130
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #17: GFLOPs: 1.1501. Time: 0.0556 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #18: GFLOPs: 0.8328. Time: 0.0769 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #19: GFLOPs: 0.0969. Time: 0.6602 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #20: GFLOPs: 0.0405. Time: 1.5789 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #21: GFLOPs: 0.0161. Time: 3.9670 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #22: GFLOPs: 0.0053. Time: 11.9966 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #23: GFLOPs: 0.0064. Time: 9.9916 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #24: GFLOPs: 0.0144. Time: 4.4511 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #25: GFLOPs: 0.0231. Time: 2.7721 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #26: GFLOPs: 0.0081. Time: 7.8766 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #27: GFLOPs: 0.0143. Time: 4.4894 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #28: GFLOPs: 0.0145. Time: 4.4127 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_nn_global_avg_pool2d"] Trial #29: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 320, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 320, 1, 1, 4], dtype="float32")
        tensor_rf = T.alloc_buffer([1, 320, 1, 1, 4, 49], dtype="float32")
        for i0_i1_fused in T.parallel(320, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(49, 1, 1, 1, 1, 4):
                with T.block("tensor_rf"):
                    vi5_i6_fused_1 = T.axis.spatial(49, ax0)
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(320, i0_i1_fused)
                    ax2_1 = T.axis.spatial(1, 0)
                    ax3_1 = T.axis.spatial(1, 0)
                    ax4_1 = T.axis.spatial(4, ax5)
                    T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 7 + vi5_i6_fused_1 // 7, ax3_1 * 7 + vi5_i6_fused_1 % 7, ax4_1])
                    T.writes(tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1])
                    tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] = placeholder[ax0_1, ax1_1, ax2_1 * 7 + vi5_i6_fused_1 // 7, ax3_1 * 7 + vi5_i6_fused_1 % 7, ax4_1]
            for i2, i3, i4, i5_i6_fused_0 in T.grid(1, 1, 4, 1):
                with T.block("tensor_init"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(320, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, i4)
                    T.reads()
                    T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                for i5_i6_fused_1 in T.serial(49):
                    with T.block("tensor_update"):
                        vi5_i6_fused_1 = T.axis.reduce(49, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(320, i0_i1_fused)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        ax4 = T.axis.spatial(4, i4)
                        T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                        tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1]
        for i0_i1_fused in T.parallel(320, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i2_i3_i4_fused in T.vectorized(4):
                with T.block("tensor_1"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(320, i0_i1_fused)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    ax4 = T.axis.spatial(4, i2_i3_i4_fused)
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 49])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
sch.enter_postproc()
b19 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b19, ann_key="meta_schedule.unroll_explicit")
b20, b21, b22 = sch.get_child_blocks(b19)
l23, l24, l25, l26, l27, l28, l29, l30 = sch.get_loops(block=b20)
l31 = sch.fuse(l23, l24)
sch.parallel(loop=l31)
sch.annotate(block_or_loop=l31, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l31, ann_key="pragma_unroll_explicit", ann_val=1)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b21)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l38, l39, l40, l41, l42 = sch.get_loops(block=b22)
l43 = sch.fuse(l38, l39)
sch.parallel(loop=l43)
l44 = sch.fuse(l40, l41, l42)
sch.vectorize(loop=l44)
sch.annotate(block_or_loop=l43, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l43, ann_key="pragma_unroll_explicit", ann_val=1)
b45 = sch.get_block(name="tensor", func_name="main")
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
b52 = sch.decompose_reduction(block=b45, loop=l51)
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #30: GFLOPs: 0.0218. Time: 2.9333 ms. Best GFLOPs: 1.1501
[03:27:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_global_avg_pool2d"] Trial #31: GFLOPs: 0.0303. Time: 2.1133 ms. Best GFLOPs: 1.1501
[03:27:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #32: "fused_nn_global_avg_pool2d"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1028
Total latency (us): 256464

[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #0: GFLOPs: 0.4767. Time: 5.3701 ms. Best GFLOPs: 0.4767
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #1: GFLOPs: 0.4327. Time: 5.9166 ms. Best GFLOPs: 0.4767
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #2: GFLOPs: 0.3823. Time: 6.6972 ms. Best GFLOPs: 0.4767
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #3: GFLOPs: 0.5335. Time: 4.7987 ms. Best GFLOPs: 0.5335
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #4: GFLOPs: 0.3532. Time: 7.2487 ms. Best GFLOPs: 0.5335
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #5: GFLOPs: 0.1925. Time: 13.2995 ms. Best GFLOPs: 0.5335
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #6: GFLOPs: 0.5091. Time: 5.0281 ms. Best GFLOPs: 0.5335
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #7: GFLOPs: 0.4431. Time: 5.7778 ms. Best GFLOPs: 0.5335
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #8: GFLOPs: 0.5565. Time: 4.5998 ms. Best GFLOPs: 0.5565
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #9: GFLOPs: 10.2749. Time: 0.2492 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #10: GFLOPs: 0.1566. Time: 16.3475 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #11: GFLOPs: 0.4386. Time: 5.8368 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #12: GFLOPs: 0.1085. Time: 23.6020 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #13: GFLOPs: 0.5461. Time: 4.6880 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #14: GFLOPs: 0.3580. Time: 7.1512 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #15: GFLOPs: 0.4674. Time: 5.4773 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #16: GFLOPs: 0.6565. Time: 3.8995 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #17: GFLOPs: 0.3961. Time: 6.4624 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #18: GFLOPs: 1.1320. Time: 2.2614 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #19: GFLOPs: 0.4167. Time: 6.1439 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #20: GFLOPs: 0.3800. Time: 6.7364 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #21: GFLOPs: 0.4269. Time: 5.9961 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #22: GFLOPs: 0.4923. Time: 5.1996 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #23: GFLOPs: 0.5688. Time: 4.5010 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #24: GFLOPs: 0.4377. Time: 5.8492 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #25: GFLOPs: 0.4571. Time: 5.6003 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #26: GFLOPs: 0.5362. Time: 4.7742 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #27: GFLOPs: 1.0452. Time: 2.4492 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #28: GFLOPs: 0.3757. Time: 6.8132 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #29: GFLOPs: 0.2200. Time: 11.6343 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #30: GFLOPs: 0.4694. Time: 5.4543 ms. Best GFLOPs: 10.2749
[03:27:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc"] Trial #31: GFLOPs: 0.5000. Time: 5.1195 ms. Best GFLOPs: 10.2749
[03:28:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #33: "fused_nn_contrib_conv2d_NCHWc"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |        10.2749 |     249.1515 |              249.1515 |     32 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1060
Total latency (us): 256713

[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #34: "fused_layout_transform_reshape"] Trial #0: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 250, 1, 1, 4), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_fused in T.parallel(1000, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(1, 0)
                ax1 = T.axis.spatial(1000, i0_i1_fused)
                T.reads(placeholder[0, ax1 % 1000 // 4, 0, 0, ax1 % 4])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T.if_then_else(0 < 1 and ax1 % 1000 < 1000 and 0 < 1 and 0 < 1, placeholder[0, ax1 % 1000 // 4, 0, 0, ax1 % 1000 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
sch.enter_postproc()
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit")
b5, = sch.get_child_blocks(b4)
l6, l7 = sch.get_loops(block=b5)
l8 = sch.fuse(l6, l7)
sch.parallel(loop=l8)
sch.annotate(block_or_loop=l8, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l8, ann_key="pragma_unroll_explicit", ann_val=1)
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #1: GFLOPs: 0.0000. Time: 4.4445 ms. Best GFLOPs: 0.0000
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #2: GFLOPs: 0.0000. Time: 2.4413 ms. Best GFLOPs: 0.0000
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #3: GFLOPs: 0.0000. Time: 1.5644 ms. Best GFLOPs: 0.0000
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #4: GFLOPs: 0.0000. Time: 3.6557 ms. Best GFLOPs: 0.0000
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #5: GFLOPs: 0.0000. Time: 1.4088 ms. Best GFLOPs: 0.0000
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #6: GFLOPs: 0.0000. Time: 5.8944 ms. Best GFLOPs: 0.0000
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #7: GFLOPs: 0.0000. Time: 1.3378 ms. Best GFLOPs: 0.0000
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #8: GFLOPs: 0.0000. Time: 0.2738 ms. Best GFLOPs: 0.0000
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #9: GFLOPs: 0.0000. Time: 0.9499 ms. Best GFLOPs: 0.0000
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #10: GFLOPs: 0.0000. Time: 0.4738 ms. Best GFLOPs: 0.0000
[03:28:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_reshape"] Trial #11: GFLOPs: 0.0000. Time: 0.2854 ms. Best GFLOPs: 0.0000
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #34: "fused_layout_transform_reshape"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |            
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |            
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |            
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |            
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |            
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |            
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |            
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |            
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |            
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |            
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |            
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |            
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |            
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |            
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |            
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |            
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |            
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |            
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |            
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |            
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |            
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |            
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |            
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |            
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |            
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |            
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |            
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |            
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |            
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |            
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |        10.2749 |     249.1515 |              249.1515 |     32 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |         0.0000 |     273.7661 |              273.7661 |     12 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1072
Total latency (us): 256987

[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #19 has finished. Remaining task(s): 34
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #18 has finished. Remaining task(s): 33
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #17 has finished. Remaining task(s): 32
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #28: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #28 has finished. Remaining task(s): 31
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #10 has finished. Remaining task(s): 30
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #29: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #29 has finished. Remaining task(s): 29
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #15 has finished. Remaining task(s): 28
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #14 has finished. Remaining task(s): 27
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #23 has finished. Remaining task(s): 26
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #22 has finished. Remaining task(s): 25
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #30: "fused_nn_contrib_conv2d_NCHWc_add_6"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #30 has finished. Remaining task(s): 24
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #2 has finished. Remaining task(s): 23
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #6 has finished. Remaining task(s): 22
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #31: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #31 has finished. Remaining task(s): 21
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #5 has finished. Remaining task(s): 20
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_contrib_conv2d_NCHWc_add"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #4 has finished. Remaining task(s): 19
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #21 has finished. Remaining task(s): 18
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #9 has finished. Remaining task(s): 17
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #24 has finished. Remaining task(s): 16
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_contrib_conv2d_NCHWc_add_1"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #7 has finished. Remaining task(s): 15
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #1 has finished. Remaining task(s): 14
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #3 has finished. Remaining task(s): 13
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #8 has finished. Remaining task(s): 12
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #11 has finished. Remaining task(s): 11
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #12 has finished. Remaining task(s): 10
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #26 has finished. Remaining task(s): 9
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_nn_contrib_conv2d_NCHWc_add_add_4"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #27 has finished. Remaining task(s): 8
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #13 has finished. Remaining task(s): 7
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #16 has finished. Remaining task(s): 6
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6"
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #20 has finished. Remaining task(s): 5
[03:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #34: "fused_layout_transform_reshape"
[03:29:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:29:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[03:29:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:29:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[03:29:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:29:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:30:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:30:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:30:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:30:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:30:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:30:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:30:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:30:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #34: "fused_layout_transform_reshape"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |          Y 
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |          Y 
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |          Y 
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |          Y 
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |          Y 
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |          Y 
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |          Y 
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |          Y 
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |          Y 
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |          Y 
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |          Y 
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |          Y 
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |          Y 
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |          Y 
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |          Y 
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |          Y 
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |          Y 
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |          Y 
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |          Y 
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |          Y 
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |          Y 
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |          Y 
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |          Y 
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |          Y 
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |          Y 
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |          Y 
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |        10.2749 |     249.1515 |              249.1515 |     32 |            
 34 |                        fused_layout_transform_reshape |        1 |      1 |         0.0000 |     273.7661 |              273.7661 |     12 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1072
Total latency (us): 256987

[03:30:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #33: "fused_nn_contrib_conv2d_NCHWc"
[03:30:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #33 has finished. Remaining task(s): 4
[03:30:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #34: "fused_layout_transform_reshape"
[03:30:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:30:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[03:30:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:30:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[03:31:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:31:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:31:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:32:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:32:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:32:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:32:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:32:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:32:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:32:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #34: "fused_layout_transform_reshape"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |          Y 
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |          Y 
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |          Y 
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |          Y 
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |          Y 
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |          Y 
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |          Y 
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |          Y 
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |          Y 
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |          Y 
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |          Y 
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |          Y 
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |          Y 
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |          Y 
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |          Y 
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |          Y 
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |          Y 
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |          Y 
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |          Y 
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |          Y 
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |          Y 
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |          Y 
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |          Y 
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |          Y 
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |          Y 
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |          Y 
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |        10.2749 |     249.1515 |              249.1515 |     32 |          Y 
 34 |                        fused_layout_transform_reshape |        1 |      1 |         0.0000 |     273.7661 |              273.7661 |     12 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1072
Total latency (us): 256987

[03:32:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #34: "fused_layout_transform_reshape"
[03:32:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:32:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[03:32:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:32:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[03:32:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:33:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:33:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:33:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:34:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:34:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:34:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:34:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:34:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:34:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #34: "fused_layout_transform_reshape"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |          Y 
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |          Y 
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |          Y 
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |          Y 
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |          Y 
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |          Y 
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |          Y 
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |          Y 
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |          Y 
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |          Y 
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |          Y 
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |          Y 
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |          Y 
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |          Y 
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |          Y 
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |          Y 
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |          Y 
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |          Y 
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |          Y 
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |          Y 
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |            
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |          Y 
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |          Y 
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |          Y 
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |          Y 
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |          Y 
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |          Y 
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |        10.2749 |     249.1515 |              249.1515 |     32 |          Y 
 34 |                        fused_layout_transform_reshape |        1 |      1 |         0.0000 |     273.7661 |              273.7661 |     12 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1072
Total latency (us): 256987

[03:34:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8"
[03:34:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #25 has finished. Remaining task(s): 3
[03:34:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #34: "fused_layout_transform_reshape"
[03:34:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:34:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[03:34:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:34:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[03:34:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:34:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:35:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:35:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:35:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:35:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:35:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:35:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:35:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:35:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #34: "fused_layout_transform_reshape"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |          Y 
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |          Y 
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |          Y 
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |          Y 
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |          Y 
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |          Y 
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |          Y 
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |          Y 
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |          Y 
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |          Y 
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |          Y 
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |          Y 
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |          Y 
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |          Y 
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |          Y 
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |          Y 
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |          Y 
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |          Y 
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |          Y 
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |          Y 
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |          Y 
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |          Y 
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |          Y 
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |          Y 
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |          Y 
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |          Y 
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |          Y 
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |            
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |        10.2749 |     249.1515 |              249.1515 |     32 |          Y 
 34 |                        fused_layout_transform_reshape |        1 |      1 |         0.0000 |     273.7661 |              273.7661 |     12 |            
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1072
Total latency (us): 256987

[03:35:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #32: "fused_nn_global_avg_pool2d"
[03:35:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #32 has finished. Remaining task(s): 2
[03:35:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #34: "fused_layout_transform_reshape"
[03:35:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:35:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 12 candidate(s) from database
[03:35:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:35:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2036 candidate(s)
[03:36:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:36:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:36:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:37:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde71b2a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acddc03858)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acdfd7ad38)]: 0 failure(s)
[03:37:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:37:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:37:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:37:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #34 has finished. Remaining task(s): 1
[03:37:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[03:37:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:37:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[03:37:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:37:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[03:37:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:37:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:38:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:38:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:38:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:38:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:38:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:38:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:38:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:38:33] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |          Y 
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |          Y 
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |          Y 
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |          Y 
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |          Y 
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |          Y 
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |          Y 
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |          Y 
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |          Y 
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |          Y 
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |          Y 
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |          Y 
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |          Y 
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |          Y 
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |          Y 
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |          Y 
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |          Y 
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |          Y 
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |          Y 
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |          Y 
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |          Y 
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |          Y 
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |          Y 
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |          Y 
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |          Y 
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |          Y 
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |          Y 
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |          Y 
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |        10.2749 |     249.1515 |              249.1515 |     32 |          Y 
 34 |                        fused_layout_transform_reshape |        1 |      1 |         0.0000 |     273.7661 |              273.7661 |     12 |          Y 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1072
Total latency (us): 256987

[03:38:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[03:38:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:38:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[03:38:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:38:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[03:38:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:38:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:39:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:39:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:39:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:39:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:39:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:39:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:39:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:39:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |          Y 
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |          Y 
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |          Y 
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |          Y 
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |          Y 
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |          Y 
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |          Y 
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |          Y 
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |          Y 
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |          Y 
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |          Y 
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |          Y 
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |          Y 
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |          Y 
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |          Y 
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |          Y 
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |          Y 
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |          Y 
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |          Y 
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |          Y 
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |          Y 
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |          Y 
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |          Y 
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |          Y 
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |          Y 
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |          Y 
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |          Y 
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |          Y 
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |        10.2749 |     249.1515 |              249.1515 |     32 |          Y 
 34 |                        fused_layout_transform_reshape |        1 |      1 |         0.0000 |     273.7661 |              273.7661 |     12 |          Y 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1072
Total latency (us): 256987

[03:39:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[03:39:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:39:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[03:39:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:39:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[03:39:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:39:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:40:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:40:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:40:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:40:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:40:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:40:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:40:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:40:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |          Y 
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |          Y 
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |          Y 
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |          Y 
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |          Y 
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |          Y 
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |          Y 
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |          Y 
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |          Y 
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |          Y 
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |          Y 
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |          Y 
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |          Y 
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |          Y 
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |          Y 
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |          Y 
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |          Y 
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |          Y 
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |          Y 
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |          Y 
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |          Y 
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |          Y 
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |          Y 
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |          Y 
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |          Y 
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |          Y 
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |          Y 
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |          Y 
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |        10.2749 |     249.1515 |              249.1515 |     32 |          Y 
 34 |                        fused_layout_transform_reshape |        1 |      1 |         0.0000 |     273.7661 |              273.7661 |     12 |          Y 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1072
Total latency (us): 256987

[03:40:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[03:40:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:40:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[03:40:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:40:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[03:40:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:41:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:41:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:41:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:41:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:41:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:41:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:41:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:41:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:41:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                                  Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_layout_transform |        1 |      1 |         0.0000 |      42.3054 |               42.3054 |      4 |            
  1 |             fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 22478848 |      1 |         4.2917 |    5237.8053 |             5237.8053 |     32 |          Y 
  2 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 26492928 |      1 |         3.5384 |    7487.1947 |             7487.1947 |     32 |          Y 
  3 |   fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu |  8028160 |      1 |         1.6067 |    4996.6660 |             4996.6660 |     32 |          Y 
  4 |                     fused_nn_contrib_conv2d_NCHWc_add | 13045760 |      1 |         2.2824 |    5715.7741 |             5715.7741 |     32 |          Y 
  5 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_2 | 40943616 |      1 |         7.0611 |    5798.4614 |             5798.4614 |     32 |          Y 
  6 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_1 |  6021120 |      1 |         0.9264 |    6499.1787 |             6499.1787 |     32 |          Y 
  7 |                   fused_nn_contrib_conv2d_NCHWc_add_1 | 14525952 |      1 |         2.7241 |    5332.3105 |             5332.3105 |     32 |          Y 
  8 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_2 |  9031680 |      1 |         1.8224 |    4956.0518 |             4956.0518 |     32 |          Y 
  9 |                 fused_nn_contrib_conv2d_NCHWc_add_add | 21826560 |      1 |         4.0055 |    5449.1942 |             5449.1942 |     32 |          Y 
 10 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_3 | 22579200 |      2 |         3.0645 |    7367.9645 |            14735.9291 |     32 |          Y 
 11 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_3 |  2257920 |      1 |         0.5061 |    4461.2053 |             4461.2053 |     32 |          Y 
 12 |                   fused_nn_contrib_conv2d_NCHWc_add_2 |  7250432 |      1 |         1.6952 |    4277.0565 |             4277.0565 |     32 |          Y 
 13 |               fused_nn_contrib_conv2d_NCHWc_add_add_1 |  9683968 |      2 |         5.7644 |    1679.9666 |             3359.9331 |     32 |          Y 
 14 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4 |  9934848 |      3 |         2.3071 |    4306.2159 |            12918.6478 |     32 |          Y 
 15 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_4 |  3010560 |      3 |         0.6774 |    4444.1565 |            13332.4695 |     32 |          Y 
 16 |                   fused_nn_contrib_conv2d_NCHWc_add_3 | 19317760 |      1 |         6.1981 |    3116.7444 |             3116.7444 |     32 |          Y 
 17 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_5 |  6021120 |      3 |         0.8778 |    6859.0097 |            20577.0290 |     32 |          Y 
 18 |               fused_nn_contrib_conv2d_NCHWc_add_add_2 | 38635520 |      3 |         5.4349 |    7108.7962 |            21326.3886 |     32 |          Y 
 19 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_5 | 39137280 |      4 |         7.0789 |    5528.7536 |            22115.0143 |     32 |          Y 
 20 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_6 |  1505280 |      1 |         0.6022 |    2499.7188 |             2499.7188 |     32 |          Y 
 21 |                   fused_nn_contrib_conv2d_NCHWc_add_4 | 14469504 |      1 |         2.5459 |    5683.4099 |             5683.4099 |     32 |          Y 
 22 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_7 |  2257920 |      2 |         0.5031 |    4488.3353 |             8976.6706 |     32 |          Y 
 23 |               fused_nn_contrib_conv2d_NCHWc_add_add_3 | 21713664 |      2 |         4.3908 |    4945.2193 |             9890.4387 |     32 |          Y 
 24 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_6 | 21901824 |      3 |        12.0904 |    1811.5031 |             5434.5093 |     32 |          Y 
 25 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_8 |   564480 |      1 |         6.8149 |      82.8297 |               82.8297 |     32 |          Y 
 26 |                   fused_nn_contrib_conv2d_NCHWc_add_5 |  9039520 |      1 |         2.3388 |    3865.0929 |             3865.0929 |     32 |          Y 
 27 |               fused_nn_contrib_conv2d_NCHWc_add_add_4 | 15068480 |      2 |         8.0876 |    1863.1693 |             3726.3385 |     32 |          Y 
 28 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_7 | 15146880 |      3 |         2.6844 |    5642.5033 |            16927.5099 |     32 |          Y 
 29 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_nn_relu_9 |   940800 |      3 |         0.2028 |    4639.1699 |            13917.5097 |     32 |          Y 
 30 |                   fused_nn_contrib_conv2d_NCHWc_add_6 | 30121280 |      1 |         3.9126 |    7698.6213 |             7698.6213 |     32 |          Y 
 31 |           fused_nn_contrib_conv2d_NCHWc_add_nn_relu_8 | 40266240 |      1 |         6.7441 |    5970.6216 |             5970.6216 |     32 |          Y 
 32 |                            fused_nn_global_avg_pool2d |    64000 |      1 |         1.1501 |      55.6462 |               55.6462 |     32 |          Y 
 33 |                         fused_nn_contrib_conv2d_NCHWc |  2560000 |      1 |        10.2749 |     249.1515 |              249.1515 |     32 |          Y 
 34 |                        fused_layout_transform_reshape |        1 |      1 |         0.0000 |     273.7661 |              273.7661 |     12 |          Y 
--------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1072
Total latency (us): 256987

[03:41:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_layout_transform"
[03:41:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:41:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 4 candidate(s) from database
[03:41:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:41:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2044 candidate(s)
[03:41:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:42:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:42:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:42:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55acde7c81a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x55acde7c9ce8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x55acde7c48a8)]: 0 failure(s)
[03:42:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:42:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:42:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:42:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #0 has finished. Remaining task(s): 0
Starting to build with relay.
/home/yj/anaconda3/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
