nohup: ignoring input
{'name': 'data', 'dtype': 'float32', 'shape': [1, 3, 112, 112]}
Starting to build with relay.
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #0: "fused_nn_conv2d_add"
[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 7, 7, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(224, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(5408):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 169)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 169 // 13)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(4096):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + ax0_ax1_ax2_ax3_fused // 32)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i1_4)
                                    yy = T.axis.spatial(7, i2_4)
                                    xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    rc = T.axis.reduce(256, i4_0 * 32 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 32, 1, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 16, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #1: "fused_nn_conv2d_add_1"
[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 14, 14, 128, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(224, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(10368):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused % 10368 // 81)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax0_ax1_ax2_ax3_fused % 81 // 27)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 27)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(32768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused // 128)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused % 128)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 4, 2, 1, 8, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + i2_3)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14)
                                    rc = T.axis.reduce(128, i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 16, 4, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 16, 8])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #2: "fused_nn_conv2d_add_2"
[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(98, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2970):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2970 // 1485)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax0_ax1_ax2_ax3_fused % 1485 // 55)
                                    v3 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 55)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + ax0_ax1_ax2_ax3_fused // 2)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 32, 2, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 32 + i1_4)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + i2_4)
                                    xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 2, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 32 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 32])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #3: "fused_nn_conv2d_add_3"
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 56, 56, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 64, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(788544):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 12321)
                                    v2 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused % 12321 // 111)
                                    v3 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused % 111)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(256):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax0_ax1_ax2_ax3_fused // 64)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 4, 1, 2, 2, 1, 1, 1, 1, 1, 14):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 4 + i1_3)
                                    yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i3_3 * 14 + i3_4)
                                    rc = T.axis.reduce(64, i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 28):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax2)
                                v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 1, 1, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 28, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 32, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #4: "fused_copy_subtract_multiply"
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 112, 112), "float32"], placeholder_1: T.Buffer[(1,), "float32"], placeholder_2: T.Buffer[(1,), "float32"], T_multiply: T.Buffer[(1, 3, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_identity = T.alloc_buffer([1, 3, 112, 112], dtype="float32")
        T_subtract = T.alloc_buffer([1, 3, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 112, 112):
            with T.block("T_identity"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_identity[ax0, ax1, ax2, ax3])
                T_identity[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 3, 112, 112):
            with T.block("T_subtract"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_identity[ax0, ax1, ax2, ax3], placeholder_1[0])
                T.writes(T_subtract[ax0, ax1, ax2, ax3])
                T_subtract[ax0, ax1, ax2, ax3] = T_identity[ax0, ax1, ax2, ax3] - placeholder_1[0]
        for i0, i1, i2, i3 in T.grid(1, 3, 112, 112):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_subtract[ax0, ax1, ax2, ax3], placeholder_2[0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = T_subtract[ax0, ax1, ax2, ax3] * placeholder_2[0]
    

[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 112, 112), "float32"], placeholder_1: T.Buffer[(1,), "float32"], placeholder_2: T.Buffer[(1,), "float32"], T_multiply: T.Buffer[(1, 3, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 3, 112, 112):
                with T.block("T_identity"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[0], placeholder_2[0])
                    T.writes(T_multiply[ax0, ax1, ax2, ax3])
                    T_multiply[ax0, ax1, ax2, ax3] = (placeholder[ax0, ax1, ax2, ax3] - placeholder_1[0]) * placeholder_2[0]
    

b0 = sch.get_block(name="T_subtract", func_name="main")
b1 = sch.get_block(name="T_multiply", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #5: "fused_nn_conv2d_add_4"
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 114, 114], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 114, 114):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 112, 112, 3, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 112, 112], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 3, 114, 114], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(12996):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(114, ax0_ax1_ax2_ax3_fused // 114)
                                    v3 = T.axis.spatial(114, ax0_ax1_ax2_ax3_fused % 114)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(576):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 4, 8, 2, 1, 1, 1, 1, 2, 14, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(112, i2_3 * 14 + i2_4)
                                    xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 * 14 + i3_3 * 7 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 112, 112], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 112, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + ax1)
                                v2 = T.axis.spatial(112, ax2)
                                v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 8, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 8, 14])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 8, 2, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #6: "fused_add"
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax1, 0, 0]
    

[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax1, 0, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #7: "fused_nn_conv2d_add_5"
[14:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 114, 114], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 114, 114):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 112, 112, 64, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(207872):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 207872 // 6496)
                                    v2 = T.axis.spatial(114, i5_0 + ax0_ax1_ax2_ax3_fused % 6496 // 58)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ax0_ax1_ax2_ax3_fused % 58)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(3072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + ax0_ax1_ax2_ax3_fused // 96)
                                    v1 = T.axis.spatial(64, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 96 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 4, 1, 7, 32, 1, 1, 1, 4, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 16 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(64, i4_0 * 32 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 16 + ax1)
                                v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 + ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 2, 4, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 16, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 4, 1, 7, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 1, 32])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #8: "fused_reshape_nn_prelu_reshape"
[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(802816,), "float32"], T_reshape: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([802816], dtype="float32")
        T_prelu = T.alloc_buffer([802816], dtype="float32")
        for i0 in T.serial(802816):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(802816, i0)
                T.reads(placeholder[0, ax0 % 802816 // 12544, ax0 % 12544 // 112, ax0 % 112])
                T.writes(T_reshape_1[ax0])
                T_reshape_1[ax0] = placeholder[0, ax0 % 802816 // 12544, ax0 % 12544 // 112, ax0 % 112]
        for i0 in T.serial(802816):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(802816, i0)
                T.reads(T_reshape_1[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape_1[ax0], T_reshape_1[ax0], T_reshape_1[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 12544 + ax2 * 112 + ax3) % 802816])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 12544 + ax2 * 112 + ax3) % 802816]
    

[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(802816,), "float32"], T_reshape: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, (ax1 * 12544 + ax2 * 112 + ax3) % 802816 // 12544, (ax2 * 112 + ax3) % 12544 // 112, ax3 % 112], placeholder_1[(ax1 * 12544 + ax2 * 112 + ax3) % 802816])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < placeholder[0, (ax1 * 12544 + ax2 * 112 + ax3) % 802816 % 802816 // 12544, (ax1 * 12544 + ax2 * 112 + ax3) % 802816 % 12544 // 112, (ax1 * 12544 + ax2 * 112 + ax3) % 802816 % 112], placeholder[0, (ax1 * 12544 + ax2 * 112 + ax3) % 802816 % 802816 // 12544, (ax1 * 12544 + ax2 * 112 + ax3) % 802816 % 12544 // 112, (ax1 * 12544 + ax2 * 112 + ax3) % 802816 % 112], placeholder[0, (ax1 * 12544 + ax2 * 112 + ax3) % 802816 % 802816 // 12544, (ax1 * 12544 + ax2 * 112 + ax3) % 802816 % 12544 // 112, (ax1 * 12544 + ax2 * 112 + ax3) % 802816 % 112] * placeholder_1[(ax1 * 12544 + ax2 * 112 + ax3) % 802816])
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_prelu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #9: "fused_nn_conv2d_add_add"
[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 114, 114], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 114, 114):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 56, 56, 64, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(404928):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused % 404928 // 6327)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ax0_ax1_ax2_ax3_fused % 6327 // 111)
                                    v3 = T.axis.spatial(114, i6_0 + ax0_ax1_ax2_ax3_fused % 111)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(6144):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + ax0_ax1_ax2_ax3_fused // 192)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused % 192 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 3, 1, 1, 4, 1, 2, 8, 1, 1, 1, 1, 4, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i1_3)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i2_4)
                                    xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                    rc = T.axis.reduce(64, i4_1 * 8 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax2)
                                v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 1, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 8, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #10: "fused_nn_conv2d_add_6"
[14:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 58, 58], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 58, 58):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 56, 56, 64, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1440):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 1440 // 180)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + ax0_ax1_ax2_ax3_fused % 180 // 6)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + ax0_ax1_ax2_ax3_fused % 6)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(4608):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 72)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 72 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 1, 1, 16, 1, 1, 2, 1, 3, 1, 1, 7, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 16 + i1_3)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_4)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4)
                                    rc = T.axis.reduce(64, i4_0 * 8 + i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 7, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 16 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 2, 16, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 2, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 4, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #11: "fused_reshape_nn_prelu_reshape_1"
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(200704,), "float32"], T_reshape: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([200704], dtype="float32")
        T_prelu = T.alloc_buffer([200704], dtype="float32")
        for i0 in T.serial(200704):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(200704, i0)
                T.reads(placeholder[0, ax0 % 200704 // 3136, ax0 % 3136 // 56, ax0 % 56])
                T.writes(T_reshape_1[ax0])
                T_reshape_1[ax0] = placeholder[0, ax0 % 200704 // 3136, ax0 % 3136 // 56, ax0 % 56]
        for i0 in T.serial(200704):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(200704, i0)
                T.reads(T_reshape_1[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape_1[ax0], T_reshape_1[ax0], T_reshape_1[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 3136 + ax2 * 56 + ax3) % 200704])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 3136 + ax2 * 56 + ax3) % 200704]
    

[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(200704,), "float32"], T_reshape: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, (ax1 * 3136 + ax2 * 56 + ax3) % 200704 // 3136, (ax2 * 56 + ax3) % 3136 // 56, ax3 % 56], placeholder_1[(ax1 * 3136 + ax2 * 56 + ax3) % 200704])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < placeholder[0, (ax1 * 3136 + ax2 * 56 + ax3) % 200704 % 200704 // 3136, (ax1 * 3136 + ax2 * 56 + ax3) % 200704 % 3136 // 56, (ax1 * 3136 + ax2 * 56 + ax3) % 200704 % 56], placeholder[0, (ax1 * 3136 + ax2 * 56 + ax3) % 200704 % 200704 // 3136, (ax1 * 3136 + ax2 * 56 + ax3) % 200704 % 3136 // 56, (ax1 * 3136 + ax2 * 56 + ax3) % 200704 % 56], placeholder[0, (ax1 * 3136 + ax2 * 56 + ax3) % 200704 % 200704 // 3136, (ax1 * 3136 + ax2 * 56 + ax3) % 200704 % 3136 // 56, (ax1 * 3136 + ax2 * 56 + ax3) % 200704 % 56] * placeholder_1[(ax1 * 3136 + ax2 * 56 + ax3) % 200704])
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_prelu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #12: "fused_nn_conv2d_add_add_1"
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 58, 58], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 58, 58):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 56, 56, 64, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3364):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0)
                                    v2 = T.axis.spatial(58, ax0_ax1_ax2_ax3_fused // 58)
                                    v3 = T.axis.spatial(58, ax0_ax1_ax2_ax3_fused % 58)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(576):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(64, i4_0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 4, 7, 1, 1, 3, 1, 1, 4, 1, 56):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_3)
                                    xx, rc, ry, rx = T.axis.remap("SRRR", [i3_4, i4_0, i5_2, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 7, 56):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 16 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 7 + ax2)
                                v3 = T.axis.spatial(56, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 8, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 56])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #13: "fused_add_1"
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax0, ax1, 0, 0]
    

[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #14: "fused_nn_conv2d_add_7"
[14:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 58, 58], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 58, 58):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 56, 56, 64, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(28672):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused % 28672 // 448)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 2 * 14 + ax0_ax1_ax2_ax3_fused % 448 // 28)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i6_0 + ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(24576):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 192)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused % 192 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 3, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 14):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_4)
                                    rc = T.axis.reduce(64, i4_1 * 4 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 16, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #15: "fused_reshape_nn_prelu_reshape_2"
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(401408,), "float32"], T_reshape: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([401408], dtype="float32")
        T_prelu = T.alloc_buffer([401408], dtype="float32")
        for i0 in T.serial(401408):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(401408, i0)
                T.reads(placeholder[0, ax0 % 401408 // 3136, ax0 % 3136 // 56, ax0 % 56])
                T.writes(T_reshape_1[ax0])
                T_reshape_1[ax0] = placeholder[0, ax0 % 401408 // 3136, ax0 % 3136 // 56, ax0 % 56]
        for i0 in T.serial(401408):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(401408, i0)
                T.reads(T_reshape_1[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape_1[ax0], T_reshape_1[ax0], T_reshape_1[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 3136 + ax2 * 56 + ax3) % 401408])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 3136 + ax2 * 56 + ax3) % 401408]
    

[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(401408,), "float32"], T_reshape: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, (ax1 * 3136 + ax2 * 56 + ax3) % 401408 // 3136, (ax2 * 56 + ax3) % 3136 // 56, ax3 % 56], placeholder_1[(ax1 * 3136 + ax2 * 56 + ax3) % 401408])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < placeholder[0, (ax1 * 3136 + ax2 * 56 + ax3) % 401408 % 401408 // 3136, (ax1 * 3136 + ax2 * 56 + ax3) % 401408 % 3136 // 56, (ax1 * 3136 + ax2 * 56 + ax3) % 401408 % 56], placeholder[0, (ax1 * 3136 + ax2 * 56 + ax3) % 401408 % 401408 // 3136, (ax1 * 3136 + ax2 * 56 + ax3) % 401408 % 3136 // 56, (ax1 * 3136 + ax2 * 56 + ax3) % 401408 % 56], placeholder[0, (ax1 * 3136 + ax2 * 56 + ax3) % 401408 % 401408 // 3136, (ax1 * 3136 + ax2 * 56 + ax3) % 401408 % 3136 // 56, (ax1 * 3136 + ax2 * 56 + ax3) % 401408 % 56] * placeholder_1[(ax1 * 3136 + ax2 * 56 + ax3) % 401408])
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_prelu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #16: "fused_nn_conv2d_add_add_2"
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 128, 28, 28), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 58, 58], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 58, 58):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 128, 28, 28), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(7, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1682):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 1682 // 841)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + ax0_ax1_ax2_ax3_fused % 841 // 29)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax0_ax1_ax2_ax3_fused % 29)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1152):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + ax0_ax1_ax2_ax3_fused // 18)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 18 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 2, 3, 3, 1, 16, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 32 + i1_3 * 16 + i1_4)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 7 + i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14)
                                    rc = T.axis.reduce(128, i4_0 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 32 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 7 + i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 1, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 14, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #17: "fused_nn_conv2d_add_8"
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 30, 30], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 30, 30):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3360):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 3360 // 420)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i5_0 + ax0_ax1_ax2_ax3_fused % 420 // 30)
                                    v3 = T.axis.spatial(30, ax0_ax1_ax2_ax3_fused % 30)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1536):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + ax0_ax1_ax2_ax3_fused // 24)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 24 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 3, 1, 64, 2, 1, 2, 1, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i1_3)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i2_3 * 7 + i2_4)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused)
                                    rc = T.axis.reduce(128, i4_0 * 8 + i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 1, 64, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 4, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #18: "fused_reshape_nn_prelu_reshape_3"
[14:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(100352,), "float32"], T_reshape: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([100352], dtype="float32")
        T_prelu = T.alloc_buffer([100352], dtype="float32")
        for i0 in T.serial(100352):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(100352, i0)
                T.reads(placeholder[0, ax0 % 100352 // 784, ax0 % 784 // 28, ax0 % 28])
                T.writes(T_reshape_1[ax0])
                T_reshape_1[ax0] = placeholder[0, ax0 % 100352 // 784, ax0 % 784 // 28, ax0 % 28]
        for i0 in T.serial(100352):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(100352, i0)
                T.reads(T_reshape_1[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape_1[ax0], T_reshape_1[ax0], T_reshape_1[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 784 + ax2 * 28 + ax3) % 100352])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 784 + ax2 * 28 + ax3) % 100352]
    

[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(100352,), "float32"], T_reshape: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, (ax1 * 784 + ax2 * 28 + ax3) % 100352 // 784, (ax2 * 28 + ax3) % 784 // 28, ax3 % 28], placeholder_1[(ax1 * 784 + ax2 * 28 + ax3) % 100352])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < placeholder[0, (ax1 * 784 + ax2 * 28 + ax3) % 100352 % 100352 // 784, (ax1 * 784 + ax2 * 28 + ax3) % 100352 % 784 // 28, (ax1 * 784 + ax2 * 28 + ax3) % 100352 % 28], placeholder[0, (ax1 * 784 + ax2 * 28 + ax3) % 100352 % 100352 // 784, (ax1 * 784 + ax2 * 28 + ax3) % 100352 % 784 // 28, (ax1 * 784 + ax2 * 28 + ax3) % 100352 % 28], placeholder[0, (ax1 * 784 + ax2 * 28 + ax3) % 100352 % 100352 // 784, (ax1 * 784 + ax2 * 28 + ax3) % 100352 % 784 // 28, (ax1 * 784 + ax2 * 28 + ax3) % 100352 % 28] * placeholder_1[(ax1 * 784 + ax2 * 28 + ax3) % 100352])
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_prelu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #19: "fused_nn_conv2d_add_add_3"
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 128, 28, 28), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 30, 30], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 30, 30):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 128, 28, 28), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(15360):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused % 15360 // 120)
                                    v2 = T.axis.spatial(30, ax0_ax1_ax2_ax3_fused % 120 // 4)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused * 4 + i6_0 + ax0_ax1_ax2_ax3_fused % 4)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(49152):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 384)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused % 384 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 1, 32, 3, 1, 1, 4, 4, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused)
                                    rc = T.axis.reduce(128, i4_1 * 32 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 4, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + ax1)
                                v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 4, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #20: "fused_add_2"
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax0, ax1, 0, 0]
    

[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #21: "fused_nn_conv2d_add_9"
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 30, 30], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 30, 30):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 28, 28, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2160):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 2160 // 270)
                                    v2 = T.axis.spatial(30, ax0_ax1_ax2_ax3_fused % 270 // 9)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + ax0_ax1_ax2_ax3_fused % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(2304):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + ax0_ax1_ax2_ax3_fused // 72)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 72 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 3, 1, 1, 1, 1, 2, 3, 1, 1, 8, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i1_4)
                                    yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused * 7 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    rc = T.axis.reduce(128, i4_0 * 8 + i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + ax1)
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused * 7 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 4, 1, 1, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 4, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #22: "fused_reshape_nn_prelu_reshape_4"
[14:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(200704,), "float32"], T_reshape: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([200704], dtype="float32")
        T_prelu = T.alloc_buffer([200704], dtype="float32")
        for i0 in T.serial(200704):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(200704, i0)
                T.reads(placeholder[0, ax0 % 200704 // 784, ax0 % 784 // 28, ax0 % 28])
                T.writes(T_reshape_1[ax0])
                T_reshape_1[ax0] = placeholder[0, ax0 % 200704 // 784, ax0 % 784 // 28, ax0 % 28]
        for i0 in T.serial(200704):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(200704, i0)
                T.reads(T_reshape_1[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape_1[ax0], T_reshape_1[ax0], T_reshape_1[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 784 + ax2 * 28 + ax3) % 200704])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 784 + ax2 * 28 + ax3) % 200704]
    

[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(200704,), "float32"], T_reshape: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, (ax1 * 784 + ax2 * 28 + ax3) % 200704 // 784, (ax2 * 28 + ax3) % 784 // 28, ax3 % 28], placeholder_1[(ax1 * 784 + ax2 * 28 + ax3) % 200704])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < placeholder[0, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 200704 // 784, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 784 // 28, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 28], placeholder[0, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 200704 // 784, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 784 // 28, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 28], placeholder[0, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 200704 // 784, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 784 // 28, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 28] * placeholder_1[(ax1 * 784 + ax2 * 28 + ax3) % 200704])
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_prelu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #23: "fused_nn_conv2d_add_add_4"
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 256, 14, 14), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 30, 30], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 30, 30):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 14, 14, 256, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 256, 14, 14), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 256, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(200448):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused % 200448 // 783)
                                    v2 = T.axis.spatial(30, ax0_ax1_ax2_ax3_fused % 783 // 27)
                                    v3 = T.axis.spatial(30, i6_0 + ax0_ax1_ax2_ax3_fused % 27)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(196608):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused // 768)
                                    v1 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused % 768 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 16, 1, 1, 4, 3, 1, 1, 2, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 32 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2)
                                    xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                    rc = T.axis.reduce(256, i4_1 * 4 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 32 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 + ax2)
                                v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 2, 16, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 64, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #24: "fused_nn_conv2d_add_10"
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 16, 16], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 16, 16):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 14, 14, 256, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 256, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(28672):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 128 + ax0_ax1_ax2_ax3_fused % 28672 // 224)
                                    v2 = T.axis.spatial(16, i5_0 + ax0_ax1_ax2_ax3_fused % 224 // 16)
                                    v3 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 16)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1536):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax0_ax1_ax2_ax3_fused // 384)
                                    v1 = T.axis.spatial(256, i4_0 * 128 + ax0_ax1_ax2_ax3_fused % 384 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 3, 1, 2, 7, 1, 16, 1, 1, 1, 1, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 2 + i1_3)
                                    yy = T.axis.spatial(14, i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14)
                                    rc = T.axis.reduce(256, i4_0 * 128 + i4_1 * 16 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 2 + ax1)
                                v2 = T.axis.spatial(14, ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 2, 1, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 8, 16])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #25: "fused_reshape_nn_prelu_reshape_5"
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(50176,), "float32"], T_reshape: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([50176], dtype="float32")
        T_prelu = T.alloc_buffer([50176], dtype="float32")
        for i0 in T.serial(50176):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(50176, i0)
                T.reads(placeholder[0, ax0 % 50176 // 196, ax0 % 196 // 14, ax0 % 14])
                T.writes(T_reshape_1[ax0])
                T_reshape_1[ax0] = placeholder[0, ax0 % 50176 // 196, ax0 % 196 // 14, ax0 % 14]
        for i0 in T.serial(50176):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(50176, i0)
                T.reads(T_reshape_1[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape_1[ax0], T_reshape_1[ax0], T_reshape_1[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 196 + ax2 * 14 + ax3) % 50176])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 196 + ax2 * 14 + ax3) % 50176]
    

[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(50176,), "float32"], T_reshape: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, (ax1 * 196 + ax2 * 14 + ax3) % 50176 // 196, (ax2 * 14 + ax3) % 196 // 14, ax3 % 14], placeholder_1[(ax1 * 196 + ax2 * 14 + ax3) % 50176])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < placeholder[0, (ax1 * 196 + ax2 * 14 + ax3) % 50176 % 50176 // 196, (ax1 * 196 + ax2 * 14 + ax3) % 50176 % 196 // 14, (ax1 * 196 + ax2 * 14 + ax3) % 50176 % 14], placeholder[0, (ax1 * 196 + ax2 * 14 + ax3) % 50176 % 50176 // 196, (ax1 * 196 + ax2 * 14 + ax3) % 50176 % 196 // 14, (ax1 * 196 + ax2 * 14 + ax3) % 50176 % 14], placeholder[0, (ax1 * 196 + ax2 * 14 + ax3) % 50176 % 50176 // 196, (ax1 * 196 + ax2 * 14 + ax3) % 50176 % 196 // 14, (ax1 * 196 + ax2 * 14 + ax3) % 50176 % 14] * placeholder_1[(ax1 * 196 + ax2 * 14 + ax3) % 50176])
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_prelu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #26: "fused_nn_conv2d_add_add_5"
[14:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 256, 14, 14), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 16, 16], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 16, 16):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 14, 14, 256, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 256, 14, 14), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 256, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(4032):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 4032 // 126)
                                    v2 = T.axis.spatial(16, i5_0 + ax0_ax1_ax2_ax3_fused % 126 // 9)
                                    v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax0_ax1_ax2_ax3_fused % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(24576):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused // 96)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 96 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 16, 1, 1, 2, 1, 3, 1, 2, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i3_4)
                                    rc = T.axis.reduce(256, i4_0 * 32 + i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 32 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 16, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #27: "fused_add_3"
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax0, ax1, 0, 0]
    

[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #28: "fused_nn_conv2d_add_11"
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 16, 16], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 16, 16):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 14, 14, 256, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1792):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 1792 // 56)
                                    v2 = T.axis.spatial(16, i5_0 + ax0_ax1_ax2_ax3_fused % 56 // 4)
                                    v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax0_ax1_ax2_ax3_fused % 4)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(49152):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused // 96)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 96 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 7, 1, 16, 1, 3, 1, 8, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i1_3 * 8 + i1_4)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 7 + i2_3)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                    rc = T.axis.reduce(256, i4_0 * 32 + i4_1 * 16 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 32, 1, 2, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 2, 16])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #29: "fused_reshape_nn_prelu_reshape_6"
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(100352,), "float32"], T_reshape: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([100352], dtype="float32")
        T_prelu = T.alloc_buffer([100352], dtype="float32")
        for i0 in T.serial(100352):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(100352, i0)
                T.reads(placeholder[0, ax0 % 100352 // 196, ax0 % 196 // 14, ax0 % 14])
                T.writes(T_reshape_1[ax0])
                T_reshape_1[ax0] = placeholder[0, ax0 % 100352 // 196, ax0 % 196 // 14, ax0 % 14]
        for i0 in T.serial(100352):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(100352, i0)
                T.reads(T_reshape_1[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape_1[ax0], T_reshape_1[ax0], T_reshape_1[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 196 + ax2 * 14 + ax3) % 100352])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 196 + ax2 * 14 + ax3) % 100352]
    

[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(100352,), "float32"], T_reshape: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, (ax1 * 196 + ax2 * 14 + ax3) % 100352 // 196, (ax2 * 14 + ax3) % 196 // 14, ax3 % 14], placeholder_1[(ax1 * 196 + ax2 * 14 + ax3) % 100352])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < placeholder[0, (ax1 * 196 + ax2 * 14 + ax3) % 100352 % 100352 // 196, (ax1 * 196 + ax2 * 14 + ax3) % 100352 % 196 // 14, (ax1 * 196 + ax2 * 14 + ax3) % 100352 % 14], placeholder[0, (ax1 * 196 + ax2 * 14 + ax3) % 100352 % 100352 // 196, (ax1 * 196 + ax2 * 14 + ax3) % 100352 % 196 // 14, (ax1 * 196 + ax2 * 14 + ax3) % 100352 % 14], placeholder[0, (ax1 * 196 + ax2 * 14 + ax3) % 100352 % 100352 // 196, (ax1 * 196 + ax2 * 14 + ax3) % 100352 % 196 // 14, (ax1 * 196 + ax2 * 14 + ax3) % 100352 % 14] * placeholder_1[(ax1 * 196 + ax2 * 14 + ax3) % 100352])
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_prelu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #30: "fused_nn_conv2d_add_add_6"
[14:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 16, 16], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 16, 16):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 7, 7, 512, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 3, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6656):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused % 6656 // 13)
                                    v2 = T.axis.spatial(16, i5_0 + ax0_ax1_ax2_ax3_fused % 13)
                                    v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i6_0 + 0)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(131072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + ax0_ax1_ax2_ax3_fused // 512)
                                    v1 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused % 512)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 7, 1, 32, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(7, i2_3)
                                    xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    rc = T.axis.reduce(512, i4_1 * 32 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 16, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 16, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #31: "fused_nn_conv2d_add_add_7"
[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 9, 9], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 9, 9):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 7, 7, 512, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 9, 9], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2304):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 2304 // 9)
                                    v2 = T.axis.spatial(9, ax0_ax1_ax2_ax3_fused % 9)
                                    v3 = T.axis.spatial(9, i6_0 + i0_0_i1_0_i2_0_i3_0_fused % 7 + 0)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(49152):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + ax0_ax1_ax2_ax3_fused // 768)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 768 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 3, 1, 1, 8, 1, 1, 4, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                    xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    rc = T.axis.reduce(512, i4_0 * 256 + i4_1 * 4 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 32 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                                v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 2, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 64, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #32: "fused_add_4"
[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax0, ax1, 0, 0]
    

[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #33: "fused_nn_conv2d_add_12"
[14:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 9, 9], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 9, 9):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 7, 7, 512, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 9, 9], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(768):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 768 // 3)
                                    v2 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused // 7 + ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(9, i6_0 + i0_0_i1_0_i2_0_i3_0_fused % 7 + 0)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(393216):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused // 768)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 768 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 64, 1, 1, 64, 3, 1, 1, 8, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i1_3 * 8 + i1_4)
                                    yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused // 7)
                                    xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    rc = T.axis.reduce(512, i4_0 * 256 + i4_1 * 64 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 512, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused // 7 + ax2)
                                v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 64, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 4, 64])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #34: "fused_reshape_nn_prelu_reshape_7"
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(25088,), "float32"], T_reshape: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([25088], dtype="float32")
        T_prelu = T.alloc_buffer([25088], dtype="float32")
        for i0 in T.serial(25088):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(25088, i0)
                T.reads(placeholder[0, ax0 % 25088 // 49, ax0 % 49 // 7, ax0 % 7])
                T.writes(T_reshape_1[ax0])
                T_reshape_1[ax0] = placeholder[0, ax0 % 25088 // 49, ax0 % 49 // 7, ax0 % 7]
        for i0 in T.serial(25088):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(25088, i0)
                T.reads(T_reshape_1[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape_1[ax0], T_reshape_1[ax0], T_reshape_1[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 49 + ax2 * 7 + ax3) % 25088])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 49 + ax2 * 7 + ax3) % 25088]
    

[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(25088,), "float32"], T_reshape: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, (ax1 * 49 + ax2 * 7 + ax3) % 25088 // 49, (ax2 * 7 + ax3) % 49 // 7, ax3 % 7], placeholder_1[(ax1 * 49 + ax2 * 7 + ax3) % 25088])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < placeholder[0, (ax1 * 49 + ax2 * 7 + ax3) % 25088 % 25088 // 49, (ax1 * 49 + ax2 * 7 + ax3) % 25088 % 49 // 7, (ax1 * 49 + ax2 * 7 + ax3) % 25088 % 7], placeholder[0, (ax1 * 49 + ax2 * 7 + ax3) % 25088 % 25088 // 49, (ax1 * 49 + ax2 * 7 + ax3) % 25088 % 49 // 7, (ax1 * 49 + ax2 * 7 + ax3) % 25088 % 7], placeholder[0, (ax1 * 49 + ax2 * 7 + ax3) % 25088 % 25088 // 49, (ax1 * 49 + ax2 * 7 + ax3) % 25088 % 49 // 7, (ax1 * 49 + ax2 * 7 + ax3) % 25088 % 7] * placeholder_1[(ax1 * 49 + ax2 * 7 + ax3) % 25088])
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_prelu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #35: "fused_nn_conv2d_add_add_multiply_add"
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_4: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_5: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 9, 9], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        T_add_2 = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        T_multiply = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 9, 9):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 7, 7, 512, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add_2[ax0, ax1, ax2, ax3])
                T_add_2[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_2[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = T_add_2[ax0, ax1, ax2, ax3] * placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add_2"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_5[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_5[ax0, ax1, 0, 0]
    

[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_4: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_5: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 9, 9], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(16128):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 16128 // 63)
                                    v2 = T.axis.spatial(9, ax0_ax1_ax2_ax3_fused % 63 // 7)
                                    v3 = T.axis.spatial(9, i6_0 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(12288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + ax0_ax1_ax2_ax3_fused // 768)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 768 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(128, 3, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                    yy = T.axis.spatial(7, i2_4)
                                    xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(512, i4_0 * 256 + i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3], placeholder_4[v0, v1, 0, 0], placeholder_5[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]) * placeholder_4[v0, v1, 0, 0] + placeholder_5[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add_2", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[32, 4, 2, 2, 1])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[2, 128, 2])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #36: "fused_nn_batch_flatten"
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], tensor: T.Buffer[(1, 25088), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 25088):
            with T.block("tensor"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[ax0, ax1 % 25088 // 49, ax1 % 49 // 7, ax1 % 7])
                T.writes(tensor[ax0, ax1])
                tensor[ax0, ax1] = placeholder[ax0, ax1 % 25088 // 49, ax1 % 49 // 7, ax1 % 7]
    

[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], tensor: T.Buffer[(1, 25088), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1 in T.grid(1, 25088):
                with T.block("tensor"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[ax0, ax1 % 25088 // 49, ax1 % 49 // 7, ax1 % 7])
                    T.writes(tensor[ax0, ax1])
                    tensor[ax0, ax1] = placeholder[ax0, ax1 % 25088 // 49, ax1 % 49 // 7, ax1 % 7]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #37: "fused_nn_dense_add"
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 512], dtype="float32")
        for i0, i1, i2 in T.grid(1, 512, 25088):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 512):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            T_matmul_NT_local = T.alloc_buffer([1, 512], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([1, 25088], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([512, 25088], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i2_0 in T.serial(28):
                            for ax0_ax1_fused in T.serial(896):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(25088, i2_0 * 896 + ax0_ax1_fused)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(114688):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_fused * 128 + ax0_ax1_fused // 896)
                                    v1 = T.axis.spatial(25088, i2_0 * 896 + ax0_ax1_fused % 896)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(64, 1, 16, 14, 1, 2):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(1, 0)
                                    j = T.axis.spatial(512, i0_0_i1_0_fused * 128 + i0_1_i1_1_fused * 32 + i1_3 * 2 + i1_4)
                                    k = T.axis.reduce(25088, i2_0 * 896 + i2_1 * 14 + i2_2)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(1, 32):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_fused * 128 + i0_1_i1_1_fused * 32 + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[4, 4, 1, 16, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[28, 64, 14])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                            fused_add |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[14:58:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_conv2d_add"
[14:58:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:59:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [14:59:10] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000056092d682bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [14:59:13] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000558cf9c09bc2
  96: 0xffffffffffffffff


[14:59:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_conv2d_add_1"
[14:59:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:59:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [14:59:28] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x00005602ffe17bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [14:59:30] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055773aa37bc2
  96: 0xffffffffffffffff


[14:59:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_nn_conv2d_add_2"
[14:59:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:00:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:00:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_nn_conv2d_add_3"
[15:00:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:00:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:00:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_copy_subtract_multiply"
[15:00:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:00:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:00:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_conv2d_add_4"
[15:00:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:01:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:01:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_add"
[15:01:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:01:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:01:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_conv2d_add_5"
[15:01:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:02:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:02:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_reshape_nn_prelu_reshape"
[15:02:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:02:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:02:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_conv2d_add_add"
[15:02:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:03:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:03:22] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055d89f921bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:03:23] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x00005584cf0cfbc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:03:27] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000559f22d00bc2
  96: 0xffffffffffffffff


[15:03:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_nn_conv2d_add_6"
[15:03:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:04:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:04:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_reshape_nn_prelu_reshape_1"
[15:04:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:04:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_conv2d_add_add_1"
[15:04:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:05:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_add_1"
[15:05:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:05:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_nn_conv2d_add_7"
[15:05:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:05:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:05:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_reshape_nn_prelu_reshape_2"
[15:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:06:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:06:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_nn_conv2d_add_add_2"
[15:06:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:06:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:06:39] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055bf37fcdbc2
  96: 0xffffffffffffffff


[15:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_nn_conv2d_add_8"
[15:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:07:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:07:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_reshape_nn_prelu_reshape_3"
[15:07:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:07:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:07:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_nn_conv2d_add_add_3"
[15:07:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:08:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:08:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_add_2"
[15:08:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:08:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:08:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_nn_conv2d_add_9"
[15:08:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:09:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:09:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_reshape_nn_prelu_reshape_4"
[15:09:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:09:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:09:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_nn_conv2d_add_add_4"
[15:09:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:09:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:10:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_nn_conv2d_add_10"
[15:10:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:10:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:10:38] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055e395f5cbc2
  96: 0xffffffffffffffff


[15:10:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_reshape_nn_prelu_reshape_5"
[15:10:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:10:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:10:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #26: "fused_nn_conv2d_add_add_5"
[15:10:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:11:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:11:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #27: "fused_add_3"
[15:11:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:11:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:11:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #28: "fused_nn_conv2d_add_11"
[15:11:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:11:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:12:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #29: "fused_reshape_nn_prelu_reshape_6"
[15:12:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:12:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:12:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #30: "fused_nn_conv2d_add_add_6"
[15:12:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:12:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:12:41] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055c423054bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:12:45] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055dc0ff88bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:12:47] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055844506bbc2
  96: 0xffffffffffffffff


[15:12:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #31: "fused_nn_conv2d_add_add_7"
[15:12:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:13:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:13:15] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000559594305bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:13:17] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055f586b2bbc2
  96: 0xffffffffffffffff


[15:13:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #32: "fused_add_4"
[15:13:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:13:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:13:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #33: "fused_nn_conv2d_add_12"
[15:13:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:13:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #34: "fused_reshape_nn_prelu_reshape_7"
[15:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:14:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #35: "fused_nn_conv2d_add_add_multiply_add"
[15:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:14:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:14:57] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055f0c37a9bc2
  96: 0xffffffffffffffff


[15:15:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #36: "fused_nn_batch_flatten"
[15:15:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:15:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:15:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #37: "fused_nn_dense_add"
[15:15:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:15:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:15:34] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055c96eee2bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:15:36] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x00005609d559bbc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:15:38] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000558007aa3bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:15:42] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x00005636d6b90bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:15:46] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000056040994dbc2
  96: 0xffffffffffffffff


[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #0: GFLOPs: 28.7172. Time: 0.4482 ms. Best GFLOPs: 28.7172
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #1: GFLOPs: 245.5833. Time: 0.0524 ms. Best GFLOPs: 245.5833
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #2: GFLOPs: 10.6073. Time: 1.2133 ms. Best GFLOPs: 245.5833
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #3: GFLOPs: 67.0675. Time: 0.1919 ms. Best GFLOPs: 245.5833
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #4: GFLOPs: 20.3964. Time: 0.6310 ms. Best GFLOPs: 245.5833
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #5: GFLOPs: 103.1439. Time: 0.1248 ms. Best GFLOPs: 245.5833
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #6: GFLOPs: 85.6686. Time: 0.1502 ms. Best GFLOPs: 245.5833
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #7: GFLOPs: 544.4771. Time: 0.0236 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #8: GFLOPs: 15.4340. Time: 0.8339 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #9: GFLOPs: 283.8979. Time: 0.0453 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #10: GFLOPs: 4.3016. Time: 2.9920 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #11: GFLOPs: 253.9378. Time: 0.0507 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #12: GFLOPs: 215.7957. Time: 0.0596 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #13: GFLOPs: 116.9852. Time: 0.1100 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #14: GFLOPs: 6.4842. Time: 1.9849 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #15: GFLOPs: 463.7012. Time: 0.0278 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #16: GFLOPs: 312.1871. Time: 0.0412 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add"] Trial #17: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(112, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_4_init in T.serial(7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            xx = T.axis.spatial(7, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 13)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 169)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(256, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                xx, rc = T.axis.remap("SR", [i3_4, i4_0])
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 16, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[256, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 32, 4])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #18: GFLOPs: 387.0131. Time: 0.0333 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #19: GFLOPs: 111.2440. Time: 0.1157 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #20: GFLOPs: 382.0218. Time: 0.0337 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #21: GFLOPs: 87.4256. Time: 0.1472 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #22: GFLOPs: 233.9639. Time: 0.0550 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #23: GFLOPs: 86.5362. Time: 0.1487 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #24: GFLOPs: 89.6399. Time: 0.1436 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #25: GFLOPs: 210.8390. Time: 0.0610 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #26: GFLOPs: 539.9314. Time: 0.0238 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #27: GFLOPs: 469.5737. Time: 0.0274 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #28: GFLOPs: 151.5471. Time: 0.0849 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #29: GFLOPs: 406.2685. Time: 0.0317 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add"] Trial #30: GFLOPs: 173.4611. Time: 0.0742 ms. Best GFLOPs: 544.4771
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add"] Trial #31: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(2, 4, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            xx = T.axis.spatial(7, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 13)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 169)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(256, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                xx, rc = T.axis.remap("SR", [i3_4, i4_0])
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [512, 256, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 32, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[256, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 32, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
/home/yj/anaconda3/lib/python3.9/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
[15:15:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_conv2d_add"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                            fused_add |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 23.6376

[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #0: GFLOPs: 225.2003. Time: 0.0573 ms. Best GFLOPs: 225.2003
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #1: GFLOPs: 18.3713. Time: 0.7019 ms. Best GFLOPs: 225.2003
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #2: GFLOPs: 170.5516. Time: 0.0756 ms. Best GFLOPs: 225.2003
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #3: GFLOPs: 381.3600. Time: 0.0338 ms. Best GFLOPs: 381.3600
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #4: GFLOPs: 17.0333. Time: 0.7571 ms. Best GFLOPs: 381.3600
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #5: GFLOPs: 323.0332. Time: 0.0399 ms. Best GFLOPs: 381.3600
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #6: GFLOPs: 600.2065. Time: 0.0215 ms. Best GFLOPs: 600.2065
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_1"] Trial #7: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 729)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 729 // 27)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1458)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 7, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 64, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 128, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 128, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #8: GFLOPs: 170.1828. Time: 0.0758 ms. Best GFLOPs: 600.2065
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #9: GFLOPs: 356.5604. Time: 0.0362 ms. Best GFLOPs: 600.2065
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #10: GFLOPs: 374.1063. Time: 0.0345 ms. Best GFLOPs: 600.2065
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #11: GFLOPs: 1064.6872. Time: 0.0121 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #12: GFLOPs: 430.0583. Time: 0.0300 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #13: GFLOPs: 833.9851. Time: 0.0155 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_1"] Trial #14: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i2_4_init)
                            xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 6 // 3)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + ax0_ax1_ax2_ax3_fused_1 % 3)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 6)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i2_4)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 1, 1], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 32, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 2, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 32, 4])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #15: GFLOPs: 213.2212. Time: 0.0605 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #16: GFLOPs: 704.3585. Time: 0.0183 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #17: GFLOPs: 20.8948. Time: 0.6171 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #18: GFLOPs: 522.5920. Time: 0.0247 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #19: GFLOPs: 608.6580. Time: 0.0212 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #20: GFLOPs: 11.9918. Time: 1.0753 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #21: GFLOPs: 42.7092. Time: 0.3019 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #22: GFLOPs: 9.9621. Time: 1.2944 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #23: GFLOPs: 308.7279. Time: 0.0418 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #24: GFLOPs: 397.7741. Time: 0.0324 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #25: GFLOPs: 967.8518. Time: 0.0133 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #26: GFLOPs: 326.6150. Time: 0.0395 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #27: GFLOPs: 80.9604. Time: 0.1593 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #28: GFLOPs: 29.5098. Time: 0.4370 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #29: GFLOPs: 134.3653. Time: 0.0960 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #30: GFLOPs: 885.2509. Time: 0.0146 ms. Best GFLOPs: 1064.6872
[15:15:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_1"] Trial #31: GFLOPs: 161.8480. Time: 0.0797 ms. Best GFLOPs: 1064.6872
[15:15:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_conv2d_add_1"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                            fused_add |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 35.7494

[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #0: GFLOPs: 351.7195. Time: 0.0368 ms. Best GFLOPs: 351.7195
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #1: GFLOPs: 12.2285. Time: 1.0586 ms. Best GFLOPs: 351.7195
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #2: GFLOPs: 135.5879. Time: 0.0955 ms. Best GFLOPs: 351.7195
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #3: GFLOPs: 47.4957. Time: 0.2726 ms. Best GFLOPs: 351.7195
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #4: GFLOPs: 14.1818. Time: 0.9128 ms. Best GFLOPs: 351.7195
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #5: GFLOPs: 430.5255. Time: 0.0301 ms. Best GFLOPs: 430.5255
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #6: GFLOPs: 659.4272. Time: 0.0196 ms. Best GFLOPs: 659.4272
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #7: GFLOPs: 377.8212. Time: 0.0343 ms. Best GFLOPs: 659.4272
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #8: GFLOPs: 51.3556. Time: 0.2521 ms. Best GFLOPs: 659.4272
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #9: GFLOPs: 43.6490. Time: 0.2966 ms. Best GFLOPs: 659.4272
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #10: GFLOPs: 214.8725. Time: 0.0602 ms. Best GFLOPs: 659.4272
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #11: GFLOPs: 591.6234. Time: 0.0219 ms. Best GFLOPs: 659.4272
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #12: GFLOPs: 294.3803. Time: 0.0440 ms. Best GFLOPs: 659.4272
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #13: GFLOPs: 185.1710. Time: 0.0699 ms. Best GFLOPs: 659.4272
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #14: GFLOPs: 339.8518. Time: 0.0381 ms. Best GFLOPs: 659.4272
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #15: GFLOPs: 352.4177. Time: 0.0367 ms. Best GFLOPs: 659.4272
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #16: GFLOPs: 748.5780. Time: 0.0173 ms. Best GFLOPs: 748.5780
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #17: GFLOPs: 25.2056. Time: 0.5136 ms. Best GFLOPs: 748.5780
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #18: GFLOPs: 939.6574. Time: 0.0138 ms. Best GFLOPs: 939.6574
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #19: GFLOPs: 179.5078. Time: 0.0721 ms. Best GFLOPs: 939.6574
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #20: GFLOPs: 1094.7078. Time: 0.0118 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #21: GFLOPs: 135.9075. Time: 0.0953 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #22: GFLOPs: 199.1671. Time: 0.0650 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #23: GFLOPs: 479.3361. Time: 0.0270 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #24: GFLOPs: 39.5558. Time: 0.3273 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #25: GFLOPs: 346.6536. Time: 0.0373 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #26: GFLOPs: 580.1495. Time: 0.0223 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #27: GFLOPs: 790.5461. Time: 0.0164 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #28: GFLOPs: 327.0464. Time: 0.0396 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #29: GFLOPs: 374.0918. Time: 0.0346 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #30: GFLOPs: 480.1967. Time: 0.0270 ms. Best GFLOPs: 1094.7078
[15:15:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_2"] Trial #31: GFLOPs: 245.1140. Time: 0.0528 ms. Best GFLOPs: 1094.7078
[15:15:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_conv2d_add_2"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                            fused_add |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 47.5748

[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #0: GFLOPs: 10.3513. Time: 2.5012 ms. Best GFLOPs: 10.3513
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #1: GFLOPs: 76.9554. Time: 0.3364 ms. Best GFLOPs: 76.9554
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #2: GFLOPs: 441.0742. Time: 0.0587 ms. Best GFLOPs: 441.0742
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #3: GFLOPs: 280.6608. Time: 0.0922 ms. Best GFLOPs: 441.0742
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #4: GFLOPs: 227.3718. Time: 0.1139 ms. Best GFLOPs: 441.0742
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #5: GFLOPs: 974.5718. Time: 0.0266 ms. Best GFLOPs: 974.5718
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #6: GFLOPs: 663.5591. Time: 0.0390 ms. Best GFLOPs: 974.5718
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #7: GFLOPs: 9.5358. Time: 2.7151 ms. Best GFLOPs: 974.5718
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #8: GFLOPs: 1013.1241. Time: 0.0256 ms. Best GFLOPs: 1013.1241
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #9: GFLOPs: 351.2189. Time: 0.0737 ms. Best GFLOPs: 1013.1241
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #10: GFLOPs: 1407.7860. Time: 0.0184 ms. Best GFLOPs: 1407.7860
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #11: GFLOPs: 708.9740. Time: 0.0365 ms. Best GFLOPs: 1407.7860
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #12: GFLOPs: 58.8986. Time: 0.4396 ms. Best GFLOPs: 1407.7860
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #13: GFLOPs: 731.0970. Time: 0.0354 ms. Best GFLOPs: 1407.7860
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #14: GFLOPs: 231.7304. Time: 0.1117 ms. Best GFLOPs: 1407.7860
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #15: GFLOPs: 1648.1371. Time: 0.0157 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #16: GFLOPs: 52.4515. Time: 0.4936 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #17: GFLOPs: 641.9226. Time: 0.0403 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #18: GFLOPs: 47.2390. Time: 0.5481 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #19: GFLOPs: 76.7151. Time: 0.3375 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #20: GFLOPs: 140.1958. Time: 0.1847 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #21: GFLOPs: 254.8402. Time: 0.1016 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #22: GFLOPs: 136.8910. Time: 0.1891 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #23: GFLOPs: 365.8946. Time: 0.0708 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #24: GFLOPs: 615.8269. Time: 0.0420 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #25: GFLOPs: 73.9746. Time: 0.3500 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #26: GFLOPs: 341.3266. Time: 0.0759 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #27: GFLOPs: 188.0650. Time: 0.1377 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #28: GFLOPs: 925.5198. Time: 0.0280 ms. Best GFLOPs: 1648.1371
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #29: GFLOPs: 1696.9294. Time: 0.0153 ms. Best GFLOPs: 1696.9294
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #30: GFLOPs: 1144.5514. Time: 0.0226 ms. Best GFLOPs: 1696.9294
[15:15:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_3"] Trial #31: GFLOPs: 492.5951. Time: 0.0526 ms. Best GFLOPs: 1696.9294
[15:15:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_conv2d_add_3"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                            fused_add |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 62.8323

[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #0: GFLOPs: 18.5418. Time: 0.0041 ms. Best GFLOPs: 18.5418
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #1: GFLOPs: 19.7272. Time: 0.0038 ms. Best GFLOPs: 19.7272
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #2: GFLOPs: 33.2316. Time: 0.0023 ms. Best GFLOPs: 33.2316
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #3: GFLOPs: 33.2247. Time: 0.0023 ms. Best GFLOPs: 33.2316
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #4: GFLOPs: 33.2507. Time: 0.0023 ms. Best GFLOPs: 33.2507
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #5: GFLOPs: 33.3074. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #6: GFLOPs: 33.2506. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #7: GFLOPs: 32.9570. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #8: GFLOPs: 33.1811. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #9: GFLOPs: 33.3044. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #10: GFLOPs: 30.3901. Time: 0.0025 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #11: GFLOPs: 33.3028. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #12: GFLOPs: 32.5303. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #13: GFLOPs: 33.2333. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #14: GFLOPs: 33.2457. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #15: GFLOPs: 32.4741. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #16: GFLOPs: 32.9482. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #17: GFLOPs: 32.6920. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #18: GFLOPs: 33.1609. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #19: GFLOPs: 32.6683. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #20: GFLOPs: 29.7477. Time: 0.0025 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #21: GFLOPs: 31.3087. Time: 0.0024 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #22: GFLOPs: 20.2686. Time: 0.0037 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #23: GFLOPs: 13.7284. Time: 0.0055 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #24: GFLOPs: 21.3965. Time: 0.0035 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #25: GFLOPs: 32.9205. Time: 0.0023 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #26: GFLOPs: 28.7969. Time: 0.0026 ms. Best GFLOPs: 33.3074
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #27: GFLOPs: 33.7783. Time: 0.0022 ms. Best GFLOPs: 33.7783
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #28: GFLOPs: 33.8536. Time: 0.0022 ms. Best GFLOPs: 33.8536
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #29: GFLOPs: 33.9418. Time: 0.0022 ms. Best GFLOPs: 33.9418
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #30: GFLOPs: 33.8358. Time: 0.0022 ms. Best GFLOPs: 33.9418
[15:15:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_copy_subtract_multiply"] Trial #31: GFLOPs: 33.8262. Time: 0.0022 ms. Best GFLOPs: 33.9418
[15:15:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_copy_subtract_multiply"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                            fused_add |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 65.0497

[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #0: GFLOPs: 1329.3275. Time: 0.0332 ms. Best GFLOPs: 1329.3275
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #1: GFLOPs: 160.2951. Time: 0.2755 ms. Best GFLOPs: 1329.3275
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #2: GFLOPs: 803.7250. Time: 0.0549 ms. Best GFLOPs: 1329.3275
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #3: GFLOPs: 1761.8256. Time: 0.0251 ms. Best GFLOPs: 1761.8256
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #4: GFLOPs: 6.6240. Time: 6.6659 ms. Best GFLOPs: 1761.8256
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #5: GFLOPs: 570.7824. Time: 0.0774 ms. Best GFLOPs: 1761.8256
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #6: GFLOPs: 154.0239. Time: 0.2867 ms. Best GFLOPs: 1761.8256
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #7: GFLOPs: 963.4234. Time: 0.0458 ms. Best GFLOPs: 1761.8256
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #8: GFLOPs: 74.9871. Time: 0.5888 ms. Best GFLOPs: 1761.8256
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #9: GFLOPs: 351.2443. Time: 0.1257 ms. Best GFLOPs: 1761.8256
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_4"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 4, 8, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 * 8 + i2_4_init)
                            xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 4 * 28 + i3_3_init * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 112, 112], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6612 // 114)
                                        v3 = T.axis.spatial(114, ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 114)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 6612)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 576)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 2, 1, 2, 1, 1, 3, 1, 4, 8, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 * 8 + i2_4)
                                xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 4 * 28 + i3_3 * 14 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 112, 112], "float32"], ["TENSOR", [64, 3, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 8, 28):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 * 8 + ax2)
                            v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 4 * 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 8, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 8])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 224])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #11: GFLOPs: 2113.6802. Time: 0.0209 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #12: GFLOPs: 50.8181. Time: 0.8689 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #13: GFLOPs: 166.0380. Time: 0.2659 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #14: GFLOPs: 614.5369. Time: 0.0719 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #15: GFLOPs: 48.7284. Time: 0.9061 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #16: GFLOPs: 739.0035. Time: 0.0597 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #17: GFLOPs: 162.3886. Time: 0.2719 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #18: GFLOPs: 190.4276. Time: 0.2319 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #19: GFLOPs: 1297.0917. Time: 0.0340 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #20: GFLOPs: 959.9952. Time: 0.0460 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #21: GFLOPs: 740.1174. Time: 0.0597 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #22: GFLOPs: 1357.5924. Time: 0.0325 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #23: GFLOPs: 121.4360. Time: 0.3636 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #24: GFLOPs: 21.0688. Time: 2.0957 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #25: GFLOPs: 771.2199. Time: 0.0573 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #26: GFLOPs: 91.0985. Time: 0.4847 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #27: GFLOPs: 1337.8698. Time: 0.0330 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #28: GFLOPs: 509.2473. Time: 0.0867 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #29: GFLOPs: 8.6947. Time: 5.0784 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #30: GFLOPs: 751.6484. Time: 0.0587 ms. Best GFLOPs: 2113.6802
[15:15:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_4"] Trial #31: GFLOPs: 794.1135. Time: 0.0556 ms. Best GFLOPs: 2113.6802
[15:15:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_conv2d_add_4"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 85.9398

[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #0: GFLOPs: 47.4768. Time: 0.0169 ms. Best GFLOPs: 47.4768
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #1: GFLOPs: 47.0159. Time: 0.0171 ms. Best GFLOPs: 47.4768
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #2: GFLOPs: 42.5939. Time: 0.0188 ms. Best GFLOPs: 47.4768
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #3: GFLOPs: 47.2454. Time: 0.0170 ms. Best GFLOPs: 47.4768
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #4: GFLOPs: 47.3515. Time: 0.0170 ms. Best GFLOPs: 47.4768
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #5: GFLOPs: 45.8411. Time: 0.0175 ms. Best GFLOPs: 47.4768
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #6: GFLOPs: 42.7463. Time: 0.0188 ms. Best GFLOPs: 47.4768
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #7: GFLOPs: 47.6574. Time: 0.0168 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #8: GFLOPs: 42.7548. Time: 0.0188 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #9: GFLOPs: 46.7257. Time: 0.0172 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #10: GFLOPs: 47.1325. Time: 0.0170 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #11: GFLOPs: 46.2455. Time: 0.0174 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #12: GFLOPs: 46.1304. Time: 0.0174 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #13: GFLOPs: 44.0041. Time: 0.0182 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #14: GFLOPs: 47.4416. Time: 0.0169 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #15: GFLOPs: 45.9177. Time: 0.0175 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #16: GFLOPs: 46.8333. Time: 0.0171 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #17: GFLOPs: 46.2326. Time: 0.0174 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #18: GFLOPs: 46.6652. Time: 0.0172 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #19: GFLOPs: 47.5815. Time: 0.0169 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #20: GFLOPs: 47.4905. Time: 0.0169 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #21: GFLOPs: 47.5169. Time: 0.0169 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #22: GFLOPs: 39.4032. Time: 0.0204 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #23: GFLOPs: 43.5869. Time: 0.0184 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #24: GFLOPs: 38.6139. Time: 0.0208 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #25: GFLOPs: 47.4076. Time: 0.0169 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #26: GFLOPs: 47.5016. Time: 0.0169 ms. Best GFLOPs: 47.6574
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #27: GFLOPs: 47.7108. Time: 0.0168 ms. Best GFLOPs: 47.7108
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #28: GFLOPs: 47.3289. Time: 0.0170 ms. Best GFLOPs: 47.7108
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #29: GFLOPs: 47.8639. Time: 0.0168 ms. Best GFLOPs: 47.8639
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #30: GFLOPs: 37.2332. Time: 0.0216 ms. Best GFLOPs: 47.8639
[15:15:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_add"] Trial #31: GFLOPs: 47.3400. Time: 0.0170 ms. Best GFLOPs: 47.8639
[15:16:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_add"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 224
Total latency (us): 102.713

[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #0: GFLOPs: 3648.2357. Time: 0.2537 ms. Best GFLOPs: 3648.2357
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #1: GFLOPs: 3315.4018. Time: 0.2792 ms. Best GFLOPs: 3648.2357
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_5"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(32, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 32 * 32 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8 * 7 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 7168 // 896)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 896 // 16)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 32, 1, 2, 1, 1, 1, 1, 1, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 32 * 32 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8 * 7 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 32 * 32 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8 * 7 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 2, 32, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 4, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 64])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #3: GFLOPs: 171.9298. Time: 5.3839 ms. Best GFLOPs: 3648.2357
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_5"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(8, 2, 2, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 8 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(60):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6720 // 3360)
                                        v2 = T.axis.spatial(114, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3360 // 30)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 8, 1, 2, 1, 1, 3, 1, 2, 8, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 8 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 8, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 16 + ax1)
                            v2 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 2, 8, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 8])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 7, 2, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_5"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 4 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(145):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 9280 // 580)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 580 // 58)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 58)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 144)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 144 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 1, 4, 3, 3, 1, 2, 4, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 4 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 1, 4, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 4])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 4, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[4, 4, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #6: GFLOPs: 1603.0634. Time: 0.5774 ms. Best GFLOPs: 3648.2357
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #7: GFLOPs: 5785.7549. Time: 0.1600 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #8: GFLOPs: 1268.2813. Time: 0.7298 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #9: GFLOPs: 565.7734. Time: 1.6361 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #10: GFLOPs: 16.0922. Time: 57.5214 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #11: GFLOPs: 5185.1537. Time: 0.1785 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #12: GFLOPs: 2718.2139. Time: 0.3405 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #13: GFLOPs: 4346.0707. Time: 0.2130 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #14: GFLOPs: 795.4729. Time: 1.1636 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #15: GFLOPs: 1109.5186. Time: 0.8343 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #16: GFLOPs: 1316.5862. Time: 0.7031 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #17: GFLOPs: 1219.8375. Time: 0.7588 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #18: GFLOPs: 4813.1281. Time: 0.1923 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #19: GFLOPs: 180.2139. Time: 5.1364 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #20: GFLOPs: 473.9064. Time: 1.9532 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #21: GFLOPs: 79.3553. Time: 11.6646 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #22: GFLOPs: 4401.9055. Time: 0.2103 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #23: GFLOPs: 1835.5451. Time: 0.5043 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #24: GFLOPs: 600.2159. Time: 1.5422 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #25: GFLOPs: 56.7989. Time: 16.2969 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #26: GFLOPs: 72.8325. Time: 12.7093 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_5"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i2_4_init, i3_4_init in T.grid(4, 2, 2, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(126):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8064 // 2016)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 2016 // 112)
                                        v3 = T.axis.spatial(114, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 112)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 2, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 4, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 4, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 2, 1, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 4, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #28: GFLOPs: 2664.5144. Time: 0.3474 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #29: GFLOPs: 75.4892. Time: 12.2620 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #30: GFLOPs: 1718.9533. Time: 0.5385 ms. Best GFLOPs: 5785.7549
[15:16:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_5"] Trial #31: GFLOPs: 55.3023. Time: 16.7379 ms. Best GFLOPs: 5785.7549
[15:16:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_conv2d_add_5"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |            N/A |          N/A |                   N/A |      0 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 256
Total latency (us): 262.7

[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #0: GFLOPs: 47.5614. Time: 0.0169 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #1: GFLOPs: 47.3562. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #2: GFLOPs: 47.3515. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #3: GFLOPs: 39.8170. Time: 0.0202 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #4: GFLOPs: 46.9095. Time: 0.0171 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #5: GFLOPs: 47.1957. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #6: GFLOPs: 42.6592. Time: 0.0188 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #7: GFLOPs: 46.6696. Time: 0.0172 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #8: GFLOPs: 42.5633. Time: 0.0189 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #9: GFLOPs: 42.1607. Time: 0.0190 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #10: GFLOPs: 47.0984. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #11: GFLOPs: 47.1528. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #12: GFLOPs: 47.0724. Time: 0.0171 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #13: GFLOPs: 47.3335. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #14: GFLOPs: 46.9777. Time: 0.0171 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #15: GFLOPs: 47.4205. Time: 0.0169 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #16: GFLOPs: 47.3164. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #17: GFLOPs: 46.5383. Time: 0.0173 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #18: GFLOPs: 47.1343. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #19: GFLOPs: 40.0691. Time: 0.0200 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #20: GFLOPs: 45.4316. Time: 0.0177 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #21: GFLOPs: 46.9931. Time: 0.0171 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #22: GFLOPs: 45.8458. Time: 0.0175 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #23: GFLOPs: 39.5723. Time: 0.0203 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #24: GFLOPs: 47.2247. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #25: GFLOPs: 46.7912. Time: 0.0172 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #26: GFLOPs: 46.7374. Time: 0.0172 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #27: GFLOPs: 47.1112. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #28: GFLOPs: 46.8641. Time: 0.0171 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #29: GFLOPs: 46.9165. Time: 0.0171 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #30: GFLOPs: 47.2698. Time: 0.0170 ms. Best GFLOPs: 47.5614
[15:16:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_nn_prelu_reshape"] Trial #31: GFLOPs: 46.2513. Time: 0.0174 ms. Best GFLOPs: 47.5614
[15:16:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_reshape_nn_prelu_reshape"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 288
Total latency (us): 296.459

[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #0: GFLOPs: 1890.3691. Time: 0.1225 ms. Best GFLOPs: 1890.3691
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #1: GFLOPs: 1209.9362. Time: 0.1914 ms. Best GFLOPs: 1890.3691
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #2: GFLOPs: 441.5778. Time: 0.5245 ms. Best GFLOPs: 1890.3691
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #3: GFLOPs: 2070.2343. Time: 0.1119 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #4: GFLOPs: 681.3831. Time: 0.3399 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_add"] Trial #5: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(392, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 98 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 98 // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(98):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6270 // 3135)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3135 // 55)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 55)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6270)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 98 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 98 // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 98 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 98 // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 14, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #6: GFLOPs: 98.1522. Time: 2.3597 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #7: GFLOPs: 62.3798. Time: 3.7129 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_add"] Trial #8: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(4, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 238 // 119)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 119 // 17)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 17)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 238)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 4, 1, 1, 2, 1, 1, 1, 2, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 4, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #9: GFLOPs: 1056.8734. Time: 0.2191 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #10: GFLOPs: 333.7245. Time: 0.6940 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #11: GFLOPs: 1601.7280. Time: 0.1446 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #12: GFLOPs: 120.9981. Time: 1.9142 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #13: GFLOPs: 628.4093. Time: 0.3686 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #14: GFLOPs: 617.2666. Time: 0.3752 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_add"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i2_3_init * 14 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(117):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 7480 // 935)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 935 // 17)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 17)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 7480)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 3, 1, 1, 2, 2, 1, 1, 1, 1, 2, 14, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i2_3 * 14 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 4, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #16: GFLOPs: 233.2114. Time: 0.9931 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_add"] Trial #17: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(392, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 196 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 196 // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(101):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6438 // 3219)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3219 // 111)
                                        v3 = T.axis.spatial(114, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 111)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6438)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 196 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 196 // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 196 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 196 // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 16, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 7, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 28, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #18: GFLOPs: 50.8614. Time: 4.5538 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #19: GFLOPs: 1495.5542. Time: 0.1549 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #20: GFLOPs: 169.3603. Time: 1.3676 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #21: GFLOPs: 789.2834. Time: 0.2934 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #22: GFLOPs: 1141.9655. Time: 0.2028 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #23: GFLOPs: 109.8200. Time: 2.1090 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #24: GFLOPs: 750.3342. Time: 0.3087 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_add"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(61):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 7684 // 1921)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1921 // 113)
                                        v3 = T.axis.spatial(114, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 113)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 7684)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 36)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 36 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 2, 2, 2, 3, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 112, 112], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 4, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 4, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #26: GFLOPs: 1691.5758. Time: 0.1369 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #27: GFLOPs: 1231.4754. Time: 0.1881 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #28: GFLOPs: 276.7610. Time: 0.8369 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #29: GFLOPs: 1905.0984. Time: 0.1216 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #30: GFLOPs: 1561.9767. Time: 0.1483 ms. Best GFLOPs: 2070.2343
[15:16:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_add"] Trial #31: GFLOPs: 708.7366. Time: 0.3268 ms. Best GFLOPs: 2070.2343
[15:16:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_conv2d_add_add"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 320
Total latency (us): 408.336

[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #0: GFLOPs: 217.0897. Time: 1.0660 ms. Best GFLOPs: 217.0897
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #1: GFLOPs: 910.7328. Time: 0.2541 ms. Best GFLOPs: 910.7328
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #2: GFLOPs: 177.3758. Time: 1.3046 ms. Best GFLOPs: 910.7328
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_6"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(2, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                            xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(232):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7424 // 464)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 464 // 58)
                                    v3 = T.axis.spatial(58, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 58)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 48)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 48 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 16, 1, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[4, 8, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 32])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #4: GFLOPs: 2639.7523. Time: 0.0877 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #5: GFLOPs: 10.7623. Time: 21.5020 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #6: GFLOPs: 161.7953. Time: 1.4303 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #7: GFLOPs: 24.7754. Time: 9.3404 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #8: GFLOPs: 40.2866. Time: 5.7441 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #9: GFLOPs: 982.2530. Time: 0.2356 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #10: GFLOPs: 33.9550. Time: 6.8152 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #11: GFLOPs: 1161.6643. Time: 0.1992 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #12: GFLOPs: 1308.2739. Time: 0.1769 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_6"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i2_4_init, i3_4_init in T.grid(4, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i2_3_init * 7 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1680 // 840)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 840 // 28)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1680)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 7, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i2_3 * 7 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 4, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 14, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #14: GFLOPs: 29.2890. Time: 7.9010 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #15: GFLOPs: 152.0380. Time: 1.5221 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #16: GFLOPs: 838.2077. Time: 0.2761 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #17: GFLOPs: 81.2362. Time: 2.8486 ms. Best GFLOPs: 2639.7523
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #18: GFLOPs: 3087.5985. Time: 0.0749 ms. Best GFLOPs: 3087.5985
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #19: GFLOPs: 49.0225. Time: 4.7205 ms. Best GFLOPs: 3087.5985
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #20: GFLOPs: 54.8811. Time: 4.2166 ms. Best GFLOPs: 3087.5985
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #21: GFLOPs: 1700.8738. Time: 0.1361 ms. Best GFLOPs: 3087.5985
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #22: GFLOPs: 157.2064. Time: 1.4720 ms. Best GFLOPs: 3087.5985
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #23: GFLOPs: 3444.0089. Time: 0.0672 ms. Best GFLOPs: 3444.0089
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_6"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 4, 8, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 8 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 + 0)
                                        v2 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1740 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1740)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(64, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 3, 3, 1, 4, 8, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 8 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 8, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 8 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 8 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 4, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 8])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 196, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 196, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #25: GFLOPs: 134.2019. Time: 1.7244 ms. Best GFLOPs: 3444.0089
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #26: GFLOPs: 1782.3300. Time: 0.1298 ms. Best GFLOPs: 3444.0089
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #27: GFLOPs: 345.0670. Time: 0.6706 ms. Best GFLOPs: 3444.0089
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #28: GFLOPs: 57.0043. Time: 4.0596 ms. Best GFLOPs: 3444.0089
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #29: GFLOPs: 339.4804. Time: 0.6817 ms. Best GFLOPs: 3444.0089
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #30: GFLOPs: 34.0489. Time: 6.7965 ms. Best GFLOPs: 3444.0089
[15:16:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_6"] Trial #31: GFLOPs: 70.8505. Time: 3.2662 ms. Best GFLOPs: 3444.0089
[15:16:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_conv2d_add_6"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 352
Total latency (us): 542.722

[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #0: GFLOPs: 32.2352. Time: 0.0062 ms. Best GFLOPs: 32.2352
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #1: GFLOPs: 60.0400. Time: 0.0033 ms. Best GFLOPs: 60.0400
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #2: GFLOPs: 62.2457. Time: 0.0032 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #3: GFLOPs: 61.4766. Time: 0.0033 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #4: GFLOPs: 40.2843. Time: 0.0050 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #5: GFLOPs: 51.3316. Time: 0.0039 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #6: GFLOPs: 62.0776. Time: 0.0032 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #7: GFLOPs: 47.5606. Time: 0.0042 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #8: GFLOPs: 39.6492. Time: 0.0051 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #9: GFLOPs: 48.7649. Time: 0.0041 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #10: GFLOPs: 37.1711. Time: 0.0054 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #11: GFLOPs: 32.2664. Time: 0.0062 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #12: GFLOPs: 37.0689. Time: 0.0054 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #13: GFLOPs: 56.5569. Time: 0.0035 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #14: GFLOPs: 33.5663. Time: 0.0060 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #15: GFLOPs: 30.4593. Time: 0.0066 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #16: GFLOPs: 30.5453. Time: 0.0066 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #17: GFLOPs: 29.0881. Time: 0.0069 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #18: GFLOPs: 61.0191. Time: 0.0033 ms. Best GFLOPs: 62.2457
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #19: GFLOPs: 62.5072. Time: 0.0032 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #20: GFLOPs: 61.3040. Time: 0.0033 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #21: GFLOPs: 61.7852. Time: 0.0032 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #22: GFLOPs: 62.1155. Time: 0.0032 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #23: GFLOPs: 55.4278. Time: 0.0036 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #24: GFLOPs: 42.9110. Time: 0.0047 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #25: GFLOPs: 28.7757. Time: 0.0070 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #26: GFLOPs: 34.2104. Time: 0.0059 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #27: GFLOPs: 60.5520. Time: 0.0033 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #28: GFLOPs: 61.4860. Time: 0.0033 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #29: GFLOPs: 43.9825. Time: 0.0046 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #30: GFLOPs: 61.4203. Time: 0.0033 ms. Best GFLOPs: 62.5072
[15:16:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_reshape_nn_prelu_reshape_1"] Trial #31: GFLOPs: 43.4611. Time: 0.0046 ms. Best GFLOPs: 62.5072
[15:16:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_reshape_nn_prelu_reshape_1"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 384
Total latency (us): 549.143

[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #0: GFLOPs: 756.0862. Time: 0.3063 ms. Best GFLOPs: 756.0862
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #1: GFLOPs: 783.8081. Time: 0.2955 ms. Best GFLOPs: 783.8081
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #2: GFLOPs: 5232.6718. Time: 0.0443 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #3: GFLOPs: 1728.9503. Time: 0.1340 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init, i3_4_init in T.grid(2, 8, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 8 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3480 // 1740)
                                        v2 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1740 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3480)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 8, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 8 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 8, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 8 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 8])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 4, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #5: GFLOPs: 2260.5570. Time: 0.1025 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #6: GFLOPs: 1314.3897. Time: 0.1762 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #7: GFLOPs: 2295.5374. Time: 0.1009 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #8: GFLOPs: 485.5723. Time: 0.4770 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #9: GFLOPs: 72.3952. Time: 3.1993 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i3_4_init in T.grid(2, 14, 14, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 6720 // 1680)
                                        v2 = T.axis.spatial(58, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1680 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6720)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 14, 14, 4, 1, 3, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 28):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 14, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 14, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 128, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #11: GFLOPs: 2688.5373. Time: 0.0861 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #12: GFLOPs: 2484.3246. Time: 0.0932 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #13: GFLOPs: 1619.5360. Time: 0.1430 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #14: GFLOPs: 18.8198. Time: 12.3068 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #15: GFLOPs: 160.6258. Time: 1.4419 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #16: GFLOPs: 1277.4547. Time: 0.1813 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #17: GFLOPs: 181.5116. Time: 1.2760 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #18: GFLOPs: 2198.4349. Time: 0.1054 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #19: GFLOPs: 36.6211. Time: 6.3246 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #20: GFLOPs: 67.3855. Time: 3.4371 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #21: GFLOPs: 2363.0117. Time: 0.0980 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #22: GFLOPs: 894.3993. Time: 0.2590 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(4, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_3_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(75):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8352 // 522)
                                        v2 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 522 // 9)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 8352)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 144)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 144 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 2, 1, 4, 3, 3, 1, 1, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_3)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 2, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 28, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #24: GFLOPs: 1228.3375. Time: 0.1886 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_4_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(34):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3712 // 464)
                                        v2 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 464 // 8)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3712)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 4, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #26: GFLOPs: 52.5487. Time: 4.4076 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #27: GFLOPs: 19.7988. Time: 11.6983 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 56, 56), "float32"], T_add: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1680 // 840)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 840 // 28)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1680)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) // 6)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 6 // 3)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1 < 384)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 2, 2, 1, 2, 1, 1, 1, 8, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [64, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 2, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 49, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 49])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #29: GFLOPs: 428.5835. Time: 0.5404 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #30: GFLOPs: 178.0498. Time: 1.3008 ms. Best GFLOPs: 5232.6718
[15:16:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #31: GFLOPs: 57.4769. Time: 4.0297 ms. Best GFLOPs: 5232.6718
[15:16:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_conv2d_add_add_1"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 416
Total latency (us): 637.669

[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #0: GFLOPs: 59.0423. Time: 0.0034 ms. Best GFLOPs: 59.0423
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #1: GFLOPs: 58.1229. Time: 0.0035 ms. Best GFLOPs: 59.0423
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #2: GFLOPs: 58.9130. Time: 0.0034 ms. Best GFLOPs: 59.0423
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #3: GFLOPs: 58.9699. Time: 0.0034 ms. Best GFLOPs: 59.0423
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #4: GFLOPs: 59.1578. Time: 0.0034 ms. Best GFLOPs: 59.1578
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #5: GFLOPs: 58.6594. Time: 0.0034 ms. Best GFLOPs: 59.1578
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #6: GFLOPs: 59.5484. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #7: GFLOPs: 58.4706. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #8: GFLOPs: 58.5858. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #9: GFLOPs: 59.3145. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #10: GFLOPs: 58.6588. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #11: GFLOPs: 56.8572. Time: 0.0035 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #12: GFLOPs: 56.8440. Time: 0.0035 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #13: GFLOPs: 56.2678. Time: 0.0036 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #14: GFLOPs: 58.8930. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #15: GFLOPs: 58.8586. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #16: GFLOPs: 59.1271. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #17: GFLOPs: 59.5126. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #18: GFLOPs: 58.5881. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #19: GFLOPs: 58.7653. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #20: GFLOPs: 58.9095. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #21: GFLOPs: 58.3659. Time: 0.0034 ms. Best GFLOPs: 59.5484
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #22: GFLOPs: 59.7624. Time: 0.0034 ms. Best GFLOPs: 59.7624
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #23: GFLOPs: 59.0636. Time: 0.0034 ms. Best GFLOPs: 59.7624
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #24: GFLOPs: 58.9079. Time: 0.0034 ms. Best GFLOPs: 59.7624
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #25: GFLOPs: 58.9379. Time: 0.0034 ms. Best GFLOPs: 59.7624
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #26: GFLOPs: 58.6437. Time: 0.0034 ms. Best GFLOPs: 59.7624
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #27: GFLOPs: 59.7196. Time: 0.0034 ms. Best GFLOPs: 59.7624
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #28: GFLOPs: 58.5091. Time: 0.0034 ms. Best GFLOPs: 59.7624
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #29: GFLOPs: 56.5833. Time: 0.0035 ms. Best GFLOPs: 59.7624
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #30: GFLOPs: 59.6432. Time: 0.0034 ms. Best GFLOPs: 59.7624
[15:16:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_add_1"] Trial #31: GFLOPs: 58.9296. Time: 0.0034 ms. Best GFLOPs: 59.7624
[15:16:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_add_1"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 448
Total latency (us): 647.744

[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #0: GFLOPs: 380.0878. Time: 1.2177 ms. Best GFLOPs: 380.0878
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #1: GFLOPs: 2307.5038. Time: 0.2006 ms. Best GFLOPs: 2307.5038
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #2: GFLOPs: 2474.7947. Time: 0.1870 ms. Best GFLOPs: 2474.7947
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #3: GFLOPs: 183.6994. Time: 2.5195 ms. Best GFLOPs: 2474.7947
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #4: GFLOPs: 3343.6344. Time: 0.1384 ms. Best GFLOPs: 3343.6344
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #5: GFLOPs: 3755.1823. Time: 0.1232 ms. Best GFLOPs: 3755.1823
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #6: GFLOPs: 2451.3301. Time: 0.1888 ms. Best GFLOPs: 3755.1823
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #7: GFLOPs: 1371.1754. Time: 0.3375 ms. Best GFLOPs: 3755.1823
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #8: GFLOPs: 30.5304. Time: 15.1594 ms. Best GFLOPs: 3755.1823
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #9: GFLOPs: 956.2130. Time: 0.4840 ms. Best GFLOPs: 3755.1823
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #10: GFLOPs: 114.6796. Time: 4.0358 ms. Best GFLOPs: 3755.1823
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #11: GFLOPs: 541.3330. Time: 0.8550 ms. Best GFLOPs: 3755.1823
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #12: GFLOPs: 191.4701. Time: 2.4172 ms. Best GFLOPs: 3755.1823
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #13: GFLOPs: 2529.8192. Time: 0.1829 ms. Best GFLOPs: 3755.1823
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #14: GFLOPs: 30.5721. Time: 15.1388 ms. Best GFLOPs: 3755.1823
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #15: GFLOPs: 3874.4383. Time: 0.1195 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #16: GFLOPs: 18.2044. Time: 25.4238 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #17: GFLOPs: 2492.9230. Time: 0.1857 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #18: GFLOPs: 804.1078. Time: 0.5756 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #19: GFLOPs: 3030.9001. Time: 0.1527 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_7"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 4)
                            xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_3_init * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(29):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 6496 // 3248)
                                    v2 = T.axis.spatial(58, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 3248 // 58)
                                    v3 = T.axis.spatial(58, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 58)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 4)
                                xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_3 * 7 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 4 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 16, 2, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 28, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 224])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 224, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_7"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init in T.grid(2, 4, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1920 // 240)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 240 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 3, 1, 2, 4, 7, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 16, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 2, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 4, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #22: GFLOPs: 3685.1298. Time: 0.1256 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #23: GFLOPs: 2347.6604. Time: 0.1971 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #24: GFLOPs: 141.8123. Time: 3.2636 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #25: GFLOPs: 715.1826. Time: 0.6471 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #26: GFLOPs: 123.6608. Time: 3.7427 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #27: GFLOPs: 42.4910. Time: 10.8923 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #28: GFLOPs: 1285.1809. Time: 0.3601 ms. Best GFLOPs: 3874.4383
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #29: GFLOPs: 3950.2134. Time: 0.1172 ms. Best GFLOPs: 3950.2134
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #30: GFLOPs: 24.8023. Time: 18.6605 ms. Best GFLOPs: 3950.2134
[15:16:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_7"] Trial #31: GFLOPs: 1890.8727. Time: 0.2448 ms. Best GFLOPs: 3950.2134
[15:16:18] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_conv2d_add_7"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 480
Total latency (us): 764.908

[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #0: GFLOPs: 79.0924. Time: 0.0051 ms. Best GFLOPs: 79.0924
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #1: GFLOPs: 83.9003. Time: 0.0048 ms. Best GFLOPs: 83.9003
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #2: GFLOPs: 84.1047. Time: 0.0048 ms. Best GFLOPs: 84.1047
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #3: GFLOPs: 92.2714. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #4: GFLOPs: 87.3279. Time: 0.0046 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #5: GFLOPs: 83.8101. Time: 0.0048 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #6: GFLOPs: 80.7387. Time: 0.0050 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #7: GFLOPs: 88.2620. Time: 0.0045 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #8: GFLOPs: 90.4808. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #9: GFLOPs: 91.2301. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #10: GFLOPs: 91.4845. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #11: GFLOPs: 91.1408. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #12: GFLOPs: 92.0776. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #13: GFLOPs: 87.7145. Time: 0.0046 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #14: GFLOPs: 82.5181. Time: 0.0049 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #15: GFLOPs: 82.9883. Time: 0.0048 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #16: GFLOPs: 83.2302. Time: 0.0048 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #17: GFLOPs: 88.7225. Time: 0.0045 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #18: GFLOPs: 79.5960. Time: 0.0050 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #19: GFLOPs: 86.3722. Time: 0.0046 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #20: GFLOPs: 89.4277. Time: 0.0045 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #21: GFLOPs: 91.8658. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #22: GFLOPs: 86.8881. Time: 0.0046 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #23: GFLOPs: 83.4665. Time: 0.0048 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #24: GFLOPs: 65.9983. Time: 0.0061 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #25: GFLOPs: 88.6961. Time: 0.0045 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #26: GFLOPs: 91.7125. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #27: GFLOPs: 87.4946. Time: 0.0046 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #28: GFLOPs: 91.9421. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #29: GFLOPs: 91.0695. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #30: GFLOPs: 88.8858. Time: 0.0045 ms. Best GFLOPs: 92.2714
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_nn_prelu_reshape_2"] Trial #31: GFLOPs: 91.8512. Time: 0.0044 ms. Best GFLOPs: 92.2714
[15:16:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_reshape_nn_prelu_reshape_2"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 512
Total latency (us): 769.258

[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #0: GFLOPs: 121.1740. Time: 1.9097 ms. Best GFLOPs: 121.1740
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #1: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 128, 28, 28), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(98, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 49 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 + 0)
                                    v2 = T.axis.spatial(58, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 1539 // 27)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused * 28 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 27)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 1539)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(128, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 8, 2, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 49 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 49 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #2: GFLOPs: 792.9214. Time: 0.2918 ms. Best GFLOPs: 792.9214
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #3: GFLOPs: 557.1164. Time: 0.4154 ms. Best GFLOPs: 792.9214
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #4: GFLOPs: 1352.4574. Time: 0.1711 ms. Best GFLOPs: 1352.4574
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #5: GFLOPs: 1709.2561. Time: 0.1354 ms. Best GFLOPs: 1709.2561
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #6: GFLOPs: 18.1688. Time: 12.7368 ms. Best GFLOPs: 1709.2561
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #7: GFLOPs: 69.1234. Time: 3.3478 ms. Best GFLOPs: 1709.2561
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #8: GFLOPs: 536.7992. Time: 0.4311 ms. Best GFLOPs: 1709.2561
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #9: GFLOPs: 820.8872. Time: 0.2819 ms. Best GFLOPs: 1709.2561
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #10: GFLOPs: 23.0178. Time: 10.0536 ms. Best GFLOPs: 1709.2561
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #11: GFLOPs: 455.3515. Time: 0.5082 ms. Best GFLOPs: 1709.2561
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #12: GFLOPs: 905.8330. Time: 0.2555 ms. Best GFLOPs: 1709.2561
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #13: GFLOPs: 1375.3390. Time: 0.1683 ms. Best GFLOPs: 1709.2561
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #14: GFLOPs: 34.5573. Time: 6.6965 ms. Best GFLOPs: 1709.2561
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #15: GFLOPs: 1803.1941. Time: 0.1283 ms. Best GFLOPs: 1803.1941
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #16: GFLOPs: 1293.2498. Time: 0.1789 ms. Best GFLOPs: 1803.1941
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #17: GFLOPs: 142.7500. Time: 1.6211 ms. Best GFLOPs: 1803.1941
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #18: GFLOPs: 1163.2938. Time: 0.1989 ms. Best GFLOPs: 1803.1941
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #19: GFLOPs: 789.6130. Time: 0.2931 ms. Best GFLOPs: 1803.1941
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #20: GFLOPs: 361.8000. Time: 0.6396 ms. Best GFLOPs: 1803.1941
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #21: GFLOPs: 1886.5939. Time: 0.1227 ms. Best GFLOPs: 1886.5939
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #22: GFLOPs: 276.1687. Time: 0.8379 ms. Best GFLOPs: 1886.5939
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #23: GFLOPs: 1916.3525. Time: 0.1208 ms. Best GFLOPs: 1916.3525
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #24: GFLOPs: 133.5882. Time: 1.7323 ms. Best GFLOPs: 1916.3525
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #25: GFLOPs: 283.4909. Time: 0.8163 ms. Best GFLOPs: 1916.3525
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #26: GFLOPs: 962.1861. Time: 0.2405 ms. Best GFLOPs: 1916.3525
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #27: GFLOPs: 389.2856. Time: 0.5945 ms. Best GFLOPs: 1916.3525
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #28: GFLOPs: 17.7736. Time: 13.0200 ms. Best GFLOPs: 1916.3525
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #29: GFLOPs: 62.2647. Time: 3.7166 ms. Best GFLOPs: 1916.3525
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #30: GFLOPs: 1258.4759. Time: 0.1839 ms. Best GFLOPs: 1916.3525
[15:16:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_add_2"] Trial #31: GFLOPs: 508.2122. Time: 0.4553 ms. Best GFLOPs: 1916.3525
[15:16:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_conv2d_add_add_2"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 544
Total latency (us): 890.015

[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #0: GFLOPs: 258.1610. Time: 0.8960 ms. Best GFLOPs: 258.1610
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #1: GFLOPs: 3126.5808. Time: 0.0740 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #2: GFLOPs: 2917.2107. Time: 0.0793 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #3: GFLOPs: 1600.3078. Time: 0.1445 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #4: GFLOPs: 1594.7212. Time: 0.1450 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #5: GFLOPs: 15.2571. Time: 15.1609 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #6: GFLOPs: 373.5442. Time: 0.6192 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #7: GFLOPs: 744.6792. Time: 0.3106 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #8: GFLOPs: 1499.5617. Time: 0.1543 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #9: GFLOPs: 64.4030. Time: 3.5916 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #10: GFLOPs: 2701.5622. Time: 0.0856 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #11: GFLOPs: 2112.1567. Time: 0.1095 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #12: GFLOPs: 2456.4104. Time: 0.0942 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #13: GFLOPs: 712.3956. Time: 0.3247 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #14: GFLOPs: 1692.7992. Time: 0.1366 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #15: GFLOPs: 431.8839. Time: 0.5356 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #16: GFLOPs: 636.3038. Time: 0.3635 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #17: GFLOPs: 538.2917. Time: 0.4297 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #18: GFLOPs: 249.4188. Time: 0.9274 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #19: GFLOPs: 557.7818. Time: 0.4147 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_8"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(98, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i3_4_init in T.grid(2, 8, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 8 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1680 // 420)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused * 14 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 420 // 30)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1680)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) // 12)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) % 12 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 < 1536)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 2, 1, 4, 1, 1, 1, 8, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 8 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 8, 2, 1, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 7, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 98, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 98])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_8"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 900)
                                        v2 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 900 // 30)
                                        v3 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3600)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 36)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 36 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 4608)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 3, 1, 2, 1, 7, 1, 3, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 32, 2, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 2, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 4, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #22: GFLOPs: 1116.7811. Time: 0.2071 ms. Best GFLOPs: 3126.5808
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #23: GFLOPs: 3129.3076. Time: 0.0739 ms. Best GFLOPs: 3129.3076
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #24: GFLOPs: 789.1179. Time: 0.2931 ms. Best GFLOPs: 3129.3076
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #25: GFLOPs: 66.0482. Time: 3.5022 ms. Best GFLOPs: 3129.3076
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #26: GFLOPs: 819.9740. Time: 0.2821 ms. Best GFLOPs: 3129.3076
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #27: GFLOPs: 248.3234. Time: 0.9315 ms. Best GFLOPs: 3129.3076
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #28: GFLOPs: 749.0608. Time: 0.3088 ms. Best GFLOPs: 3129.3076
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #29: GFLOPs: 948.5998. Time: 0.2438 ms. Best GFLOPs: 3129.3076
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_8"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init in T.grid(7, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 14 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 720 // 180)
                                        v2 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 180 // 6)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 720)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(144):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 36)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 36 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 7, 1, 4, 3, 1, 1, 8, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 14 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 14, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 14 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 4, 1, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:16:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_8"] Trial #31: GFLOPs: 2284.0805. Time: 0.1013 ms. Best GFLOPs: 3129.3076
[15:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_conv2d_add_8"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 576
Total latency (us): 1777.03

[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #0: GFLOPs: 43.2360. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #1: GFLOPs: 43.0555. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #2: GFLOPs: 42.9741. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #3: GFLOPs: 42.7869. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #4: GFLOPs: 41.7993. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #5: GFLOPs: 40.7421. Time: 0.0025 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #6: GFLOPs: 41.9345. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #7: GFLOPs: 42.7318. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #8: GFLOPs: 42.3096. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #9: GFLOPs: 42.2104. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #10: GFLOPs: 42.7780. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #11: GFLOPs: 42.5927. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #12: GFLOPs: 42.9450. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #13: GFLOPs: 42.3092. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #14: GFLOPs: 42.6524. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #15: GFLOPs: 42.6487. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #16: GFLOPs: 42.7487. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #17: GFLOPs: 42.4658. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #18: GFLOPs: 43.0467. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #19: GFLOPs: 42.1345. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #20: GFLOPs: 42.9405. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #21: GFLOPs: 42.4785. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #22: GFLOPs: 42.5261. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #23: GFLOPs: 42.7616. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #24: GFLOPs: 42.5453. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #25: GFLOPs: 42.5420. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #26: GFLOPs: 42.7627. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #27: GFLOPs: 42.9078. Time: 0.0023 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #28: GFLOPs: 42.5242. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #29: GFLOPs: 42.4822. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #30: GFLOPs: 42.6763. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_reshape_nn_prelu_reshape_3"] Trial #31: GFLOPs: 42.4032. Time: 0.0024 ms. Best GFLOPs: 43.2360
[15:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_reshape_nn_prelu_reshape_3"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 608
Total latency (us): 1804.88

[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #0: GFLOPs: 127.2098. Time: 1.8191 ms. Best GFLOPs: 127.2098
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #1: GFLOPs: 38.5926. Time: 5.9963 ms. Best GFLOPs: 127.2098
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #2: GFLOPs: 57.4718. Time: 4.0265 ms. Best GFLOPs: 127.2098
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #3: GFLOPs: 569.3609. Time: 0.4064 ms. Best GFLOPs: 569.3609
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #4: GFLOPs: 1278.0847. Time: 0.1811 ms. Best GFLOPs: 1278.0847
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #5: GFLOPs: 49.3687. Time: 4.6874 ms. Best GFLOPs: 1278.0847
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #6: GFLOPs: 2305.3725. Time: 0.1004 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #7: GFLOPs: 967.0769. Time: 0.2393 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #8: GFLOPs: 289.7933. Time: 0.7985 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #9: GFLOPs: 96.1343. Time: 2.4072 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #10: GFLOPs: 94.2875. Time: 2.4543 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 128, 28, 28), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i3_4_init in T.grid(4, 7, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 1568 // 392)
                                    v2 = T.axis.spatial(30, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 392 // 14)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 1568)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 7, 7, 4, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #12: GFLOPs: 101.6556. Time: 2.2764 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #13: GFLOPs: 212.7138. Time: 1.0879 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #14: GFLOPs: 904.0189. Time: 0.2560 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #15: GFLOPs: 39.0892. Time: 5.9201 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #16: GFLOPs: 207.1719. Time: 1.1170 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #17: GFLOPs: 197.3707. Time: 1.1725 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #18: GFLOPs: 55.3833. Time: 4.1784 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #19: GFLOPs: 124.0562. Time: 1.8654 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 128, 28, 28), "float32"], T_add: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(7, 2, 8, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i2_3_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i3_3_init * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(38):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 900)
                                        v2 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 900 // 30)
                                        v3 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 3600)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 36)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 36 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 1, 7, 2, 4, 1, 1, 1, 8, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i2_3)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i3_3 * 7 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 2, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #21: GFLOPs: 153.8407. Time: 1.5042 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #22: GFLOPs: 68.4521. Time: 3.3806 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #23: GFLOPs: 325.5360. Time: 0.7109 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #24: GFLOPs: 91.1710. Time: 2.5382 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #25: GFLOPs: 448.1836. Time: 0.5163 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #26: GFLOPs: 113.9405. Time: 2.0310 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #27: GFLOPs: 306.1084. Time: 0.7560 ms. Best GFLOPs: 2305.3725
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #28: GFLOPs: 2424.9949. Time: 0.0954 ms. Best GFLOPs: 2424.9949
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #29: GFLOPs: 1937.3438. Time: 0.1194 ms. Best GFLOPs: 2424.9949
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #30: GFLOPs: 239.8731. Time: 0.9647 ms. Best GFLOPs: 2424.9949
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_add_3"] Trial #31: GFLOPs: 1795.5405. Time: 0.1289 ms. Best GFLOPs: 2424.9949
[15:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_conv2d_add_add_3"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 640
Total latency (us): 2950.01

[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #0: GFLOPs: 40.1567. Time: 0.0025 ms. Best GFLOPs: 40.1567
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #1: GFLOPs: 40.6237. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #2: GFLOPs: 40.6026. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #3: GFLOPs: 40.2590. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #4: GFLOPs: 40.1630. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #5: GFLOPs: 39.1000. Time: 0.0026 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #6: GFLOPs: 39.6495. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #7: GFLOPs: 40.3052. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #8: GFLOPs: 40.2116. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #9: GFLOPs: 40.5280. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #10: GFLOPs: 40.5839. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #11: GFLOPs: 38.9048. Time: 0.0026 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #12: GFLOPs: 40.3248. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #13: GFLOPs: 40.1597. Time: 0.0025 ms. Best GFLOPs: 40.6237
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #14: GFLOPs: 40.7094. Time: 0.0025 ms. Best GFLOPs: 40.7094
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #15: GFLOPs: 40.4457. Time: 0.0025 ms. Best GFLOPs: 40.7094
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #16: GFLOPs: 40.6513. Time: 0.0025 ms. Best GFLOPs: 40.7094
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #17: GFLOPs: 40.3722. Time: 0.0025 ms. Best GFLOPs: 40.7094
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #18: GFLOPs: 40.7419. Time: 0.0025 ms. Best GFLOPs: 40.7419
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #19: GFLOPs: 40.6918. Time: 0.0025 ms. Best GFLOPs: 40.7419
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #20: GFLOPs: 40.7763. Time: 0.0025 ms. Best GFLOPs: 40.7763
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #21: GFLOPs: 40.3685. Time: 0.0025 ms. Best GFLOPs: 40.7763
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #22: GFLOPs: 40.6147. Time: 0.0025 ms. Best GFLOPs: 40.7763
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #23: GFLOPs: 40.5894. Time: 0.0025 ms. Best GFLOPs: 40.7763
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #24: GFLOPs: 40.8403. Time: 0.0025 ms. Best GFLOPs: 40.8403
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #25: GFLOPs: 40.4910. Time: 0.0025 ms. Best GFLOPs: 40.8403
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #26: GFLOPs: 40.5405. Time: 0.0025 ms. Best GFLOPs: 40.8403
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #27: GFLOPs: 40.6063. Time: 0.0025 ms. Best GFLOPs: 40.8403
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #28: GFLOPs: 40.6485. Time: 0.0025 ms. Best GFLOPs: 40.8403
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #29: GFLOPs: 40.6792. Time: 0.0025 ms. Best GFLOPs: 40.8403
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #30: GFLOPs: 40.8090. Time: 0.0025 ms. Best GFLOPs: 40.8403
[15:16:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_add_2"] Trial #31: GFLOPs: 40.2622. Time: 0.0025 ms. Best GFLOPs: 40.8403
[15:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_add_2"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 672
Total latency (us): 2981.96

[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #0: GFLOPs: 907.0552. Time: 0.5100 ms. Best GFLOPs: 907.0552
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #1: GFLOPs: 357.3666. Time: 1.2945 ms. Best GFLOPs: 907.0552
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #2: GFLOPs: 170.3588. Time: 2.7156 ms. Best GFLOPs: 907.0552
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #3: GFLOPs: 95.5228. Time: 4.8431 ms. Best GFLOPs: 907.0552
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #4: GFLOPs: 40.2775. Time: 11.4859 ms. Best GFLOPs: 907.0552
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #5: GFLOPs: 241.0923. Time: 1.9189 ms. Best GFLOPs: 907.0552
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #6: GFLOPs: 5640.4352. Time: 0.0820 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #7: GFLOPs: 26.5087. Time: 17.4518 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #8: GFLOPs: 339.0273. Time: 1.3646 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #9: GFLOPs: 1.6849. Time: 274.5625 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #10: GFLOPs: 160.9383. Time: 2.8745 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #11: GFLOPs: 82.0835. Time: 5.6360 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #12: GFLOPs: 10.7182. Time: 43.1623 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #13: GFLOPs: 121.8003. Time: 3.7982 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #14: GFLOPs: 1821.1021. Time: 0.2540 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #15: GFLOPs: 2546.1864. Time: 0.1817 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #16: GFLOPs: 234.9022. Time: 1.9694 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #17: GFLOPs: 34.1489. Time: 13.5472 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #18: GFLOPs: 302.4049. Time: 1.5298 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #19: GFLOPs: 4222.2104. Time: 0.1096 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #20: GFLOPs: 67.5261. Time: 6.8510 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #21: GFLOPs: 1564.5830. Time: 0.2957 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #22: GFLOPs: 108.8376. Time: 4.2506 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_9"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(98, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(113):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 900)
                                    v2 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 900 // 30)
                                    v3 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 30)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 3600)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(72):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 36)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 36 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 3, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 2, 8, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 2, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 4, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 32])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #24: GFLOPs: 1200.1988. Time: 0.3855 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #25: GFLOPs: 2190.2378. Time: 0.2112 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #26: GFLOPs: 144.0361. Time: 3.2119 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #27: GFLOPs: 389.0528. Time: 1.1891 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #28: GFLOPs: 1245.2404. Time: 0.3715 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_9"] Trial #29: GFLOPs: 1327.2156. Time: 0.3486 ms. Best GFLOPs: 5640.4352
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_9"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i3_4_init in T.grid(2, 2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_3_init * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 900)
                                    v2 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 900 // 30)
                                    v3 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 30)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 1800)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 2, 2, 2, 2, 3, 1, 1, 1, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_3 * 7 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 16, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 2, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 112])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 112, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:16:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_9"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i2_4_init, i3_4_init in T.grid(2, 14, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            yy = T.axis.spatial(28, i2_3_init * 14 + i2_4_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(113):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 900)
                                        v2 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 900 // 30)
                                        v3 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 7200)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 72)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 72 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 3, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 14, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(28, i2_3 * 14 + i2_4)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(28, ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 1, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 14])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 8, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:16:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_nn_conv2d_add_9"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 704
Total latency (us): 3063.97

[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #0: GFLOPs: 62.9219. Time: 0.0032 ms. Best GFLOPs: 62.9219
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #1: GFLOPs: 60.4067. Time: 0.0033 ms. Best GFLOPs: 62.9219
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #2: GFLOPs: 63.2187. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #3: GFLOPs: 63.1854. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #4: GFLOPs: 61.9501. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #5: GFLOPs: 60.0038. Time: 0.0033 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #6: GFLOPs: 61.1130. Time: 0.0033 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #7: GFLOPs: 60.0767. Time: 0.0033 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #8: GFLOPs: 56.6654. Time: 0.0035 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #9: GFLOPs: 62.4050. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #10: GFLOPs: 62.2878. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #11: GFLOPs: 62.9109. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #12: GFLOPs: 62.2616. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #13: GFLOPs: 63.0322. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #14: GFLOPs: 62.4575. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #15: GFLOPs: 60.9919. Time: 0.0033 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #16: GFLOPs: 52.5541. Time: 0.0038 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #17: GFLOPs: 35.3198. Time: 0.0057 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #18: GFLOPs: 50.2683. Time: 0.0040 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #19: GFLOPs: 62.2340. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #20: GFLOPs: 62.4224. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #21: GFLOPs: 61.6651. Time: 0.0033 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #22: GFLOPs: 50.6821. Time: 0.0040 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #23: GFLOPs: 61.6433. Time: 0.0033 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #24: GFLOPs: 35.2692. Time: 0.0057 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #25: GFLOPs: 62.5677. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #26: GFLOPs: 30.8453. Time: 0.0065 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #27: GFLOPs: 46.5883. Time: 0.0043 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #28: GFLOPs: 34.8590. Time: 0.0058 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #29: GFLOPs: 62.8254. Time: 0.0032 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #30: GFLOPs: 59.7304. Time: 0.0034 ms. Best GFLOPs: 63.2187
[15:16:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_reshape_nn_prelu_reshape_4"] Trial #31: GFLOPs: 60.5746. Time: 0.0033 ms. Best GFLOPs: 63.2187
[15:16:48] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_reshape_nn_prelu_reshape_4"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 736
Total latency (us): 3067.15

[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #0: GFLOPs: 775.8002. Time: 0.2982 ms. Best GFLOPs: 775.8002
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #1: GFLOPs: 937.1657. Time: 0.2468 ms. Best GFLOPs: 937.1657
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #2: GFLOPs: 1863.5128. Time: 0.1241 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #3: GFLOPs: 235.4379. Time: 0.9825 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #4: GFLOPs: 1045.7614. Time: 0.2212 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #5: GFLOPs: 163.4809. Time: 1.4149 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #6: GFLOPs: 688.0807. Time: 0.3362 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #7: GFLOPs: 18.1318. Time: 12.7573 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #8: GFLOPs: 9.7175. Time: 23.8035 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #9: GFLOPs: 450.1257. Time: 0.5139 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #10: GFLOPs: 179.4383. Time: 1.2891 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #11: GFLOPs: 61.0033. Time: 3.7918 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #12: GFLOPs: 996.6815. Time: 0.2321 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #13: GFLOPs: 1461.1173. Time: 0.1583 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #14: GFLOPs: 45.0570. Time: 5.1338 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #15: GFLOPs: 1146.5869. Time: 0.2017 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #16: GFLOPs: 360.5112. Time: 0.6416 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #17: GFLOPs: 118.2752. Time: 1.9557 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #18: GFLOPs: 1359.1674. Time: 0.1702 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #19: GFLOPs: 1343.0120. Time: 0.1722 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #20: GFLOPs: 62.4058. Time: 3.7066 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #21: GFLOPs: 699.5913. Time: 0.3306 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #22: GFLOPs: 631.2205. Time: 0.3665 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #23: GFLOPs: 69.7345. Time: 3.3170 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #24: GFLOPs: 113.5524. Time: 2.0370 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #25: GFLOPs: 728.7603. Time: 0.3174 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #26: GFLOPs: 634.9642. Time: 0.3643 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #27: GFLOPs: 469.8662. Time: 0.4923 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #28: GFLOPs: 109.2957. Time: 2.1164 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #29: GFLOPs: 94.6281. Time: 2.4444 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #30: GFLOPs: 1305.4948. Time: 0.1772 ms. Best GFLOPs: 1863.5128
[15:16:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_add_4"] Trial #31: GFLOPs: 677.2618. Time: 0.3415 ms. Best GFLOPs: 1863.5128
[15:16:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_conv2d_add_add_4"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 768
Total latency (us): 3191.28

[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #0: GFLOPs: 1267.7074. Time: 0.1824 ms. Best GFLOPs: 1267.7074
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #1: GFLOPs: 506.8571. Time: 0.4563 ms. Best GFLOPs: 1267.7074
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #2: GFLOPs: 697.3893. Time: 0.3316 ms. Best GFLOPs: 1267.7074
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #3: GFLOPs: 60.2556. Time: 3.8380 ms. Best GFLOPs: 1267.7074
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #4: GFLOPs: 603.4091. Time: 0.3833 ms. Best GFLOPs: 1267.7074
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #5: GFLOPs: 114.7718. Time: 2.0150 ms. Best GFLOPs: 1267.7074
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #6: GFLOPs: 1378.3952. Time: 0.1678 ms. Best GFLOPs: 1378.3952
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #7: GFLOPs: 49.6675. Time: 4.6562 ms. Best GFLOPs: 1378.3952
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #8: GFLOPs: 133.6383. Time: 1.7305 ms. Best GFLOPs: 1378.3952
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #9: GFLOPs: 331.3285. Time: 0.6980 ms. Best GFLOPs: 1378.3952
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #10: GFLOPs: 1021.5960. Time: 0.2264 ms. Best GFLOPs: 1378.3952
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_10"] Trial #11: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(256, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_add: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4_init)
                            xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 + 0)
                                        v2 = T.axis.spatial(16, i5_0 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 42 // 3)
                                        v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 42)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1, v2 = T.axis.remap("SS", [i4_0, i5_0])
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 2, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4)
                                xx, rc, ry, rx = T.axis.remap("SRRR", [i0_0_i1_0_i2_0_i3_0_fused, i4_0, i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [256, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 64, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[256, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #12: GFLOPs: 100.9040. Time: 2.2919 ms. Best GFLOPs: 1378.3952
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #13: GFLOPs: 1280.0739. Time: 0.1807 ms. Best GFLOPs: 1378.3952
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #14: GFLOPs: 1465.8126. Time: 0.1578 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #15: GFLOPs: 945.6321. Time: 0.2446 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #16: GFLOPs: 171.6490. Time: 1.3473 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #17: GFLOPs: 26.8615. Time: 8.6094 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #18: GFLOPs: 146.5291. Time: 1.5783 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #19: GFLOPs: 749.7967. Time: 0.3084 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #20: GFLOPs: 62.5000. Time: 3.7002 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #21: GFLOPs: 75.4717. Time: 3.0642 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #22: GFLOPs: 955.5131. Time: 0.2420 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #23: GFLOPs: 498.1734. Time: 0.4642 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #24: GFLOPs: 541.1078. Time: 0.4274 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #25: GFLOPs: 892.5708. Time: 0.2591 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #26: GFLOPs: 96.9864. Time: 2.3845 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #27: GFLOPs: 54.9675. Time: 4.2072 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #28: GFLOPs: 392.9541. Time: 0.5885 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #29: GFLOPs: 731.9092. Time: 0.3160 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #30: GFLOPs: 620.6333. Time: 0.3726 ms. Best GFLOPs: 1465.8126
[15:16:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_10"] Trial #31: GFLOPs: 90.4710. Time: 2.5562 ms. Best GFLOPs: 1465.8126
[15:16:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_conv2d_add_10"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 800
Total latency (us): 7766.6

[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #0: GFLOPs: 22.7148. Time: 0.0022 ms. Best GFLOPs: 22.7148
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #1: GFLOPs: 22.7169. Time: 0.0022 ms. Best GFLOPs: 22.7169
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #2: GFLOPs: 22.8618. Time: 0.0022 ms. Best GFLOPs: 22.8618
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #3: GFLOPs: 22.5817. Time: 0.0022 ms. Best GFLOPs: 22.8618
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #4: GFLOPs: 22.6857. Time: 0.0022 ms. Best GFLOPs: 22.8618
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #5: GFLOPs: 22.9766. Time: 0.0022 ms. Best GFLOPs: 22.9766
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #6: GFLOPs: 22.8667. Time: 0.0022 ms. Best GFLOPs: 22.9766
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #7: GFLOPs: 22.8889. Time: 0.0022 ms. Best GFLOPs: 22.9766
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #8: GFLOPs: 22.7925. Time: 0.0022 ms. Best GFLOPs: 22.9766
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #9: GFLOPs: 22.8794. Time: 0.0022 ms. Best GFLOPs: 22.9766
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #10: GFLOPs: 22.7305. Time: 0.0022 ms. Best GFLOPs: 22.9766
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #11: GFLOPs: 22.7909. Time: 0.0022 ms. Best GFLOPs: 22.9766
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #12: GFLOPs: 22.4946. Time: 0.0022 ms. Best GFLOPs: 22.9766
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #13: GFLOPs: 22.9033. Time: 0.0022 ms. Best GFLOPs: 22.9766
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #14: GFLOPs: 22.9206. Time: 0.0022 ms. Best GFLOPs: 22.9766
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #15: GFLOPs: 23.0176. Time: 0.0022 ms. Best GFLOPs: 23.0176
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #16: GFLOPs: 22.9011. Time: 0.0022 ms. Best GFLOPs: 23.0176
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #17: GFLOPs: 22.9174. Time: 0.0022 ms. Best GFLOPs: 23.0176
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #18: GFLOPs: 22.8318. Time: 0.0022 ms. Best GFLOPs: 23.0176
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #19: GFLOPs: 22.7416. Time: 0.0022 ms. Best GFLOPs: 23.0176
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #20: GFLOPs: 22.8024. Time: 0.0022 ms. Best GFLOPs: 23.0176
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #21: GFLOPs: 22.9057. Time: 0.0022 ms. Best GFLOPs: 23.0176
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #22: GFLOPs: 22.9856. Time: 0.0022 ms. Best GFLOPs: 23.0176
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #23: GFLOPs: 23.0513. Time: 0.0022 ms. Best GFLOPs: 23.0513
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #24: GFLOPs: 22.8616. Time: 0.0022 ms. Best GFLOPs: 23.0513
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #25: GFLOPs: 22.9664. Time: 0.0022 ms. Best GFLOPs: 23.0513
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #26: GFLOPs: 23.0206. Time: 0.0022 ms. Best GFLOPs: 23.0513
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #27: GFLOPs: 23.0604. Time: 0.0022 ms. Best GFLOPs: 23.0604
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #28: GFLOPs: 23.0944. Time: 0.0022 ms. Best GFLOPs: 23.0944
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #29: GFLOPs: 22.9576. Time: 0.0022 ms. Best GFLOPs: 23.0944
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #30: GFLOPs: 22.8606. Time: 0.0022 ms. Best GFLOPs: 23.0944
[15:16:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_nn_prelu_reshape_5"] Trial #31: GFLOPs: 23.0495. Time: 0.0022 ms. Best GFLOPs: 23.0944
[15:16:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_reshape_nn_prelu_reshape_5"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 832
Total latency (us): 7829.61

[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #0: GFLOPs: 102.2442. Time: 2.2623 ms. Best GFLOPs: 102.2442
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #1: GFLOPs: 92.0752. Time: 2.5122 ms. Best GFLOPs: 102.2442
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #2: GFLOPs: 3.0677. Time: 75.4014 ms. Best GFLOPs: 102.2442
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #3: GFLOPs: 297.2347. Time: 0.7782 ms. Best GFLOPs: 297.2347
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #4: GFLOPs: 254.6610. Time: 0.9083 ms. Best GFLOPs: 297.2347
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #5: GFLOPs: 199.9578. Time: 1.1568 ms. Best GFLOPs: 297.2347
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #6: GFLOPs: 122.6450. Time: 1.8860 ms. Best GFLOPs: 297.2347
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #7: GFLOPs: 344.6246. Time: 0.6712 ms. Best GFLOPs: 344.6246
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #8: GFLOPs: 16.7002. Time: 13.8508 ms. Best GFLOPs: 344.6246
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #9: GFLOPs: 793.4389. Time: 0.2915 ms. Best GFLOPs: 793.4389
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #10: GFLOPs: 105.4505. Time: 2.1936 ms. Best GFLOPs: 793.4389
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #11: GFLOPs: 94.7500. Time: 2.4413 ms. Best GFLOPs: 793.4389
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #12: GFLOPs: 664.4153. Time: 0.3481 ms. Best GFLOPs: 793.4389
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #13: GFLOPs: 437.9860. Time: 0.5281 ms. Best GFLOPs: 793.4389
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #14: GFLOPs: 8.3071. Time: 27.8449 ms. Best GFLOPs: 793.4389
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #15: GFLOPs: 1383.5467. Time: 0.1672 ms. Best GFLOPs: 1383.5467
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #16: GFLOPs: 8.6100. Time: 26.8655 ms. Best GFLOPs: 1383.5467
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #17: GFLOPs: 152.1053. Time: 1.5207 ms. Best GFLOPs: 1383.5467
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #18: GFLOPs: 365.7162. Time: 0.6325 ms. Best GFLOPs: 1383.5467
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #19: GFLOPs: 20.4501. Time: 11.3110 ms. Best GFLOPs: 1383.5467
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #20: GFLOPs: 1661.5130. Time: 0.1392 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #21: GFLOPs: 115.0953. Time: 2.0097 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #22: GFLOPs: 286.2847. Time: 0.8080 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #23: GFLOPs: 219.4284. Time: 1.0542 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #24: GFLOPs: 224.9901. Time: 1.0281 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #25: GFLOPs: 632.6519. Time: 0.3656 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #26: GFLOPs: 404.1277. Time: 0.5724 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #27: GFLOPs: 518.6267. Time: 0.4460 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #28: GFLOPs: 455.3356. Time: 0.5080 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #29: GFLOPs: 15.0541. Time: 15.3653 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #30: GFLOPs: 339.8211. Time: 0.6807 ms. Best GFLOPs: 1661.5130
[15:16:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_5"] Trial #31: GFLOPs: 606.5220. Time: 0.3814 ms. Best GFLOPs: 1661.5130
[15:16:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_conv2d_add_add_5"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 864
Total latency (us): 11866.9

[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #0: GFLOPs: 22.4630. Time: 0.0022 ms. Best GFLOPs: 22.4630
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #1: GFLOPs: 21.8592. Time: 0.0023 ms. Best GFLOPs: 22.4630
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #2: GFLOPs: 21.8376. Time: 0.0023 ms. Best GFLOPs: 22.4630
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #3: GFLOPs: 22.5064. Time: 0.0022 ms. Best GFLOPs: 22.5064
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #4: GFLOPs: 22.6774. Time: 0.0022 ms. Best GFLOPs: 22.6774
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #5: GFLOPs: 22.7110. Time: 0.0022 ms. Best GFLOPs: 22.7110
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #6: GFLOPs: 22.3495. Time: 0.0022 ms. Best GFLOPs: 22.7110
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #7: GFLOPs: 22.7467. Time: 0.0022 ms. Best GFLOPs: 22.7467
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #8: GFLOPs: 22.6355. Time: 0.0022 ms. Best GFLOPs: 22.7467
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #9: GFLOPs: 22.7093. Time: 0.0022 ms. Best GFLOPs: 22.7467
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #10: GFLOPs: 22.6873. Time: 0.0022 ms. Best GFLOPs: 22.7467
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #11: GFLOPs: 22.6127. Time: 0.0022 ms. Best GFLOPs: 22.7467
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #12: GFLOPs: 22.6587. Time: 0.0022 ms. Best GFLOPs: 22.7467
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #13: GFLOPs: 22.5728. Time: 0.0022 ms. Best GFLOPs: 22.7467
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #14: GFLOPs: 22.6973. Time: 0.0022 ms. Best GFLOPs: 22.7467
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #15: GFLOPs: 22.7164. Time: 0.0022 ms. Best GFLOPs: 22.7467
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #16: GFLOPs: 22.8432. Time: 0.0022 ms. Best GFLOPs: 22.8432
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #17: GFLOPs: 22.7331. Time: 0.0022 ms. Best GFLOPs: 22.8432
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #18: GFLOPs: 22.5424. Time: 0.0022 ms. Best GFLOPs: 22.8432
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #19: GFLOPs: 22.7283. Time: 0.0022 ms. Best GFLOPs: 22.8432
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #20: GFLOPs: 22.2837. Time: 0.0023 ms. Best GFLOPs: 22.8432
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #21: GFLOPs: 22.4249. Time: 0.0022 ms. Best GFLOPs: 22.8432
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #22: GFLOPs: 22.1942. Time: 0.0023 ms. Best GFLOPs: 22.8432
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #23: GFLOPs: 22.8911. Time: 0.0022 ms. Best GFLOPs: 22.8911
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #24: GFLOPs: 22.1682. Time: 0.0023 ms. Best GFLOPs: 22.8911
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #25: GFLOPs: 22.8663. Time: 0.0022 ms. Best GFLOPs: 22.8911
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #26: GFLOPs: 23.0536. Time: 0.0022 ms. Best GFLOPs: 23.0536
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #27: GFLOPs: 22.9386. Time: 0.0022 ms. Best GFLOPs: 23.0536
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #28: GFLOPs: 23.1100. Time: 0.0022 ms. Best GFLOPs: 23.1100
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #29: GFLOPs: 22.9625. Time: 0.0022 ms. Best GFLOPs: 23.1100
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #30: GFLOPs: 22.9135. Time: 0.0022 ms. Best GFLOPs: 23.1100
[15:16:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_add_3"] Trial #31: GFLOPs: 23.0025. Time: 0.0022 ms. Best GFLOPs: 23.1100
[15:16:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_add_3"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 896
Total latency (us): 11932

[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #0: GFLOPs: 525.3923. Time: 0.8803 ms. Best GFLOPs: 525.3923
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #1: GFLOPs: 39.0767. Time: 11.8363 ms. Best GFLOPs: 525.3923
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #2: GFLOPs: 643.7973. Time: 0.7184 ms. Best GFLOPs: 643.7973
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #3: GFLOPs: 1131.3427. Time: 0.4088 ms. Best GFLOPs: 1131.3427
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #4: GFLOPs: 2056.9623. Time: 0.2249 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #5: GFLOPs: 68.3233. Time: 6.7696 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #6: GFLOPs: 102.1591. Time: 4.5275 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #7: GFLOPs: 46.6472. Time: 9.9153 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #8: GFLOPs: 1167.4396. Time: 0.3962 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #9: GFLOPs: 1518.2688. Time: 0.3046 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #10: GFLOPs: 336.7216. Time: 1.3736 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #11: GFLOPs: 1060.7500. Time: 0.4360 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #12: GFLOPs: 812.2199. Time: 0.5695 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #13: GFLOPs: 583.6894. Time: 0.7924 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #14: GFLOPs: 1562.1529. Time: 0.2961 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #15: GFLOPs: 53.2527. Time: 8.6854 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #16: GFLOPs: 593.9734. Time: 0.7787 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #17: GFLOPs: 570.8515. Time: 0.8102 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #18: GFLOPs: 991.6871. Time: 0.4664 ms. Best GFLOPs: 2056.9623
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #19: GFLOPs: 2785.1256. Time: 0.1661 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #20: GFLOPs: 1349.6021. Time: 0.3427 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #21: GFLOPs: 836.3928. Time: 0.5530 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #22: GFLOPs: 956.0265. Time: 0.4838 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #23: GFLOPs: 527.7766. Time: 0.8764 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #24: GFLOPs: 2545.3687. Time: 0.1817 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #25: GFLOPs: 875.0807. Time: 0.5285 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #26: GFLOPs: 1319.7750. Time: 0.3505 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #27: GFLOPs: 2406.6695. Time: 0.1922 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #28: GFLOPs: 24.8763. Time: 18.5929 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #29: GFLOPs: 705.4514. Time: 0.6556 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #30: GFLOPs: 106.0141. Time: 4.3628 ms. Best GFLOPs: 2785.1256
[15:16:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_11"] Trial #31: GFLOPs: 162.3868. Time: 2.8483 ms. Best GFLOPs: 2785.1256
[15:17:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_conv2d_add_11"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |      2785.1256 |     166.0688 |              166.0688 |     32 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 928
Total latency (us): 12098.1

[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #0: GFLOPs: 44.1516. Time: 0.0023 ms. Best GFLOPs: 44.1516
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #1: GFLOPs: 43.6912. Time: 0.0023 ms. Best GFLOPs: 44.1516
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #2: GFLOPs: 44.1562. Time: 0.0023 ms. Best GFLOPs: 44.1562
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #3: GFLOPs: 43.9453. Time: 0.0023 ms. Best GFLOPs: 44.1562
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #4: GFLOPs: 44.1990. Time: 0.0023 ms. Best GFLOPs: 44.1990
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #5: GFLOPs: 43.6747. Time: 0.0023 ms. Best GFLOPs: 44.1990
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #6: GFLOPs: 44.1251. Time: 0.0023 ms. Best GFLOPs: 44.1990
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #7: GFLOPs: 44.0021. Time: 0.0023 ms. Best GFLOPs: 44.1990
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #8: GFLOPs: 44.1552. Time: 0.0023 ms. Best GFLOPs: 44.1990
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #9: GFLOPs: 43.8040. Time: 0.0023 ms. Best GFLOPs: 44.1990
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #10: GFLOPs: 44.1778. Time: 0.0023 ms. Best GFLOPs: 44.1990
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #11: GFLOPs: 43.7459. Time: 0.0023 ms. Best GFLOPs: 44.1990
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #12: GFLOPs: 44.1647. Time: 0.0023 ms. Best GFLOPs: 44.1990
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #13: GFLOPs: 43.8592. Time: 0.0023 ms. Best GFLOPs: 44.1990
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #14: GFLOPs: 44.2543. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #15: GFLOPs: 43.7671. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #16: GFLOPs: 44.1653. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #17: GFLOPs: 41.7712. Time: 0.0024 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #18: GFLOPs: 43.8527. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #19: GFLOPs: 44.2462. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #20: GFLOPs: 43.6717. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #21: GFLOPs: 44.2270. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #22: GFLOPs: 44.0867. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #23: GFLOPs: 44.1729. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #24: GFLOPs: 43.8879. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #25: GFLOPs: 44.1817. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #26: GFLOPs: 43.7871. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #27: GFLOPs: 44.1713. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #28: GFLOPs: 43.8911. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #29: GFLOPs: 44.1658. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #30: GFLOPs: 43.6746. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape_nn_prelu_reshape_6"] Trial #31: GFLOPs: 44.2178. Time: 0.0023 ms. Best GFLOPs: 44.2543
[15:17:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #29: "fused_reshape_nn_prelu_reshape_6"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |      2785.1256 |     166.0688 |              166.0688 |     32 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |        44.2543 |       2.2676 |                2.2676 |     32 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 960
Total latency (us): 12100.4

[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #0: GFLOPs: 344.1938. Time: 0.6719 ms. Best GFLOPs: 344.1938
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #1: GFLOPs: 110.2895. Time: 2.0969 ms. Best GFLOPs: 344.1938
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #2: GFLOPs: 57.9730. Time: 3.9891 ms. Best GFLOPs: 344.1938
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #3: GFLOPs: 50.5815. Time: 4.5720 ms. Best GFLOPs: 344.1938
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #4: GFLOPs: 171.6397. Time: 1.3474 ms. Best GFLOPs: 344.1938
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #5: GFLOPs: 107.9490. Time: 2.1423 ms. Best GFLOPs: 344.1938
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #6: GFLOPs: 323.8169. Time: 0.7142 ms. Best GFLOPs: 344.1938
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #7: GFLOPs: 12.9672. Time: 17.8344 ms. Best GFLOPs: 344.1938
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #8: GFLOPs: 73.6331. Time: 3.1407 ms. Best GFLOPs: 344.1938
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #9: GFLOPs: 10.3720. Time: 22.2967 ms. Best GFLOPs: 344.1938
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #10: GFLOPs: 606.8085. Time: 0.3811 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #11: GFLOPs: 15.0311. Time: 15.3855 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #12: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    with T.block("conv2d_nchw_init"):
                        nn = T.axis.spatial(1, 0)
                        ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused)
                        yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                        xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                        T.reads()
                        T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 90 // 45)
                                        v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 45 // 15)
                                        v3 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 90)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(512, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                                xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                rc = T.axis.reduce(512, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 128, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 128, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #13: GFLOPs: 251.1868. Time: 0.9207 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #14: GFLOPs: 88.4748. Time: 2.6139 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #15: GFLOPs: 55.9995. Time: 4.1297 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #16: GFLOPs: 490.8057. Time: 0.4712 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #17: GFLOPs: 14.9848. Time: 15.4331 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #18: GFLOPs: 247.5657. Time: 0.9341 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #19: GFLOPs: 124.1218. Time: 1.8632 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #20: GFLOPs: 205.9368. Time: 1.1230 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #21: GFLOPs: 203.9243. Time: 1.1341 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #22: GFLOPs: 95.0223. Time: 2.4338 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #23: GFLOPs: 11.5292. Time: 20.0587 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #24: GFLOPs: 219.1762. Time: 1.0551 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #25: GFLOPs: 178.0414. Time: 1.2989 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #26: GFLOPs: 420.1147. Time: 0.5505 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #27: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(98, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    with T.block("conv2d_nchw_init"):
                        nn = T.axis.spatial(1, 0)
                        ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 128 + i0_2_i1_2_i2_2_i3_2_fused)
                        yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                        xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                        T.reads()
                        T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 390 // 195)
                                        v2 = T.axis.spatial(16, ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(16, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 390)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(512, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 128 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                                xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                rc = T.axis.reduce(512, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 128 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 128, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 128, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #28: GFLOPs: 403.1853. Time: 0.5736 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #29: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(16, 7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 390 // 195)
                                        v2 = T.axis.spatial(16, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 195 // 15)
                                        v3 = T.axis.spatial(16, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 390)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(512, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 16, 7, 7, 2, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3)
                                yy, xx = T.axis.remap("SS", [i2_3, i3_3])
                                rc = T.axis.reduce(512, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 16 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #30: GFLOPs: 76.1998. Time: 3.0349 ms. Best GFLOPs: 606.8085
[15:17:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_add_6"] Trial #31: GFLOPs: 582.9989. Time: 0.3967 ms. Best GFLOPs: 606.8085
[15:17:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #30: "fused_nn_conv2d_add_add_6"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |      2785.1256 |     166.0688 |              166.0688 |     32 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |        44.2543 |       2.2676 |                2.2676 |     32 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |       606.8085 |     381.1107 |              381.1107 |     32 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 992
Total latency (us): 12481.5

[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #0: GFLOPs: 544.4863. Time: 0.4247 ms. Best GFLOPs: 544.4863
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #1: GFLOPs: 90.2568. Time: 2.5623 ms. Best GFLOPs: 544.4863
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #2: GFLOPs: 131.7470. Time: 1.7553 ms. Best GFLOPs: 544.4863
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #3: GFLOPs: 245.6728. Time: 0.9413 ms. Best GFLOPs: 544.4863
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #4: GFLOPs: 106.1534. Time: 2.1786 ms. Best GFLOPs: 544.4863
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #5: GFLOPs: 336.3037. Time: 0.6877 ms. Best GFLOPs: 544.4863
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #6: GFLOPs: 769.5836. Time: 0.3005 ms. Best GFLOPs: 769.5836
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #7: GFLOPs: 199.0004. Time: 1.1621 ms. Best GFLOPs: 769.5836
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #8: GFLOPs: 116.1523. Time: 1.9910 ms. Best GFLOPs: 769.5836
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #9: GFLOPs: 303.4851. Time: 0.7620 ms. Best GFLOPs: 769.5836
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #10: GFLOPs: 46.8110. Time: 4.9403 ms. Best GFLOPs: 769.5836
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #11: GFLOPs: 50.3871. Time: 4.5897 ms. Best GFLOPs: 769.5836
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #12: GFLOPs: 456.7891. Time: 0.5063 ms. Best GFLOPs: 769.5836
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #13: GFLOPs: 1496.5495. Time: 0.1545 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #14: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i2_4_init)
                            xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 + 0)
                                    v2 = T.axis.spatial(9, ax0_ax1_ax2_ax3_fused_1 % 27 // 3)
                                    v3 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax0_ax1_ax2_ax3_fused_1 % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 27)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(512, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy = T.axis.spatial(7, i2_4)
                                xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #15: GFLOPs: 3.7572. Time: 61.5513 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #16: GFLOPs: 228.0114. Time: 1.0143 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #17: GFLOPs: 72.2996. Time: 3.1986 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #18: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init)
                            yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                            xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 54 // 27)
                                    v2 = T.axis.spatial(9, ax0_ax1_ax2_ax3_fused_1 % 27 // 3)
                                    v3 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax0_ax1_ax2_ax3_fused_1 % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 54)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(512, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                rc = T.axis.reduce(512, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 7 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 64, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #19: GFLOPs: 290.9968. Time: 0.7947 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #20: GFLOPs: 124.5498. Time: 1.8568 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #21: GFLOPs: 246.4714. Time: 0.9383 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #22: GFLOPs: 598.0958. Time: 0.3867 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #23: GFLOPs: 70.3368. Time: 3.2879 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #24: GFLOPs: 124.6006. Time: 1.8560 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #25: GFLOPs: 615.9029. Time: 0.3755 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #26: GFLOPs: 421.8993. Time: 0.5481 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #27: GFLOPs: 423.7427. Time: 0.5458 ms. Best GFLOPs: 1496.5495
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #28: GFLOPs: 1647.1083. Time: 0.1404 ms. Best GFLOPs: 1647.1083
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #29: GFLOPs: 678.8516. Time: 0.3407 ms. Best GFLOPs: 1647.1083
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #30: GFLOPs: 40.4536. Time: 5.7167 ms. Best GFLOPs: 1647.1083
[15:17:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_add_add_7"] Trial #31: GFLOPs: 905.8906. Time: 0.2553 ms. Best GFLOPs: 1647.1083
[15:17:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #31: "fused_nn_conv2d_add_add_7"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |      2785.1256 |     166.0688 |              166.0688 |     32 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |        44.2543 |       2.2676 |                2.2676 |     32 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |       606.8085 |     381.1107 |              381.1107 |     32 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |      1647.1083 |     140.4044 |              140.4044 |     32 |            
 32 |                          fused_add_4 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1024
Total latency (us): 12621.9

[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #0: GFLOPs: 11.4929. Time: 0.0022 ms. Best GFLOPs: 11.4929
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #1: GFLOPs: 11.4946. Time: 0.0022 ms. Best GFLOPs: 11.4946
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #2: GFLOPs: 11.5340. Time: 0.0022 ms. Best GFLOPs: 11.5340
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #3: GFLOPs: 11.5242. Time: 0.0022 ms. Best GFLOPs: 11.5340
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #4: GFLOPs: 11.4752. Time: 0.0022 ms. Best GFLOPs: 11.5340
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #5: GFLOPs: 11.5704. Time: 0.0022 ms. Best GFLOPs: 11.5704
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #6: GFLOPs: 11.5245. Time: 0.0022 ms. Best GFLOPs: 11.5704
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #7: GFLOPs: 11.5128. Time: 0.0022 ms. Best GFLOPs: 11.5704
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #8: GFLOPs: 11.7034. Time: 0.0021 ms. Best GFLOPs: 11.7034
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #9: GFLOPs: 11.6627. Time: 0.0022 ms. Best GFLOPs: 11.7034
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #10: GFLOPs: 11.6451. Time: 0.0022 ms. Best GFLOPs: 11.7034
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #11: GFLOPs: 11.6837. Time: 0.0021 ms. Best GFLOPs: 11.7034
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #12: GFLOPs: 11.6851. Time: 0.0021 ms. Best GFLOPs: 11.7034
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #13: GFLOPs: 11.7282. Time: 0.0021 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #14: GFLOPs: 10.5343. Time: 0.0024 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #15: GFLOPs: 10.2467. Time: 0.0024 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #16: GFLOPs: 10.6378. Time: 0.0024 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #17: GFLOPs: 10.6856. Time: 0.0023 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #18: GFLOPs: 10.5271. Time: 0.0024 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #19: GFLOPs: 6.3190. Time: 0.0040 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #20: GFLOPs: 7.5744. Time: 0.0033 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #21: GFLOPs: 11.2324. Time: 0.0022 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #22: GFLOPs: 8.6677. Time: 0.0029 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #23: GFLOPs: 4.7608. Time: 0.0053 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #24: GFLOPs: 5.1534. Time: 0.0049 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #25: GFLOPs: 5.2176. Time: 0.0048 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #26: GFLOPs: 5.9719. Time: 0.0042 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #27: GFLOPs: 7.1933. Time: 0.0035 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #28: GFLOPs: 11.3658. Time: 0.0022 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #29: GFLOPs: 9.0879. Time: 0.0028 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #30: GFLOPs: 5.9677. Time: 0.0042 ms. Best GFLOPs: 11.7282
[15:17:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_add_4"] Trial #31: GFLOPs: 6.0550. Time: 0.0041 ms. Best GFLOPs: 11.7282
[15:17:18] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #32: "fused_add_4"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |      2785.1256 |     166.0688 |              166.0688 |     32 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |        44.2543 |       2.2676 |                2.2676 |     32 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |       606.8085 |     381.1107 |              381.1107 |     32 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |      1647.1083 |     140.4044 |              140.4044 |     32 |            
 32 |                          fused_add_4 |     25088 |      2 |        11.7282 |       2.1391 |                4.2782 |     32 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1056
Total latency (us): 12626.2

[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #0: GFLOPs: 352.6935. Time: 0.6556 ms. Best GFLOPs: 352.6935
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #1: GFLOPs: 57.6673. Time: 4.0098 ms. Best GFLOPs: 352.6935
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #2: GFLOPs: 888.4730. Time: 0.2603 ms. Best GFLOPs: 888.4730
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #3: GFLOPs: 564.0312. Time: 0.4100 ms. Best GFLOPs: 888.4730
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #4: GFLOPs: 11.0326. Time: 20.9593 ms. Best GFLOPs: 888.4730
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #5: GFLOPs: 314.9178. Time: 0.7343 ms. Best GFLOPs: 888.4730
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #6: GFLOPs: 176.1368. Time: 1.3128 ms. Best GFLOPs: 888.4730
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #7: GFLOPs: 568.3178. Time: 0.4069 ms. Best GFLOPs: 888.4730
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #8: GFLOPs: 112.9770. Time: 2.0468 ms. Best GFLOPs: 888.4730
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #9: GFLOPs: 645.2811. Time: 0.3583 ms. Best GFLOPs: 888.4730
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #10: GFLOPs: 1128.7948. Time: 0.2049 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #11: GFLOPs: 114.0583. Time: 2.0273 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #12: GFLOPs: 149.5903. Time: 1.5458 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #13: GFLOPs: 251.9640. Time: 0.9177 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #14: GFLOPs: 89.8128. Time: 2.5746 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #15: GFLOPs: 846.5394. Time: 0.2732 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #16: GFLOPs: 734.4595. Time: 0.3148 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #17: GFLOPs: 58.9624. Time: 3.9218 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #18: GFLOPs: 68.8764. Time: 3.3573 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #19: GFLOPs: 415.4595. Time: 0.5566 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #20: GFLOPs: 104.9522. Time: 2.2033 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #21: GFLOPs: 493.2199. Time: 0.4688 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #22: GFLOPs: 844.8752. Time: 0.2737 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #23: GFLOPs: 98.4681. Time: 2.3483 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #24: GFLOPs: 206.1610. Time: 1.1216 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #25: GFLOPs: 122.9132. Time: 1.8813 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #26: GFLOPs: 205.6009. Time: 1.1247 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #27: GFLOPs: 178.0514. Time: 1.2987 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #28: GFLOPs: 196.6848. Time: 1.1757 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #29: GFLOPs: 19.7741. Time: 11.6939 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #30: GFLOPs: 102.8075. Time: 2.2492 ms. Best GFLOPs: 1128.7948
[15:17:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_conv2d_add_12"] Trial #31: GFLOPs: 1190.8506. Time: 0.1942 ms. Best GFLOPs: 1190.8506
[15:17:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #33: "fused_nn_conv2d_add_12"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |      2785.1256 |     166.0688 |              166.0688 |     32 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |        44.2543 |       2.2676 |                2.2676 |     32 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |       606.8085 |     381.1107 |              381.1107 |     32 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |      1647.1083 |     140.4044 |              140.4044 |     32 |            
 32 |                          fused_add_4 |     25088 |      2 |        11.7282 |       2.1391 |                4.2782 |     32 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |      1190.8506 |     194.1773 |              388.3545 |     32 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1088
Total latency (us): 13014.5

[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #0: GFLOPs: 7.4650. Time: 0.0034 ms. Best GFLOPs: 7.4650
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #1: GFLOPs: 6.1191. Time: 0.0041 ms. Best GFLOPs: 7.4650
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #2: GFLOPs: 11.3586. Time: 0.0022 ms. Best GFLOPs: 11.3586
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #3: GFLOPs: 11.1768. Time: 0.0022 ms. Best GFLOPs: 11.3586
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #4: GFLOPs: 11.2066. Time: 0.0022 ms. Best GFLOPs: 11.3586
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #5: GFLOPs: 10.9826. Time: 0.0023 ms. Best GFLOPs: 11.3586
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #6: GFLOPs: 11.1165. Time: 0.0023 ms. Best GFLOPs: 11.3586
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #7: GFLOPs: 10.8751. Time: 0.0023 ms. Best GFLOPs: 11.3586
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #8: GFLOPs: 9.0513. Time: 0.0028 ms. Best GFLOPs: 11.3586
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #9: GFLOPs: 11.2415. Time: 0.0022 ms. Best GFLOPs: 11.3586
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #10: GFLOPs: 11.2098. Time: 0.0022 ms. Best GFLOPs: 11.3586
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #11: GFLOPs: 6.9373. Time: 0.0036 ms. Best GFLOPs: 11.3586
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #12: GFLOPs: 11.3988. Time: 0.0022 ms. Best GFLOPs: 11.3988
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #13: GFLOPs: 11.8411. Time: 0.0021 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #14: GFLOPs: 8.8548. Time: 0.0028 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #15: GFLOPs: 6.7087. Time: 0.0037 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #16: GFLOPs: 8.3702. Time: 0.0030 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #17: GFLOPs: 8.9281. Time: 0.0028 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #18: GFLOPs: 11.1893. Time: 0.0022 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #19: GFLOPs: 11.2062. Time: 0.0022 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #20: GFLOPs: 11.2576. Time: 0.0022 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #21: GFLOPs: 11.2849. Time: 0.0022 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #22: GFLOPs: 11.3261. Time: 0.0022 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #23: GFLOPs: 11.3551. Time: 0.0022 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #24: GFLOPs: 8.1713. Time: 0.0031 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #25: GFLOPs: 6.5745. Time: 0.0038 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #26: GFLOPs: 11.2375. Time: 0.0022 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #27: GFLOPs: 6.3197. Time: 0.0040 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #28: GFLOPs: 5.5303. Time: 0.0045 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #29: GFLOPs: 9.0133. Time: 0.0028 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #30: GFLOPs: 5.2512. Time: 0.0048 ms. Best GFLOPs: 11.8411
[15:17:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_reshape_nn_prelu_reshape_7"] Trial #31: GFLOPs: 11.2225. Time: 0.0022 ms. Best GFLOPs: 11.8411
[15:17:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #34: "fused_reshape_nn_prelu_reshape_7"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |      2785.1256 |     166.0688 |              166.0688 |     32 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |        44.2543 |       2.2676 |                2.2676 |     32 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |       606.8085 |     381.1107 |              381.1107 |     32 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |      1647.1083 |     140.4044 |              140.4044 |     32 |            
 32 |                          fused_add_4 |     25088 |      2 |        11.7282 |       2.1391 |                4.2782 |     32 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |      1190.8506 |     194.1773 |              388.3545 |     32 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |        11.8411 |       2.1187 |                4.2374 |     32 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1120
Total latency (us): 13018.8

[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #0: GFLOPs: 255.1490. Time: 0.9066 ms. Best GFLOPs: 255.1490
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #1: GFLOPs: 528.2215. Time: 0.4379 ms. Best GFLOPs: 528.2215
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #2: GFLOPs: 389.6454. Time: 0.5936 ms. Best GFLOPs: 528.2215
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #3: GFLOPs: 564.1946. Time: 0.4100 ms. Best GFLOPs: 564.1946
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #4: GFLOPs: 14.6358. Time: 15.8045 ms. Best GFLOPs: 564.1946
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #5: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_4: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_5: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    with T.block("conv2d_nchw_init"):
                        nn = T.axis.spatial(1, 0)
                        ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 49 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                        yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7)
                        xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                        T.reads()
                        T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 18 // 9)
                                    v2 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 + ax0_ax1_ax2_ax3_fused_1 % 9 // 3)
                                    v3 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax0_ax1_ax2_ax3_fused_1 % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 18)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 49 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(512, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 49 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7)
                                xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                rc = T.axis.reduce(512, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 49 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3], placeholder_4[v0, v1, 0, 0], placeholder_5[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]) * placeholder_4[v0, v1, 0, 0] + placeholder_5[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add_2", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 2, 64, 1, 1])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b76)
l110, l111 = sch.split(loop=l109, factors=[None, 64])
sch.bind(loop=l111, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b89)
l119, l120, l121 = sch.split(loop=l118, factors=[None, 64, 4])
sch.vectorize(loop=l121)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b122 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b122, ann_key="meta_schedule.unroll_explicit")
b123, b124, b125, b126 = sch.get_child_blocks(b122)
l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b126)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #6: GFLOPs: 292.5738. Time: 0.7906 ms. Best GFLOPs: 564.1946
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #7: GFLOPs: 25.9919. Time: 8.8994 ms. Best GFLOPs: 564.1946
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #8: GFLOPs: 14.1583. Time: 16.3375 ms. Best GFLOPs: 564.1946
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #9: GFLOPs: 238.2076. Time: 0.9710 ms. Best GFLOPs: 564.1946
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #10: GFLOPs: 924.3649. Time: 0.2502 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #11: GFLOPs: 275.7010. Time: 0.8390 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #12: GFLOPs: 367.0647. Time: 0.6302 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #13: GFLOPs: 57.1782. Time: 4.0454 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(512, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_4: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_5: T.Buffer[(1, 512, 1, 1), "float32"], T_add: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(224, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3_init)
                            yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0)
                                    v2 = T.axis.spatial(9, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v3 = T.axis.spatial(9, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 81)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(83):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(512, i4_0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 4608)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [512, 512, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3], placeholder_4[v0, v1, 0, 0], placeholder_5[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]) * placeholder_4[v0, v1, 0, 0] + placeholder_5[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add_2", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 32, 8, 2, 1])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b76)
l110, l111 = sch.split(loop=l109, factors=[None, 56])
sch.bind(loop=l111, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b89)
l119, l120 = sch.split(loop=l118, factors=[None, 56])
sch.bind(loop=l120, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #15: GFLOPs: 10.0824. Time: 22.9421 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #16: GFLOPs: 491.1331. Time: 0.4710 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #17: GFLOPs: 179.3613. Time: 1.2896 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #18: GFLOPs: 19.3889. Time: 11.9301 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #19: GFLOPs: 12.4142. Time: 18.6327 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #20: GFLOPs: 87.0199. Time: 2.6581 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #21: GFLOPs: 855.2428. Time: 0.2705 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #22: GFLOPs: 463.4401. Time: 0.4991 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #23: GFLOPs: 323.3952. Time: 0.7153 ms. Best GFLOPs: 924.3649
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #24: GFLOPs: 941.2020. Time: 0.2458 ms. Best GFLOPs: 941.2020
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #25: GFLOPs: 444.8585. Time: 0.5200 ms. Best GFLOPs: 941.2020
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #26: GFLOPs: 109.6192. Time: 2.1101 ms. Best GFLOPs: 941.2020
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #27: GFLOPs: 169.1619. Time: 1.3674 ms. Best GFLOPs: 941.2020
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #28: GFLOPs: 181.1203. Time: 1.2771 ms. Best GFLOPs: 941.2020
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #29: GFLOPs: 121.6864. Time: 1.9009 ms. Best GFLOPs: 941.2020
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #30: GFLOPs: 328.3735. Time: 0.7044 ms. Best GFLOPs: 941.2020
[15:17:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_add_add_multiply_add"] Trial #31: GFLOPs: 14.5944. Time: 15.8493 ms. Best GFLOPs: 941.2020
[15:17:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #35: "fused_nn_conv2d_add_add_multiply_add"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |      2785.1256 |     166.0688 |              166.0688 |     32 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |        44.2543 |       2.2676 |                2.2676 |     32 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |       606.8085 |     381.1107 |              381.1107 |     32 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |      1647.1083 |     140.4044 |              140.4044 |     32 |            
 32 |                          fused_add_4 |     25088 |      2 |        11.7282 |       2.1391 |                4.2782 |     32 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |      1190.8506 |     194.1773 |              388.3545 |     32 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |        11.8411 |       2.1187 |                4.2374 |     32 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |       941.2020 |     245.7617 |              245.7617 |     32 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1152
Total latency (us): 13264.5

[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #0: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #1: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #2: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #3: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #4: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #5: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #6: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #7: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #8: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #9: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #10: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #11: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #12: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #13: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #14: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #15: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #16: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #17: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #18: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #19: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #20: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #21: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #22: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #23: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #24: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #25: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #26: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #27: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #28: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #29: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #30: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_batch_flatten"] Trial #31: GFLOPs: 0.0000. Time: 0.0021 ms. Best GFLOPs: 0.0000
[15:17:31] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #36: "fused_nn_batch_flatten"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |      2785.1256 |     166.0688 |              166.0688 |     32 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |        44.2543 |       2.2676 |                2.2676 |     32 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |       606.8085 |     381.1107 |              381.1107 |     32 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |      1647.1083 |     140.4044 |              140.4044 |     32 |            
 32 |                          fused_add_4 |     25088 |      2 |        11.7282 |       2.1391 |                4.2782 |     32 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |      1190.8506 |     194.1773 |              388.3545 |     32 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |        11.8411 |       2.1187 |                4.2374 |     32 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |       941.2020 |     245.7617 |              245.7617 |     32 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |         0.0005 |       2.1267 |                2.1267 |     32 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1184
Total latency (us): 13266.7

[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #0: GFLOPs: 3.2572. Time: 7.8873 ms. Best GFLOPs: 3.2572
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #1: GFLOPs: 4.0938. Time: 6.2756 ms. Best GFLOPs: 4.0938
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #2: GFLOPs: 1.5671. Time: 16.3935 ms. Best GFLOPs: 4.0938
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #3: GFLOPs: 46.0456. Time: 0.5579 ms. Best GFLOPs: 46.0456
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #4: GFLOPs: 7.3684. Time: 3.4866 ms. Best GFLOPs: 46.0456
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #5: GFLOPs: 6.2355. Time: 4.1200 ms. Best GFLOPs: 46.0456
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #6: GFLOPs: 12.3629. Time: 2.0781 ms. Best GFLOPs: 46.0456
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #7: GFLOPs: 14.2450. Time: 1.8035 ms. Best GFLOPs: 46.0456
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #8: GFLOPs: 8.4373. Time: 3.0449 ms. Best GFLOPs: 46.0456
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #9: GFLOPs: 28.1932. Time: 0.9112 ms. Best GFLOPs: 46.0456
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #10: GFLOPs: 7.1240. Time: 3.6062 ms. Best GFLOPs: 46.0456
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #11: GFLOPs: 49.7990. Time: 0.5159 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #12: GFLOPs: 3.0392. Time: 8.4532 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #37: "fused_nn_dense_add"] Trial #13: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 512], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 25088], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([512, 25088], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init in T.serial(16):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(512, i0_2_i1_2_fused * 16 + i1_3_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(3584):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(25088, i2_0 * 7 + ax0_ax1_fused_1)
                                    T.where(ax0_ax1_fused_1 < 7)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(56):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_fused_0 * 64 + ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2) // 7)
                                        v1 = T.axis.spatial(25088, i2_0 * 7 + (ax0_ax1_fused_0 * 64 + ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2) % 7)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(7, 1, 16, 1, 1, 1):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(512, i0_2_i1_2_fused * 16 + i1_3)
                                k = T.axis.reduce(25088, i2_0 * 7 + i2_1)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 16):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_2_i1_2_fused * 16 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 32, 16, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[3584, 7, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 32])
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 32, 2])
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #37: "fused_nn_dense_add"] Trial #14: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 512], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 25088], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([512, 25088], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(512, i0_0_i1_0_fused * 64 + i0_2_i1_2_fused * 2 + i1_3_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(256):
                        for ax0_ax1_fused_0 in T.serial(4):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(25088, i2_0 * 98 + (ax0_ax1_fused_0 * 32 + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * 32 + ax0_ax1_fused_1 < 98)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(49):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_fused * 64 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 98)
                                        v1 = T.axis.spatial(25088, i2_0 * 98 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 98)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(14, 1, 2, 7, 1, 1):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(512, i0_0_i1_0_fused * 64 + i0_2_i1_2_fused * 2 + i1_3)
                                k = T.axis.reduce(25088, i2_0 * 98 + i2_1 * 7 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 2):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_fused * 64 + i0_2_i1_2_fused * 2 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[8, 1, 32, 2, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 14, 7])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 32])
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 32, 4])
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #15: GFLOPs: 47.8116. Time: 0.5373 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #16: GFLOPs: 17.3438. Time: 1.4813 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #37: "fused_nn_dense_add"] Trial #17: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 512], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 25088], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([512, 25088], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(4, 2):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(512, i0_0_i1_0_fused * 256 + i0_2_i1_2_fused * 8 + i1_3_init * 2 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(1792):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(25088, i2_0 * 14 + (ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2))
                                        T.where(ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2 < 14)
                                        T.reads(placeholder[v0, v1])
                                        T.writes(placeholder_shared[v0, v1])
                                        placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(28):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_fused * 256 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 14)
                                        v1 = T.axis.spatial(25088, i2_0 * 14 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 14)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(7, 1, 4, 2, 1, 2):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(512, i0_0_i1_0_fused * 256 + i0_2_i1_2_fused * 8 + i1_3 * 2 + i1_4)
                                k = T.axis.reduce(25088, i2_0 * 14 + i2_1 * 2 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 8):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_fused * 256 + i0_2_i1_2_fused * 8 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 1, 32, 4, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[1792, 7, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 32, 2])
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4])
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #18: GFLOPs: 7.7879. Time: 3.2988 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #19: GFLOPs: 13.4280. Time: 1.9132 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #37: "fused_nn_dense_add"] Trial #20: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 512], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 25088], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([512, 25088], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(512, i0_0_i1_0_fused * 256 + i0_2_i1_2_fused * 2 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(1792):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(25088, i2_0 * 14 + (ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2))
                                        T.where(ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2 < 14)
                                        T.reads(placeholder[v0, v1])
                                        T.writes(placeholder_shared[v0, v1])
                                        placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(7):
                            for ax0_ax1_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_fused * 256 + (ax0_ax1_fused_0 * 512 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 14)
                                        v1 = T.axis.spatial(25088, i2_0 * 14 + (ax0_ax1_fused_0 * 512 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 14)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(14, 1, 1, 1, 1, 2):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(512, i0_0_i1_0_fused * 256 + i0_2_i1_2_fused * 2 + i1_4)
                                k = T.axis.reduce(25088, i2_0 * 14 + i2_1)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 2):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_fused * 256 + i0_2_i1_2_fused * 2 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 1, 128, 1, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[1792, 14, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 128, 2])
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 128, 4])
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #21: GFLOPs: 5.4936. Time: 4.6765 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #22: GFLOPs: 3.2111. Time: 8.0006 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #23: GFLOPs: 6.6969. Time: 3.8362 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #24: GFLOPs: 27.8552. Time: 0.9223 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #37: "fused_nn_dense_add"] Trial #25: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 512], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 25088], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([512, 25088], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init in T.serial(4):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(512, i0_0_i1_0_fused * 128 + i0_2_i1_2_fused * 4 + i1_3_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(3584):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(25088, i2_0 * 7 + ax0_ax1_fused_1)
                                    T.where(ax0_ax1_fused_1 < 7)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(7):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_fused * 128 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 7)
                                        v1 = T.axis.spatial(25088, i2_0 * 7 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 7)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(1, 1, 4, 7, 1, 1):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(512, i0_0_i1_0_fused * 128 + i0_2_i1_2_fused * 4 + i1_3)
                                k = T.axis.reduce(25088, i2_0 * 7 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 25088], "float32"], ["TENSOR", [512, 25088], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 4):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_fused * 128 + i0_2_i1_2_fused * 4 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[4, 1, 32, 4, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[3584, 1, 7])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 32])
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 32, 4])
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #26: GFLOPs: 15.3344. Time: 1.6754 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #27: GFLOPs: 11.5810. Time: 2.2183 ms. Best GFLOPs: 49.7990
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #28: GFLOPs: 65.6981. Time: 0.3910 ms. Best GFLOPs: 65.6981
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #29: GFLOPs: 6.2025. Time: 4.1420 ms. Best GFLOPs: 65.6981
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #30: GFLOPs: 12.2443. Time: 2.0982 ms. Best GFLOPs: 65.6981
[15:17:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_dense_add"] Trial #31: GFLOPs: 1.8596. Time: 13.8152 ms. Best GFLOPs: 65.6981
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #37: "fused_nn_dense_add"
 ID |                                 Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_conv2d_add |  12870144 |      1 |       544.4771 |      23.6376 |               23.6376 |     32 |            
  1 |                fused_nn_conv2d_add_1 |  12895232 |      1 |      1064.6872 |      12.1118 |               12.1118 |     32 |            
  2 |                fused_nn_conv2d_add_2 |  12945408 |      1 |      1094.7078 |      11.8254 |               11.8254 |     32 |            
  3 |                fused_nn_conv2d_add_3 |  25890816 |      1 |      1696.9294 |      15.2575 |               15.2575 |     32 |            
  4 |         fused_copy_subtract_multiply |     75264 |      1 |        33.9418 |       2.2174 |                2.2174 |     32 |            
  5 |                fused_nn_conv2d_add_4 |  44154880 |      1 |      2113.6802 |      20.8900 |               20.8900 |     32 |            
  6 |                            fused_add |    802816 |      1 |        47.8639 |      16.7729 |               16.7729 |     32 |            
  7 |                fused_nn_conv2d_add_5 | 925646848 |      1 |      5785.7549 |     159.9872 |              159.9872 |     32 |            
  8 |       fused_reshape_nn_prelu_reshape |    802816 |      2 |        47.5614 |      16.8796 |               33.7591 |     32 |            
  9 |              fused_nn_conv2d_add_add | 231612416 |      1 |      2070.2343 |     111.8774 |              111.8774 |     32 |            
 10 |                fused_nn_conv2d_add_6 | 231411712 |      2 |      3444.0089 |      67.1925 |              134.3851 |     32 |            
 11 |     fused_reshape_nn_prelu_reshape_1 |    200704 |      2 |        62.5072 |       3.2109 |                6.4218 |     32 |            
 12 |            fused_nn_conv2d_add_add_1 | 231612416 |      2 |      5232.6718 |      44.2627 |               88.5255 |     32 |            
 13 |                          fused_add_1 |    200704 |      3 |        59.7624 |       3.3584 |               10.0751 |     32 |            
 14 |                fused_nn_conv2d_add_7 | 462823424 |      1 |      3950.2134 |     117.1642 |              117.1642 |     32 |            
 15 |     fused_reshape_nn_prelu_reshape_2 |    401408 |      1 |        92.2714 |       4.3503 |                4.3503 |     32 |            
 16 |            fused_nn_conv2d_add_add_2 | 231411712 |      1 |      1916.3525 |     120.7563 |              120.7563 |     32 |            
 17 |                fused_nn_conv2d_add_8 | 231311360 |     12 |      3129.3076 |      73.9177 |              887.0129 |     32 |            
 18 |     fused_reshape_nn_prelu_reshape_3 |    100352 |     12 |        43.2360 |       2.3210 |               27.8523 |     32 |            
 19 |            fused_nn_conv2d_add_add_3 | 231411712 |     12 |      2424.9949 |      95.4277 |             1145.1325 |     32 |            
 20 |                          fused_add_2 |    100352 |     13 |        40.8403 |       2.4572 |               31.9433 |     32 |            
 21 |                fused_nn_conv2d_add_9 | 462622720 |      1 |      5640.4352 |      82.0190 |               82.0190 |     32 |            
 22 |     fused_reshape_nn_prelu_reshape_4 |    200704 |      1 |        63.2187 |       3.1748 |                3.1748 |     32 |            
 23 |            fused_nn_conv2d_add_add_4 | 231311360 |      1 |      1863.5128 |     124.1265 |              124.1265 |     32 |            
 24 |               fused_nn_conv2d_add_10 | 231261184 |     29 |      1465.8126 |     157.7699 |             4575.3285 |     32 |            
 25 |     fused_reshape_nn_prelu_reshape_5 |     50176 |     29 |        23.0944 |       2.1727 |               63.0069 |     32 |            
 26 |            fused_nn_conv2d_add_add_5 | 231311360 |     29 |      1661.5130 |     139.2173 |             4037.3017 |     32 |            
 27 |                          fused_add_3 |     50176 |     30 |        23.1100 |       2.1712 |               65.1356 |     32 |            
 28 |               fused_nn_conv2d_add_11 | 462522368 |      1 |      2785.1256 |     166.0688 |              166.0688 |     32 |            
 29 |     fused_reshape_nn_prelu_reshape_6 |    100352 |      1 |        44.2543 |       2.2676 |                2.2676 |     32 |            
 30 |            fused_nn_conv2d_add_add_6 | 231261184 |      1 |       606.8085 |     381.1107 |              381.1107 |     32 |            
 31 |            fused_nn_conv2d_add_add_7 | 231261184 |      1 |      1647.1083 |     140.4044 |              140.4044 |     32 |            
 32 |                          fused_add_4 |     25088 |      2 |        11.7282 |       2.1391 |                4.2782 |     32 |            
 33 |               fused_nn_conv2d_add_12 | 231236096 |      2 |      1190.8506 |     194.1773 |              388.3545 |     32 |            
 34 |     fused_reshape_nn_prelu_reshape_7 |     25088 |      2 |        11.8411 |       2.1187 |                4.2374 |     32 |            
 35 | fused_nn_conv2d_add_add_multiply_add | 231311360 |      1 |       941.2020 |     245.7617 |              245.7617 |     32 |            
 36 |               fused_nn_batch_flatten |         1 |      1 |         0.0005 |       2.1267 |                2.1267 |     32 |            
 37 |                   fused_nn_dense_add |  25690624 |      1 |        65.6981 |     391.0403 |              391.0403 |     32 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1216
Total latency (us): 13657.7

[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_nn_conv2d_add_10"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #24 has finished. Remaining task(s): 37
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #26: "fused_nn_conv2d_add_add_5"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #26 has finished. Remaining task(s): 36
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_nn_conv2d_add_add_3"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #19 has finished. Remaining task(s): 35
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_nn_conv2d_add_8"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #17 has finished. Remaining task(s): 34
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #37: "fused_nn_dense_add"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #37 has finished. Remaining task(s): 33
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #33: "fused_nn_conv2d_add_12"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #33 has finished. Remaining task(s): 32
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #30: "fused_nn_conv2d_add_add_6"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #30 has finished. Remaining task(s): 31
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #35: "fused_nn_conv2d_add_add_multiply_add"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #35 has finished. Remaining task(s): 30
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #28: "fused_nn_conv2d_add_11"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #28 has finished. Remaining task(s): 29
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_conv2d_add_5"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #7 has finished. Remaining task(s): 28
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #31: "fused_nn_conv2d_add_add_7"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #31 has finished. Remaining task(s): 27
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_nn_conv2d_add_6"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #10 has finished. Remaining task(s): 26
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_nn_conv2d_add_add_4"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #23 has finished. Remaining task(s): 25
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_nn_conv2d_add_add_2"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #16 has finished. Remaining task(s): 24
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_nn_conv2d_add_7"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #14 has finished. Remaining task(s): 23
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_conv2d_add_add"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #9 has finished. Remaining task(s): 22
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_conv2d_add_add_1"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #12 has finished. Remaining task(s): 21
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_nn_conv2d_add_9"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #21 has finished. Remaining task(s): 20
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #27: "fused_add_3"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #27 has finished. Remaining task(s): 19
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_reshape_nn_prelu_reshape_5"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #25 has finished. Remaining task(s): 18
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_reshape_nn_prelu_reshape"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #8 has finished. Remaining task(s): 17
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_add_2"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #20 has finished. Remaining task(s): 16
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_reshape_nn_prelu_reshape_3"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #18 has finished. Remaining task(s): 15
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_conv2d_add"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #0 has finished. Remaining task(s): 14
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_conv2d_add_4"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #5 has finished. Remaining task(s): 13
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_add"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #6 has finished. Remaining task(s): 12
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_nn_conv2d_add_3"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #3 has finished. Remaining task(s): 11
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_conv2d_add_1"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #1 has finished. Remaining task(s): 10
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_nn_conv2d_add_2"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #2 has finished. Remaining task(s): 9
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_add_1"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #13 has finished. Remaining task(s): 8
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_reshape_nn_prelu_reshape_1"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #11 has finished. Remaining task(s): 7
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_reshape_nn_prelu_reshape_2"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #15 has finished. Remaining task(s): 6
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #32: "fused_add_4"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #32 has finished. Remaining task(s): 5
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #34: "fused_reshape_nn_prelu_reshape_7"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #34 has finished. Remaining task(s): 4
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_reshape_nn_prelu_reshape_4"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #22 has finished. Remaining task(s): 3
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #29: "fused_reshape_nn_prelu_reshape_6"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #29 has finished. Remaining task(s): 2
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_copy_subtract_multiply"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #4 has finished. Remaining task(s): 1
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #36: "fused_nn_batch_flatten"
[15:17:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #36 has finished. Remaining task(s): 0
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add
[15:21:32] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add
[15:21:45] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_1
[15:21:46] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_1
[15:21:53] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_2
[15:21:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_2
[15:22:01] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_3
[15:22:02] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_3
[15:22:03] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_multiply_add
[[-1.24640942e-01 -1.11050047e-01  1.43470764e-01  1.61246017e-01
   4.64105979e-02  1.61525995e-01 -2.89988607e-01 -7.36976638e-02
   1.80324644e-01 -1.39899641e-01  6.93391114e-02  6.08562827e-02
  -4.37028438e-01  5.97205088e-02 -8.76724273e-02 -9.21801850e-02
  -1.64959818e-01  7.27278590e-02  9.02747661e-02  1.16491228e-01
   2.50093848e-01  2.00189501e-01  3.23669463e-02 -9.14306417e-02
   2.22067833e-02 -8.01960975e-02  2.47398645e-01  9.77709815e-02
   2.67344862e-01  2.76376575e-01 -4.61083129e-02 -2.15255246e-01
  -1.64102316e-01  1.32583782e-01  1.09234780e-01 -1.05223961e-01
   1.62879258e-01  2.03773186e-01 -7.35138208e-02 -1.88275799e-01
  -1.27969429e-01 -2.34344423e-01 -4.67198603e-02 -3.83985974e-03
   5.50674759e-02  5.68486512e-01  3.43846709e-01 -2.49334201e-01
   1.33167133e-01  7.86834136e-02  2.04145629e-02 -1.78715229e-01
  -2.30633214e-01 -3.28711756e-02 -1.84144497e-01  3.00960243e-02
   6.39422238e-02  3.55317146e-02  1.84920505e-01  2.69095659e-01
   4.91958708e-02 -1.20420039e-01 -5.62650375e-02  1.96394578e-01
   2.04228058e-01  2.05738738e-01 -2.66703814e-02  1.39156833e-01
   6.33867532e-02 -1.14984766e-01  4.19507563e-01 -4.86287959e-02
   4.12079215e-01  4.01502192e-01 -1.23376623e-01 -5.71074262e-02
   6.44237176e-02  2.23459780e-01 -1.91960484e-01 -3.70246887e-01
   1.56130359e-01  2.69017935e-01 -6.78144619e-02  6.26548156e-02
  -4.02049720e-01 -6.73607737e-03 -8.75894949e-02 -1.79018959e-01
   3.24596278e-02 -3.48601490e-01  5.95746264e-02 -1.33001223e-01
  -3.10583357e-02  2.99626999e-02 -1.79396525e-01 -2.76833057e-01
   2.17958838e-01 -2.87377350e-02  2.42754027e-01 -1.61960386e-02
  -2.89349079e-01  2.49078572e-01 -2.42237210e-01  6.63713440e-02
   3.04301679e-01 -7.89724588e-02  1.61449492e-01  3.63228559e-01
  -1.36389583e-01 -1.27137452e-01  8.29117671e-02 -2.95981765e-01
  -5.65717742e-02  2.70907968e-01 -5.94051257e-02 -1.95292443e-01
  -1.51184157e-01 -1.61465168e-01  1.11729987e-01  3.11993271e-01
  -5.01736887e-02 -5.51007017e-02  6.31456003e-02 -2.57288311e-02
   8.62975121e-02  1.57344136e-02 -4.01131630e-01  2.33296052e-01
  -1.09693773e-01  1.10275574e-01  6.27225637e-02  2.59595234e-02
   3.49844307e-01 -1.80324554e-01  1.82264894e-01 -1.83446229e-01
   4.85828007e-03  3.14105421e-01  2.71212965e-01  1.94748804e-01
   3.89929742e-01  1.06331132e-01 -1.64094031e-01 -3.35087962e-02
   1.63520426e-01 -2.10108772e-01  2.09802836e-01  8.09360221e-02
   3.85409057e-01  3.13488424e-01 -1.99270666e-01  1.89669356e-01
   8.23362619e-02 -5.29112279e-01 -4.00948256e-01  6.91203959e-03
   2.36600623e-01 -9.73407105e-02 -1.55958354e-01  1.45219550e-01
  -3.56150866e-01  4.99977767e-02  4.68665138e-02  1.53479978e-01
   1.61756322e-01 -1.42169043e-01  2.71783501e-01 -3.10521960e-01
  -3.10410082e-01 -3.60175818e-02  2.55620539e-01 -1.59057491e-02
   8.29919279e-02  2.87620306e-01  7.18925968e-02  3.58470157e-03
  -1.32440180e-01  2.89778829e-01 -4.33209389e-02  1.38173532e-02
  -2.02218860e-01  1.98223159e-01  1.16816305e-01  1.51509881e-01
  -2.22129837e-01  3.66064012e-02  1.04948491e-01 -2.46742889e-01
   1.21340513e-01 -3.15295398e-01 -1.95236802e-01  2.28807941e-01
  -3.20227295e-02  2.62416154e-01 -4.42646891e-02  3.60939980e-01
  -1.24700852e-01 -2.92654246e-01 -3.46971571e-01 -1.66094244e-01
   2.05344319e-01  3.85144323e-01 -1.82528585e-01 -5.90179153e-02
  -4.43433784e-03  5.81435952e-03  2.14744136e-01 -1.02726564e-01
   2.50739217e-01 -2.48765588e-01  9.13758669e-03  7.48458803e-02
  -9.08081830e-02  9.78621244e-02  1.73291028e-01  2.88562596e-01
   7.75828511e-02  9.82538909e-02  3.86179015e-02  1.44321516e-01
  -7.52568990e-02 -1.52308419e-01  4.48018134e-01 -2.69387066e-01
   2.92839762e-02  2.87973229e-02 -1.03211328e-01 -2.90179998e-01
  -2.18467221e-01  2.18811408e-01 -7.53495544e-02 -1.92126781e-01
  -7.79679716e-02  1.69989690e-01  4.27660905e-03 -2.77038842e-01
   5.08962721e-02  4.20171887e-01  7.61865675e-02  1.76110998e-01
  -3.53261828e-02  3.31878439e-02  2.29950309e-01 -1.41442582e-01
  -2.63552964e-02  4.11839813e-01 -9.80626792e-02 -7.67391846e-02
  -2.55942941e-02 -4.08975244e-01 -1.69011533e-01  4.02464420e-02
  -5.99616319e-02 -8.07891637e-02  2.76537985e-01 -9.29032713e-02
   6.94108903e-02  1.19176179e-01 -1.53598279e-01  9.83815417e-02
   2.34342337e-01  2.27628723e-02 -1.58250317e-01 -1.66587695e-01
   1.94486067e-01 -4.34173755e-02 -1.15765251e-01 -3.92295748e-01
  -1.30329490e-01  2.73125947e-01  4.36708659e-01  5.59075534e-01
   8.41551423e-02  9.74657089e-02 -4.65400852e-02  1.46073289e-04
  -9.63210464e-02  6.90899640e-02  6.18999749e-02  6.36083335e-02
   3.61006148e-02 -2.97610015e-01  2.37780407e-01 -2.58159876e-01
   4.85749356e-02 -3.01962718e-02  3.51188257e-02 -3.04782510e-01
   8.81854594e-02  1.04635298e-01  2.68001884e-01 -2.40670085e-01
   8.31288658e-03 -1.04848742e-01  3.48639697e-01  1.41088754e-01
  -2.52189159e-01 -2.91924197e-02 -3.24391723e-01 -5.66660613e-02
   1.80264279e-01  1.70100987e-01 -3.23635757e-01  1.63629234e-01
   1.21774979e-01 -3.98328304e-02 -1.67170644e-01  1.47642404e-01
  -6.36918768e-02  2.65600178e-02  3.31508577e-01  1.44446820e-01
   3.72471996e-02 -2.51289785e-01  7.73359165e-02  9.78391990e-02
   1.67233665e-02  4.48025055e-02 -7.55522847e-02  3.02941918e-01
  -7.89141059e-02 -6.57205507e-02 -1.08076781e-01  1.65995747e-01
  -2.28776515e-01 -3.87861952e-02 -8.18989426e-02 -3.16078603e-01
  -2.93117668e-02 -3.31858173e-02 -4.16649431e-01  4.99394447e-01
  -4.47156206e-02 -1.38757572e-01  3.99303883e-02 -1.62431046e-01
   2.83395350e-01 -3.45272422e-01  9.11808088e-02  1.96322516e-01
  -1.59283459e-01 -1.59887254e-01  4.40367371e-01  1.29309207e-01
   4.48435768e-02  1.24133319e-01  5.82822002e-02  1.25023007e-01
  -4.16485488e-01  8.04581270e-02  5.20613529e-02  2.26026490e-01
   3.26159209e-01  8.46587718e-02 -8.35908353e-02 -1.36753470e-01
   1.69861004e-01  3.24309200e-01  2.47047603e-01  9.08669680e-02
   3.20507944e-01 -6.47970885e-02 -6.96696043e-02 -1.17924415e-01
  -1.84499398e-01 -4.19666439e-01 -5.30235648e-01 -1.66379780e-01
  -1.83254883e-01  2.90002763e-01 -7.16941506e-02  3.14371228e-01
   4.64562550e-02 -3.68258059e-01 -9.38435793e-02 -1.83050379e-01
  -2.12950170e-01  1.57391816e-01 -1.42093569e-01 -2.03733236e-01
  -2.81924605e-01 -1.70254871e-01 -3.01296357e-02  1.31318986e-01
   2.90365219e-01  1.30647942e-01  1.77672684e-01 -5.03494032e-02
   2.32551098e-02  2.89787889e-01  3.63857865e-01 -2.15285107e-01
  -2.09885463e-01 -1.61181092e-01  1.40913241e-02 -2.28003621e-01
   2.13374764e-01 -1.47053316e-01  4.62928042e-03 -6.02834225e-01
   1.51435956e-01  1.80719301e-01 -9.91116017e-02  2.40692079e-01
  -1.10999413e-01  4.73386765e-01 -7.25755543e-02  1.23947740e-01
  -2.72122622e-01  1.68211311e-01 -4.88916695e-01  9.58773540e-04
   1.38745427e-01 -3.58335972e-01  1.05197363e-01 -2.23316312e-01
   2.23571673e-01  2.91059524e-01 -5.17259538e-02 -1.35174006e-01
  -1.22184962e-01 -4.27695513e-01  1.81811333e-01 -8.41559917e-02
   2.43639603e-01 -2.17968538e-01 -4.19782400e-01 -1.62083998e-01
  -1.43112078e-01  4.87887599e-02  2.01536566e-02  1.12948477e-01
  -1.08694553e-01  8.51695463e-02 -9.29312557e-02 -2.56029516e-03
   6.66806877e-01 -5.08032739e-02 -2.49128625e-01  4.25873786e-01
   5.53557724e-02  2.55062461e-01  1.01128057e-01 -1.35206375e-02
  -1.14726365e-01 -8.45154077e-02  3.36188316e-01  1.31884828e-01
  -5.53752296e-02  3.45172524e-01  1.51106045e-02 -3.29217672e-01
  -3.10694814e-01 -1.21165477e-01 -2.20855437e-02 -2.63606519e-01
   2.29718871e-02  1.04218021e-01  2.64508158e-01  5.91837130e-02
  -3.50819081e-01 -8.20543543e-02 -1.02613941e-01 -1.27454875e-02
   1.13678843e-01 -2.84574002e-01 -4.98349130e-01  6.34954795e-02
   1.43877432e-01 -5.48707247e-01 -8.76898877e-03 -1.05480142e-01
  -2.27916524e-01  2.22789332e-01  1.97369978e-01 -3.07678193e-01
   1.14440117e-02  5.11143543e-02 -3.40007618e-02  3.04060161e-01
  -4.29971874e-01  1.02235503e-01 -2.06257194e-01  2.01422364e-01
   1.71159044e-01  2.06720099e-01 -1.24625087e-01  6.88388348e-02
   1.23080589e-01  8.23318064e-02  1.56715587e-01  1.38864771e-01
  -3.40172231e-01 -4.12158638e-01  1.80543631e-01  8.88053104e-02
  -4.40502539e-03 -2.64852643e-01  6.61439374e-02 -4.81596678e-01
   8.38343203e-02 -1.49701238e-01 -6.63630441e-02  1.53225400e-02
  -1.48431703e-01 -1.05786294e-01  3.54352966e-03  1.46149024e-01
  -1.36790559e-01  3.79457444e-01  2.58714825e-01  4.74570274e-01]]
[[-1.24642201e-01 -1.11049995e-01  1.43470243e-01  1.61245421e-01
   4.64105308e-02  1.61525473e-01 -2.89989114e-01 -7.36965984e-02
   1.80324465e-01 -1.39900222e-01  6.93381056e-02  6.08567819e-02
  -4.37026262e-01  5.97211421e-02 -8.76717865e-02 -9.21801478e-02
  -1.64958924e-01  7.27279186e-02  9.02757049e-02  1.16492301e-01
   2.50094056e-01  2.00189516e-01  3.23665217e-02 -9.14303586e-02
   2.22062059e-02 -8.01960230e-02  2.47398168e-01  9.77712497e-02
   2.67344296e-01  2.76375979e-01 -4.61081453e-02 -2.15254351e-01
  -1.64103359e-01  1.32584557e-01  1.09235816e-01 -1.05222844e-01
   1.62878573e-01  2.03773364e-01 -7.35148937e-02 -1.88275978e-01
  -1.27969325e-01 -2.34343886e-01 -4.67200428e-02 -3.84066440e-03
   5.50668091e-02  5.68485618e-01  3.43846560e-01 -2.49334455e-01
   1.33167535e-01  7.86825120e-02  2.04148181e-02 -1.78716660e-01
  -2.30634019e-01 -3.28709856e-02 -1.84144795e-01  3.00956629e-02
   6.39426708e-02  3.55316401e-02  1.84921041e-01  2.69095778e-01
   4.91945110e-02 -1.20419718e-01 -5.62653728e-02  1.96393654e-01
   2.04228356e-01  2.05737874e-01 -2.66713798e-02  1.39156461e-01
   6.33863509e-02 -1.14984110e-01  4.19507802e-01 -4.86294776e-02
   4.12080884e-01  4.01500583e-01 -1.23377733e-01 -5.71067929e-02
   6.44233003e-02  2.23460481e-01 -1.91960663e-01 -3.70247096e-01
   1.56130105e-01  2.69019037e-01 -6.78143874e-02  6.26553595e-02
  -4.02050555e-01 -6.73684198e-03 -8.75882804e-02 -1.79016769e-01
   3.24595086e-02 -3.48600388e-01  5.95737733e-02 -1.32999897e-01
  -3.10589615e-02  2.99625732e-02 -1.79397210e-01 -2.76834697e-01
   2.17958242e-01 -2.87380666e-02  2.42755160e-01 -1.61961988e-02
  -2.89349467e-01  2.49077111e-01 -2.42236286e-01  6.63719550e-02
   3.04302275e-01 -7.89731070e-02  1.61448285e-01  3.63226742e-01
  -1.36388481e-01 -1.27138332e-01  8.29108208e-02 -2.95980364e-01
  -5.65709434e-02  2.70906776e-01 -5.94043285e-02 -1.95291832e-01
  -1.51184976e-01 -1.61465406e-01  1.11728787e-01  3.11992168e-01
  -5.01731746e-02 -5.51009923e-02  6.31463081e-02 -2.57290825e-02
   8.62979218e-02  1.57338213e-02 -4.01131809e-01  2.33296975e-01
  -1.09693624e-01  1.10275708e-01  6.27234057e-02  2.59589441e-02
   3.49844247e-01 -1.80325478e-01  1.82264760e-01 -1.83446586e-01
   4.85866284e-03  3.14105362e-01  2.71213561e-01  1.94749206e-01
   3.89927626e-01  1.06330208e-01 -1.64093882e-01 -3.35090607e-02
   1.63520336e-01 -2.10109562e-01  2.09800720e-01  8.09362754e-02
   3.85409653e-01  3.13487679e-01 -1.99271321e-01  1.89670190e-01
   8.23351666e-02 -5.29112041e-01 -4.00948763e-01  6.91368897e-03
   2.36599296e-01 -9.73419398e-02 -1.55959815e-01  1.45219743e-01
  -3.56149912e-01  4.99981083e-02  4.68670800e-02  1.53478816e-01
   1.61755338e-01 -1.42168760e-01  2.71784276e-01 -3.10521185e-01
  -3.10409576e-01 -3.60176340e-02  2.55620956e-01 -1.59049127e-02
   8.29922780e-02  2.87618399e-01  7.18928128e-02  3.58482078e-03
  -1.32439449e-01  2.89779693e-01 -4.33205292e-02  1.38167609e-02
  -2.02217698e-01  1.98222265e-01  1.16817132e-01  1.51509508e-01
  -2.22130984e-01  3.66065502e-02  1.04948267e-01 -2.46742204e-01
   1.21341318e-01 -3.15294027e-01 -1.95237279e-01  2.28808418e-01
  -3.20230536e-02  2.62416661e-01 -4.42646220e-02  3.60938877e-01
  -1.24701075e-01 -2.92654335e-01 -3.46972167e-01 -1.66093349e-01
   2.05344945e-01  3.85143876e-01 -1.82527736e-01 -5.90179972e-02
  -4.43326123e-03  5.81372716e-03  2.14744598e-01 -1.02727696e-01
   2.50739932e-01 -2.48765782e-01  9.13787354e-03  7.48458281e-02
  -9.08068642e-02  9.78617668e-02  1.73290238e-01  2.88561940e-01
   7.75820464e-02  9.82533842e-02  3.86173725e-02  1.44321278e-01
  -7.52564669e-02 -1.52308792e-01  4.48018104e-01 -2.69385815e-01
   2.92841587e-02  2.87968181e-02 -1.03210360e-01 -2.90179580e-01
  -2.18466908e-01  2.18810499e-01 -7.53500462e-02 -1.92125842e-01
  -7.79671595e-02  1.69988960e-01  4.27593663e-03 -2.77038574e-01
   5.08959144e-02  4.20171946e-01  7.61858076e-02  1.76110849e-01
  -3.53267342e-02  3.31875831e-02  2.29949653e-01 -1.41441569e-01
  -2.63548009e-02  4.11840558e-01 -9.80615467e-02 -7.67394677e-02
  -2.55948957e-02 -4.08974826e-01 -1.69012547e-01  4.02452573e-02
  -5.99617921e-02 -8.07890594e-02  2.76536196e-01 -9.29040238e-02
   6.94106072e-02  1.19175874e-01 -1.53598234e-01  9.83813480e-02
   2.34344244e-01  2.27632411e-02 -1.58249542e-01 -1.66586190e-01
   1.94486603e-01 -4.34170254e-02 -1.15765266e-01 -3.92296255e-01
  -1.30328953e-01  2.73125768e-01  4.36706692e-01  5.59076130e-01
   8.41544271e-02  9.74654853e-02 -4.65399772e-02  1.45771541e-04
  -9.63212624e-02  6.90892115e-02  6.18991852e-02  6.36084229e-02
   3.61011475e-02 -2.97609329e-01  2.37781182e-01 -2.58160144e-01
   4.85752411e-02 -3.01953293e-02  3.51195931e-02 -3.04783940e-01
   8.81848186e-02  1.04635850e-01  2.68002331e-01 -2.40669534e-01
   8.31347890e-03 -1.04848504e-01  3.48639041e-01  1.41089126e-01
  -2.52189517e-01 -2.91924272e-02 -3.24391633e-01 -5.66660017e-02
   1.80264682e-01  1.70101598e-01 -3.23634952e-01  1.63628817e-01
   1.21775590e-01 -3.98329645e-02 -1.67170942e-01  1.47642732e-01
  -6.36921749e-02  2.65594553e-02  3.31509948e-01  1.44446909e-01
   3.72472480e-02 -2.51289785e-01  7.73352310e-02  9.78395194e-02
   1.67231336e-02  4.48023975e-02 -7.55521953e-02  3.02941203e-01
  -7.89149851e-02 -6.57205135e-02 -1.08076110e-01  1.65995225e-01
  -2.28776768e-01 -3.87861244e-02 -8.19000453e-02 -3.16078842e-01
  -2.93118078e-02 -3.31859738e-02 -4.16648954e-01  4.99394715e-01
  -4.47146893e-02 -1.38758838e-01  3.99311408e-02 -1.62432119e-01
   2.83395916e-01 -3.45272571e-01  9.11808982e-02  1.96322605e-01
  -1.59283489e-01 -1.59886122e-01  4.40365672e-01  1.29309386e-01
   4.48434949e-02  1.24132365e-01  5.82823940e-02  1.25022262e-01
  -4.16485518e-01  8.04573298e-02  5.20619117e-02  2.26025224e-01
   3.26160461e-01  8.46587718e-02 -8.35909098e-02 -1.36754438e-01
   1.69861615e-01  3.24309587e-01  2.47048169e-01  9.08657983e-02
   3.20508659e-01 -6.47967085e-02 -6.96704164e-02 -1.17924772e-01
  -1.84498936e-01 -4.19666588e-01 -5.30234396e-01 -1.66378945e-01
  -1.83254078e-01  2.90001601e-01 -7.16952235e-02  3.14371020e-01
   4.64557223e-02 -3.68257970e-01 -9.38451588e-02 -1.83050871e-01
  -2.12949291e-01  1.57392755e-01 -1.42094791e-01 -2.03734457e-01
  -2.81924158e-01 -1.70253873e-01 -3.01300567e-02  1.31318808e-01
   2.90363640e-01  1.30648062e-01  1.77672893e-01 -5.03487587e-02
   2.32541077e-02  2.89786369e-01  3.63856822e-01 -2.15285406e-01
  -2.09885076e-01 -1.61182106e-01  1.40902977e-02 -2.28003502e-01
   2.13376239e-01 -1.47053257e-01  4.62950207e-03 -6.02834225e-01
   1.51434392e-01  1.80719793e-01 -9.91119295e-02  2.40690053e-01
  -1.10998660e-01  4.73387629e-01 -7.25755692e-02  1.23947963e-01
  -2.72122890e-01  1.68211535e-01 -4.88916814e-01  9.58339777e-04
   1.38745964e-01 -3.58335078e-01  1.05197661e-01 -2.23316759e-01
   2.23571122e-01  2.91058987e-01 -5.17263338e-02 -1.35174677e-01
  -1.22184321e-01 -4.27694082e-01  1.81810319e-01 -8.41559842e-02
   2.43640348e-01 -2.17968196e-01 -4.19781178e-01 -1.62084803e-01
  -1.43111378e-01  4.87878397e-02  2.01540254e-02  1.12947345e-01
  -1.08694717e-01  8.51692632e-02 -9.29316208e-02 -2.56072357e-03
   6.66804910e-01 -5.08029908e-02 -2.49128848e-01  4.25874829e-01
   5.53551763e-02  2.55062729e-01  1.01127580e-01 -1.35211544e-02
  -1.14726111e-01 -8.45154449e-02  3.36187959e-01  1.31883413e-01
  -5.53751700e-02  3.45173687e-01  1.51104182e-02 -3.29218298e-01
  -3.10694396e-01 -1.21165015e-01 -2.20854357e-02 -2.63607115e-01
   2.29722243e-02  1.04218230e-01  2.64508069e-01  5.91832437e-02
  -3.50820541e-01 -8.20543990e-02 -1.02614418e-01 -1.27456505e-02
   1.13678172e-01 -2.84573942e-01 -4.98351395e-01  6.34956062e-02
   1.43877849e-01 -5.48709273e-01 -8.76801834e-03 -1.05481595e-01
  -2.27916613e-01  2.22789586e-01  1.97370261e-01 -3.07677627e-01
   1.14440173e-02  5.11138663e-02 -3.40003036e-02  3.04057896e-01
  -4.29973155e-01  1.02236174e-01 -2.06256211e-01  2.01422796e-01
   1.71159267e-01  2.06721500e-01 -1.24625027e-01  6.88389167e-02
   1.23081781e-01  8.23319107e-02  1.56715736e-01  1.38865367e-01
  -3.40172410e-01 -4.12157893e-01  1.80544287e-01  8.88069347e-02
  -4.40459698e-03 -2.64852583e-01  6.61438257e-02 -4.81596410e-01
   8.38347524e-02 -1.49700016e-01 -6.63621053e-02  1.53228194e-02
  -1.48431391e-01 -1.05786666e-01  3.54445539e-03  1.46147847e-01
  -1.36791602e-01  3.79457444e-01  2.58714795e-01  4.74569380e-01]]
