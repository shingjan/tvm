nohup: ignoring input
[01:01:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_nn_pad"
[01:01:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 28, 28), "float32"], T_pad: T.Buffer[(1, 1, 32, 32), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 1, 32, 32):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 2, ax3 - 2], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(2 <= ax2 and ax2 < 30 and 2 <= ax3 and ax3 < 30, placeholder[ax0, ax1, ax2 - 2, ax3 - 2], T_cast[()], dtype="float32")
    

[01:01:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:01:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 28, 28), "float32"], T_pad: T.Buffer[(1, 1, 32, 32), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 1, 32, 32):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2 - 2, ax3 - 2])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(2 <= ax2 and ax2 < 30 and 2 <= ax3 and ax3 < 30, placeholder[ax0, ax1, ax2 - 2, ax3 - 2], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:01:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_conv2d_add_nn_relu"
[01:01:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 32, 32), "float32"], placeholder_1: T.Buffer[(8, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1), "float32"], T_relu: T.Buffer[(1, 8, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1, 32, 32], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 8, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1, 32, 32):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 8, 28, 28, 1, 5, 5):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1, 32, 32], "float32"], ["TENSOR", [8, 1, 5, 5], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 8, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 8, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[01:01:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:01:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 32, 32), "float32"], placeholder_1: T.Buffer[(8, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1), "float32"], T_relu: T.Buffer[(1, 8, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 8, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1, 32, 32], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([8, 1, 5, 5], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 5):
                            for ax0_ax1_ax2_ax3_fused in T.serial(22):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused % 56 // 14 * 7 + ax0_ax1_ax2_ax3_fused % 22 // 2)
                                    v3 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i6_0 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(20):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused // 56 * 4 + ax0_ax1_ax2_ax3_fused // 5)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(5, ax0_ax1_ax2_ax3_fused % 5)
                                    v3 = T.axis.spatial(5, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 5, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused // 56 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_3)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 56 // 14 * 7 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused)
                                    rc = T.axis.reduce(1, 0)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1, 32, 32], "float32"], ["TENSOR", [8, 1, 5, 5], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused // 56 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 56 // 14 * 7 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 1, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 5])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:01:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_max_pool2d"
[01:01:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 28, 28), "float32"], tensor: T.Buffer[(1, 8, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 8, 14, 14, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[01:01:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:01:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 28, 28), "float32"], tensor: T.Buffer[(1, 8, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 8, 14, 14, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:01:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_pad_1"
[01:01:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 14, 14), "float32"], T_pad: T.Buffer[(1, 8, 18, 18), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 8, 18, 18):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 2, ax3 - 2], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(2 <= ax2 and ax2 < 16 and 2 <= ax3 and ax3 < 16, placeholder[ax0, ax1, ax2 - 2, ax3 - 2], T_cast[()], dtype="float32")
    

[01:01:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:01:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 14, 14), "float32"], T_pad: T.Buffer[(1, 8, 18, 18), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 8, 18, 18):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2 - 2, ax3 - 2])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(2 <= ax2 and ax2 < 16 and 2 <= ax3 and ax3 < 16, placeholder[ax0, ax1, ax2 - 2, ax3 - 2], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:01:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_conv2d_add_nn_relu_1"
[01:01:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 18, 18), "float32"], placeholder_1: T.Buffer[(16, 8, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 8, 18, 18], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 16, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 8, 18, 18):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 14, 14, 8, 5, 5):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 8, 18, 18], "float32"], ["TENSOR", [16, 8, 5, 5], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 16, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 16, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[01:01:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:01:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 18, 18), "float32"], placeholder_1: T.Buffer[(16, 8, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 16, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 8, 18, 18], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([16, 8, 5, 5], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 5):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2016):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(8, ax0_ax1_ax2_ax3_fused % 2016 // 252)
                                    v2 = T.axis.spatial(18, ax0_ax1_ax2_ax3_fused % 252 // 14)
                                    v3 = T.axis.spatial(18, i6_0 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(640):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused // 40)
                                    v1 = T.axis.spatial(8, ax0_ax1_ax2_ax3_fused % 40 // 5)
                                    v2 = T.axis.spatial(5, ax0_ax1_ax2_ax3_fused % 5)
                                    v3 = T.axis.spatial(5, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 5, 1, 1, 2, 1, 2, 8, 1, 1, 1, 2, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 49 * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + i2_4)
                                    xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 8, 18, 18], "float32"], ["TENSOR", [16, 8, 5, 5], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 49 * 4 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + ax2)
                                v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 5, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:01:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_max_pool2d_1"
[01:01:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 14, 14), "float32"], tensor: T.Buffer[(1, 16, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 16, 4, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1])
    

[01:01:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:01:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 14, 14), "float32"], tensor: T.Buffer[(1, 16, 4, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 16, 4, 4, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 3 + rv0, ax3 * 3 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:01:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_reshape"
[01:01:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 4, 4), "float32"], T_reshape: T.Buffer[(1, 256), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 256):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4]
    

[01:01:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:01:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 4, 4), "float32"], T_reshape: T.Buffer[(1, 256), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1 in T.grid(1, 256):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, ax1 % 256 // 16, ax1 % 16 // 4, ax1 % 4]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:01:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_dense_add"
[01:01:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 10], dtype="float32")
        for i0, i1, i2 in T.grid(1, 10, 256):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"workload":["dense_small_batch.gpu", ["TENSOR", [1, 256], "float32"], ["TENSOR", [10, 256], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 10):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

[01:01:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:01:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            T_matmul_NT_local = T.alloc_buffer([1, 10], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([1, 256], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([10, 256], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(5, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i2_0 in T.serial(2):
                            for ax0_ax1_fused in T.serial(128):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i2_0 * 128 + ax0_ax1_fused)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(640):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(10, i0_0_i1_0_fused * 5 + ax0_ax1_fused // 128)
                                    v1 = T.axis.spatial(256, i2_0 * 128 + ax0_ax1_fused % 128)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(4, 1, 1, 32, 1, 1):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(1, 0)
                                    j = T.axis.spatial(10, i0_0_i1_0_fused * 5 + i0_1_i1_1_fused)
                                    k = T.axis.reduce(256, i2_0 * 128 + i2_1 * 32 + i2_2)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":1, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 256], "float32"], ["TENSOR", [10, 256], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(1, 1):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(10, i0_0_i1_0_fused * 5 + i0_1_i1_1_fused + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 5, 1, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 4, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
[01:01:49] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                fused_nn_pad_1 |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |            fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[01:01:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad"
[01:01:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:01:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:02:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[01:02:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:03:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[01:03:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[01:03:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[01:04:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[01:04:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9054  0.6215  0.3300  0.1326  0.0971
[01:04:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:04:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:04:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:04:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:04:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_nn_relu"
[01:04:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:04:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:05:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b2c9688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b13aba8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b20a1d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0e29c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b13b378)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b0f48b8)]: 1823 failure(s)
[01:05:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 225 candidate(s)
[01:05:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b2c9688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b13aba8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b20a1d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0e29c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b13b378)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b0f48b8)]: 240 failure(s)
[01:07:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b2c9688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b13aba8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b20a1d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0e29c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b13b378)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b0f48b8)]: 245 failure(s)
[01:08:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b2c9688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b13aba8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b20a1d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0e29c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b13b378)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b0f48b8)]: 239 failure(s)
[01:10:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b2c9688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b13aba8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b20a1d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0e29c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b13b378)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b0f48b8)]: 227 failure(s)
[01:10:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9998  0.9997  0.9997  0.9997  0.9996  0.9995  0.9995  0.9994  0.9986  0.9985  0.9983  0.9981  0.9974  0.9974
[17 : 32]:	0.9970  0.9967  0.9966  0.9965  0.9964  0.9964  0.9956  0.9954  0.9952  0.9952  0.9950  0.9949  0.9948  0.9946  0.9946  0.9946
[01:10:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:10:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:10:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:11:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:11:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[01:11:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:11:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:12:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[01:12:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:12:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[01:12:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[01:13:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[01:13:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[01:13:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9661  0.7240  0.7208  0.5126  0.3871
[01:13:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:13:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:13:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:13:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:13:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad_1"
[01:13:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:13:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:15:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:15:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:16:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:17:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:17:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:18:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:19:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9228  0.9220  0.3775  0.3464  0.1156
[01:19:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:19:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:19:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:19:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:19:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_conv2d_add_nn_relu_1"
[01:19:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:19:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:19:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b0f17d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b148668)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2077b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b095898)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b0e5ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b148ad8)]: 1872 failure(s)
[01:19:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 176 candidate(s)
[01:21:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b0f17d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b148668)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2077b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b095898)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b0e5ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b148ad8)]: 265 failure(s)
[01:22:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b0f17d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b148668)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2077b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b095898)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b0e5ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b148ad8)]: 225 failure(s)
[01:24:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b0f17d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b148668)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2077b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b095898)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b0e5ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b148ad8)]: 236 failure(s)
[01:26:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b0f17d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b148668)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2077b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b095898)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b0e5ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b148ad8)]: 225 failure(s)
[01:26:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9997  0.9996  0.9995  0.9993  0.9993  0.9991  0.9991  0.9991  0.9990  0.9989  0.9987  0.9987  0.9984  0.9977
[17 : 32]:	0.9970  0.9964  0.9962  0.9956  0.9955  0.9954  0.9953  0.9949  0.9949  0.9949  0.9945  0.9944  0.9943  0.9942  0.9942  0.9941
[01:27:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:27:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:27:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:27:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:27:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d_1"
[01:27:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:27:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:28:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[01:28:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:28:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[01:28:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[01:29:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[01:29:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[01:29:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8687  0.5360  0.4210  0.3066  0.0174
[01:29:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:29:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:29:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:29:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:29:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_reshape"
[01:29:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:29:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:29:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[01:29:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:29:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[01:29:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[01:30:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[01:30:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[01:30:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9943  0.9037  0.8264  0.6891  0.1315
[01:30:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:30:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:30:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:30:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:30:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_dense_add"
[01:30:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:30:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:30:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b2c12c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490ae68f38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b0fcf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b24bbe8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b126e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b1f3788)]: 0 failure(s)
[01:30:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:31:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b2c12c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490ae68f38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b0fcf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b24bbe8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b126e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b1f3788)]: 0 failure(s)
[01:32:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b2c12c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490ae68f38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b0fcf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b24bbe8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b126e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b1f3788)]: 0 failure(s)
[01:33:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b2c12c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490ae68f38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b0fcf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b24bbe8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b126e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b1f3788)]: 0 failure(s)
[01:35:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b2c12c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490ae68f38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b0fcf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b24bbe8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b126e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b1f3788)]: 0 failure(s)
[01:35:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9998  0.9997  0.9996  0.9996  0.9994  0.9994  0.9992  0.9992  0.9992  0.9992  0.9987  0.9987  0.9986  0.9986
[17 : 32]:	0.9985  0.9984  0.9981  0.9977  0.9972  0.9969  0.9968  0.9967  0.9965  0.9961  0.9959  0.9958  0.9958  0.9954  0.9953  0.9952
[01:36:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:36:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:36:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:36:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:36:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad"] Trial #0: GFLOPs: 0.0000. Time: 0.0098 ms. Best GFLOPs: 0.0000
[01:36:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad"] Trial #1: GFLOPs: 0.0000. Time: 0.0253 ms. Best GFLOPs: 0.0000
[01:36:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad"] Trial #2: GFLOPs: 0.0000. Time: 0.0058 ms. Best GFLOPs: 0.0000
[01:36:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad"] Trial #3: GFLOPs: 0.0000. Time: 0.0248 ms. Best GFLOPs: 0.0000
[01:36:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_pad"] Trial #4: GFLOPs: 0.0000. Time: 0.0155 ms. Best GFLOPs: 0.0000
[01:37:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_pad"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                fused_nn_pad_1 |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |            fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 5
Total latency (us): 5.83936

[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #0: GFLOPs: 8.4133. Time: 0.0388 ms. Best GFLOPs: 8.4133
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #1: GFLOPs: 61.3457. Time: 0.0053 ms. Best GFLOPs: 61.3457
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #2: GFLOPs: 10.4342. Time: 0.0313 ms. Best GFLOPs: 61.3457
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #3: GFLOPs: 41.0959. Time: 0.0079 ms. Best GFLOPs: 61.3457
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #4: GFLOPs: 93.2127. Time: 0.0035 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #5: GFLOPs: 64.0036. Time: 0.0051 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #6: GFLOPs: 8.0397. Time: 0.0406 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #7: GFLOPs: 43.5240. Time: 0.0075 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #8: GFLOPs: 8.3699. Time: 0.0390 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #9: GFLOPs: 16.2908. Time: 0.0200 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #10: GFLOPs: 5.4136. Time: 0.0602 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #11: GFLOPs: 8.9434. Time: 0.0365 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #12: GFLOPs: 11.1554. Time: 0.0292 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #13: GFLOPs: 92.9597. Time: 0.0035 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #14: GFLOPs: 9.0823. Time: 0.0359 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #15: GFLOPs: 7.0296. Time: 0.0464 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #16: GFLOPs: 44.9871. Time: 0.0072 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #17: GFLOPs: 41.9619. Time: 0.0078 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #18: GFLOPs: 12.2619. Time: 0.0266 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #19: GFLOPs: 85.0734. Time: 0.0038 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #20: GFLOPs: 12.8239. Time: 0.0254 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #21: GFLOPs: 19.1839. Time: 0.0170 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #22: GFLOPs: 24.7659. Time: 0.0132 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #23: GFLOPs: 21.0454. Time: 0.0155 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #24: GFLOPs: 8.7734. Time: 0.0372 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #25: GFLOPs: 53.0448. Time: 0.0061 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #26: GFLOPs: 38.1755. Time: 0.0085 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #27: GFLOPs: 11.5577. Time: 0.0282 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 32, 32), "float32"], placeholder_1: T.Buffer[(8, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1), "float32"], T_relu: T.Buffer[(1, 8, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 8, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1, 32, 32], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([8, 1, 5, 5], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v3 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(8, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 25)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(5, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 25 // 5)
                                        v3 = T.axis.spatial(5, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 5)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 200)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i2_3_init, i2_4_init, i3_4_init in T.grid(7, 4, 7):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(8, i0_2_i1_2_i2_2_i3_2_fused // 4)
                                    yy = T.axis.spatial(28, i2_3_init * 4 + i2_4_init)
                                    xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4_init)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1, 32, 32], "float32"], ["TENSOR", [8, 1, 5, 5], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(5, 5, 1, 1, 7, 1, 1, 1, 1, 1, 1, 4, 7):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(8, i0_2_i1_2_i2_2_i3_2_fused // 4)
                                    yy = T.axis.spatial(28, i2_3 * 4 + i2_4)
                                    xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4)
                                    rc = T.axis.reduce(1, 0)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1, 32, 32], "float32"], ["TENSOR", [8, 1, 5, 5], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(8, i0_2_i1_2_i2_2_i3_2_fused // 4 + ax1)
                            v2 = T.axis.spatial(28, ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 5, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 5, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l179)
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #29: GFLOPs: 30.2337. Time: 0.0108 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #30: GFLOPs: 34.2284. Time: 0.0095 ms. Best GFLOPs: 93.2127
[01:37:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #31: GFLOPs: 92.7652. Time: 0.0035 ms. Best GFLOPs: 93.2127
[01:37:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_conv2d_add_nn_relu"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                fused_nn_pad_1 |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |            fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 37
Total latency (us): 9.33829

[01:37:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 1.2491. Time: 0.0050 ms. Best GFLOPs: 1.2491
[01:37:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 0.2681. Time: 0.0234 ms. Best GFLOPs: 1.2491
[01:37:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 0.3057. Time: 0.0205 ms. Best GFLOPs: 1.2491
[01:37:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 0.4813. Time: 0.0130 ms. Best GFLOPs: 1.2491
[01:37:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 1.1842. Time: 0.0053 ms. Best GFLOPs: 1.2491
[01:37:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |            fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 42
Total latency (us): 14.3595

[01:37:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad_1"] Trial #0: GFLOPs: 0.0000. Time: 0.0191 ms. Best GFLOPs: 0.0000
[01:37:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0184 ms. Best GFLOPs: 0.0000
[01:37:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad_1"] Trial #2: GFLOPs: 0.0000. Time: 0.0152 ms. Best GFLOPs: 0.0000
[01:37:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad_1"] Trial #3: GFLOPs: 0.0000. Time: 0.0199 ms. Best GFLOPs: 0.0000
[01:37:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_pad_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0462 ms. Best GFLOPs: 0.0000
[01:38:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_pad_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |            fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 47
Total latency (us): 29.5383

[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #0: GFLOPs: 58.8139. Time: 0.0214 ms. Best GFLOPs: 58.8139
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #1: GFLOPs: 127.7298. Time: 0.0099 ms. Best GFLOPs: 127.7298
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #2: GFLOPs: 74.2471. Time: 0.0170 ms. Best GFLOPs: 127.7298
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #3: GFLOPs: 49.8280. Time: 0.0253 ms. Best GFLOPs: 127.7298
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #4: GFLOPs: 77.0491. Time: 0.0164 ms. Best GFLOPs: 127.7298
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #5: GFLOPs: 48.4903. Time: 0.0260 ms. Best GFLOPs: 127.7298
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #6: GFLOPs: 63.0194. Time: 0.0200 ms. Best GFLOPs: 127.7298
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #7: GFLOPs: 45.4574. Time: 0.0277 ms. Best GFLOPs: 127.7298
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #8: GFLOPs: 210.2933. Time: 0.0060 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #9: GFLOPs: 42.9071. Time: 0.0294 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #10: GFLOPs: 157.7994. Time: 0.0080 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #11: GFLOPs: 86.4715. Time: 0.0146 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #12: GFLOPs: 122.5621. Time: 0.0103 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #13: GFLOPs: 51.9534. Time: 0.0243 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #14: GFLOPs: 27.4896. Time: 0.0459 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #15: GFLOPs: 118.1846. Time: 0.0107 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #16: GFLOPs: 28.2846. Time: 0.0446 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #17: GFLOPs: 33.9354. Time: 0.0371 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #18: GFLOPs: 40.3830. Time: 0.0312 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #19: GFLOPs: 31.7369. Time: 0.0397 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #20: GFLOPs: 72.3477. Time: 0.0174 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #21: GFLOPs: 110.2907. Time: 0.0114 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #22: GFLOPs: 39.4504. Time: 0.0320 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #23: GFLOPs: 115.3078. Time: 0.0109 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #24: GFLOPs: 21.1692. Time: 0.0596 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 18, 18), "float32"], placeholder_1: T.Buffer[(16, 8, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 8, 18, 18], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 8, 5, 5], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_4_init in T.serial(14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            yy = T.axis.spatial(14, i2_4_init)
                            xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 8, 18, 18], "float32"], ["TENSOR", [16, 8, 5, 5], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(8, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 324)
                                        v2 = T.axis.spatial(18, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 324 // 18)
                                        v3 = T.axis.spatial(18, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 18)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1296)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(29):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 100)
                                    v1 = T.axis.spatial(8, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 100 // 25)
                                    v2 = T.axis.spatial(5, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 25 // 5)
                                    v3 = T.axis.spatial(5, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 5)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 1600)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 1, 5, 5, 1, 1, 14, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                yy = T.axis.spatial(14, i2_4)
                                xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(8, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 8, 18, 18], "float32"], ["TENSOR", [16, 8, 5, 5], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(14, ax2)
                            v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 5])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 5])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 56])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #26: GFLOPs: 18.6201. Time: 0.0677 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #27: GFLOPs: 84.4417. Time: 0.0149 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #28: GFLOPs: 106.1968. Time: 0.0119 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #29: GFLOPs: 69.5106. Time: 0.0181 ms. Best GFLOPs: 210.2933
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 18, 18), "float32"], placeholder_1: T.Buffer[(16, 8, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_relu: T.Buffer[(1, 16, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 8, 18, 18], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 8, 5, 5], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i3_4_init in T.serial(2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 8, 18, 18], "float32"], ["TENSOR", [16, 8, 5, 5], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(8, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 672 // 84)
                                            v2 = T.axis.spatial(18, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 84 // 14)
                                            v3 = T.axis.spatial(18, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 40)
                                            v1 = T.axis.spatial(8, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 40 // 5)
                                            v2 = T.axis.spatial(5, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 5)
                                            v3 = T.axis.spatial(5, i6_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 320)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 5, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                    xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                    rc = T.axis.reduce(8, i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 8, 18, 18], "float32"], ["TENSOR", [16, 8, 5, 5], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 5, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l177)
[01:38:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_1"] Trial #31: GFLOPs: 247.3760. Time: 0.0051 ms. Best GFLOPs: 247.3760
[01:39:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_conv2d_add_nn_relu_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |            fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 79
Total latency (us): 34.6345

[01:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #0: GFLOPs: 0.1946. Time: 0.0118 ms. Best GFLOPs: 0.1946
[01:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #1: GFLOPs: 0.4051. Time: 0.0057 ms. Best GFLOPs: 0.4051
[01:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #2: GFLOPs: 0.1038. Time: 0.0222 ms. Best GFLOPs: 0.4051
[01:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #3: GFLOPs: 0.6771. Time: 0.0034 ms. Best GFLOPs: 0.6771
[01:39:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_max_pool2d_1"] Trial #4: GFLOPs: 0.6567. Time: 0.0035 ms. Best GFLOPs: 0.6771
[01:39:48] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |            fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 84
Total latency (us): 38.0373

[01:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0061 ms. Best GFLOPs: 0.0000
[01:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0092 ms. Best GFLOPs: 0.0000
[01:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0045 ms. Best GFLOPs: 0.0000
[01:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0058 ms. Best GFLOPs: 0.0000
[01:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0058 ms. Best GFLOPs: 0.0000
[01:40:18] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_reshape"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |            N/A |          N/A |                   N/A |      0 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 89
Total latency (us): 42.5363

[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #0: GFLOPs: 0.1094. Time: 0.0469 ms. Best GFLOPs: 0.1094
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #1: GFLOPs: 0.5572. Time: 0.0092 ms. Best GFLOPs: 0.5572
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #2: GFLOPs: 0.1344. Time: 0.0382 ms. Best GFLOPs: 0.5572
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_dense_add"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"layout_free_placeholders": [1], "global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 10], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 256], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([10, 256], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 5):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(10, i1_3_init * 5 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":1, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 256], "float32"], ["TENSOR", [10, 256], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(4):
                        for ax0_ax1_fused_0 in T.serial(32):
                            for ax0_ax1_fused_1 in T.thread_binding(1, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i2_0 * 64 + ax0_ax1_fused_0 * 2 + ax0_ax1_fused_2)
                                        T.reads(placeholder[v0, v1])
                                        T.writes(placeholder_shared[v0, v1])
                                        placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(320):
                            for ax0_ax1_fused_1 in T.thread_binding(1, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(10, (ax0_ax1_fused_0 * 2 + ax0_ax1_fused_2) // 64)
                                        v1 = T.axis.spatial(256, i2_0 * 64 + (ax0_ax1_fused_0 * 2 + ax0_ax1_fused_2) % 64)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(4, 1, 2, 16, 1, 5):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(10, i1_3 * 5 + i1_4)
                                k = T.axis.reduce(256, i2_0 * 64 + i2_1 * 16 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":1, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 256], "float32"], ["TENSOR", [10, 256], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 10):
                        with T.block("T_matmul_NT_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 5])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[4, 4, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 1, 2])
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 1, 2])
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #4: GFLOPs: 0.3011. Time: 0.0170 ms. Best GFLOPs: 0.5572
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #5: GFLOPs: 0.1407. Time: 0.0365 ms. Best GFLOPs: 0.5572
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #6: GFLOPs: 0.1070. Time: 0.0480 ms. Best GFLOPs: 0.5572
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #7: GFLOPs: 0.3498. Time: 0.0147 ms. Best GFLOPs: 0.5572
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #8: GFLOPs: 0.1007. Time: 0.0509 ms. Best GFLOPs: 0.5572
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_dense_add"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"layout_free_placeholders": [1], "global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 10], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 256], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([10, 256], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(5, thread="threadIdx.x"):
                    with T.block("T_matmul_NT_init"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(10, i0_1_i1_1_fused * 5 + i0_2_i1_2_fused)
                        T.reads()
                        T.writes(T_matmul_NT_local[i, j])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":1, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 256], "float32"], ["TENSOR", [10, 256], "float32"], None, "float32"]})
                        T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(2):
                        for ax0_ax1_fused_0 in T.serial(26):
                            for ax0_ax1_fused_1 in T.thread_binding(5, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i2_0 * 128 + (ax0_ax1_fused_0 * 5 + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * 5 + ax0_ax1_fused_1 < 128)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(128):
                            for ax0_ax1_fused_1 in T.thread_binding(5, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(10, (ax0_ax1_fused_0 * 10 + ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2) // 128)
                                        v1 = T.axis.spatial(256, i2_0 * 128 + (ax0_ax1_fused_0 * 10 + ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2) % 128)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(32, 1, 1, 4, 1, 1):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(10, i0_1_i1_1_fused * 5 + i0_2_i1_2_fused)
                                k = T.axis.reduce(256, i2_0 * 128 + i2_1 * 4 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":1, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 256], "float32"], ["TENSOR", [10, 256], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 1):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(10, i0_1_i1_1_fused * 5 + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 2, 5, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 32, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 5])
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 5, 2])
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #10: GFLOPs: 0.1060. Time: 0.0484 ms. Best GFLOPs: 0.5572
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #11: GFLOPs: 0.0913. Time: 0.0562 ms. Best GFLOPs: 0.5572
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #12: GFLOPs: 0.7769. Time: 0.0066 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #13: GFLOPs: 0.3364. Time: 0.0152 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #14: GFLOPs: 0.0799. Time: 0.0642 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #15: GFLOPs: 0.1436. Time: 0.0357 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #16: GFLOPs: 0.2283. Time: 0.0225 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #17: GFLOPs: 0.1886. Time: 0.0272 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #18: GFLOPs: 0.5395. Time: 0.0095 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_dense_add"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"layout_free_placeholders": [1], "global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 10], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 256], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([10, 256], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                    for i1_3_init in T.serial(5):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(10, i0_1_i1_1_fused * 5 + i1_3_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":1, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 256], "float32"], ["TENSOR", [10, 256], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(4):
                        for ax0_ax1_fused_0 in T.serial(64):
                            for ax0_ax1_fused_1 in T.thread_binding(1, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i2_0 * 64 + ax0_ax1_fused_0)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(640):
                            for ax0_ax1_fused_1 in T.thread_binding(1, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(10, ax0_ax1_fused_0 // 64)
                                    v1 = T.axis.spatial(256, i2_0 * 64 + ax0_ax1_fused_0 % 64)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(1, 1, 5, 64, 1, 1):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(10, i0_1_i1_1_fused * 5 + i1_3)
                                k = T.axis.reduce(256, i2_0 * 64 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":1, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 256], "float32"], ["TENSOR", [10, 256], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 5):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(10, i0_1_i1_1_fused * 5 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 2, 1, 5, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[4, 1, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 1])
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68 = sch.split(loop=l66, factors=[None, 1])
sch.bind(loop=l68, thread_axis="threadIdx.x")
b69 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b69, ann_key="meta_schedule.unroll_explicit")
b70, b71, b72, b73 = sch.get_child_blocks(b69)
l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l86, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l86, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="T_matmul_NT", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b101)
b112 = sch.decompose_reduction(block=b101, loop=l105)
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #20: GFLOPs: 0.6226. Time: 0.0082 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #21: GFLOPs: 0.1119. Time: 0.0459 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #22: GFLOPs: 0.1206. Time: 0.0425 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #23: GFLOPs: 0.5625. Time: 0.0091 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #24: GFLOPs: 0.1168. Time: 0.0439 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_dense_add"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256), "float32"], placeholder_1: T.Buffer[(10, 256), "float32"], placeholder_2: T.Buffer[(1, 10), "float32"], T_add: T.Buffer[(1, 10), "float32"]) -> None:
        # function attr dict
        T.func_attr({"layout_free_placeholders": [1], "global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 10], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 256], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([10, 256], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(5, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                    with T.block("T_matmul_NT_init"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(10, i0_1_i1_1_fused * 2 + i0_2_i1_2_fused)
                        T.reads()
                        T.writes(T_matmul_NT_local[i, j])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":1, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 256], "float32"], ["TENSOR", [10, 256], "float32"], None, "float32"]})
                        T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(2):
                        for ax0_ax1_fused_0 in T.serial(64):
                            for ax0_ax1_fused_1 in T.thread_binding(2, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i2_0 * 128 + ax0_ax1_fused_0 * 2 + ax0_ax1_fused_1)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(160):
                            for ax0_ax1_fused_1 in T.thread_binding(2, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(10, (ax0_ax1_fused_0 * 8 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 128)
                                        v1 = T.axis.spatial(256, i2_0 * 128 + (ax0_ax1_fused_0 * 8 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 128)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(64, 1, 1, 2, 1, 1):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(10, i0_1_i1_1_fused * 2 + i0_2_i1_2_fused)
                                k = T.axis.reduce(256, i2_0 * 128 + i2_1 * 2 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":1, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 256], "float32"], ["TENSOR", [10, 256], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 1):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(10, i0_1_i1_1_fused * 2 + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 5, 2, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 64, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 2])
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 2, 4])
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #26: GFLOPs: 0.1121. Time: 0.0457 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #27: GFLOPs: 0.3172. Time: 0.0162 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #28: GFLOPs: 0.2849. Time: 0.0180 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #29: GFLOPs: 0.1004. Time: 0.0511 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #30: GFLOPs: 0.1477. Time: 0.0347 ms. Best GFLOPs: 0.7769
[01:40:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_dense_add"] Trial #31: GFLOPs: 0.0860. Time: 0.0597 ms. Best GFLOPs: 0.7769
[01:40:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_dense_add"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[01:40:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad_1"
[01:40:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:40:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[01:42:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:42:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[01:43:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:45:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:46:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:48:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:49:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[01:49:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[01:49:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[01:49:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[01:49:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[01:49:17] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_pad_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[01:49:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad_1"
[01:49:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:49:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[01:50:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:50:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[01:52:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:54:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:56:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:58:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[01:59:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[01:59:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[01:59:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[01:59:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[01:59:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[01:59:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_pad_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |            
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[01:59:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_dense_add"
[01:59:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #7 has finished. Remaining task(s): 7
[01:59:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad"
[01:59:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:59:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:00:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:00:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:01:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:02:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:03:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:05:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:05:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:05:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:05:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_pad"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |            
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[02:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_conv2d_add_nn_relu_1"
[02:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #4 has finished. Remaining task(s): 6
[02:06:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad_1"
[02:06:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:06:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:07:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:07:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:09:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:10:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:12:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:14:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:15:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:15:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:15:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:15:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:15:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:15:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_pad_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[02:15:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[02:15:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:15:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:15:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[02:15:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:16:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[02:17:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[02:17:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[02:18:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[02:18:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:18:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:18:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[02:18:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_reshape"
[02:18:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:18:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:18:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[02:18:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:19:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[02:19:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[02:20:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[02:20:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[02:20:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:20:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:20:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:20:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:20:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:20:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_reshape"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[02:20:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad_1"
[02:20:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:20:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:22:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:22:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:23:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:24:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:26:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:28:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:29:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:29:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:29:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_pad_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |            
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[02:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_nn_relu"
[02:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #1 has finished. Remaining task(s): 5
[02:29:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d_1"
[02:29:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:29:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:29:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[02:29:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:29:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[02:30:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[02:30:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[02:31:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[02:31:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:31:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:31:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:31:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:31:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:31:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |            
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[02:31:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_pad_1"
[02:31:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:31:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:33:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:33:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:35:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:37:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:38:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:40:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b11f408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b2bf758)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b112748)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b284288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b167fb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2be808)]: 0 failure(s)
[02:41:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:41:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:41:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:41:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #3 has finished. Remaining task(s): 4
[02:41:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad"
[02:41:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:41:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:42:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:42:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:43:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:44:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:45:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:46:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:47:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:47:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:47:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:47:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:47:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:47:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_pad"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[02:47:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[02:47:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:47:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:47:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[02:47:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:47:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[02:48:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[02:49:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[02:50:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[02:50:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:50:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:50:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:50:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:50:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:50:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[02:50:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_reshape"
[02:50:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:50:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:50:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[02:50:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:50:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[02:51:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[02:51:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[02:51:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[02:51:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:51:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:51:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:51:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:51:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:51:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_reshape"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[02:51:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad"
[02:51:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:51:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:52:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:52:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:54:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:55:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:56:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:57:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[02:58:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[02:58:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[02:58:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[02:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[02:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[02:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_pad"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[02:58:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d_1"
[02:58:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:58:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[02:58:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[02:58:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[02:59:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[02:59:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:00:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:00:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:01:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:01:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:01:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:01:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:01:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:01:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[03:01:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[03:01:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:01:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:01:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:01:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:02:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:02:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:03:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:04:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:04:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:04:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:04:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:04:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:04:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:04:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[03:04:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_reshape"
[03:04:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:04:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:04:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:04:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:05:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:05:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:05:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:05:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:06:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:06:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:06:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:06:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:06:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:06:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_reshape"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[03:06:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad"
[03:06:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:06:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:07:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[03:07:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:08:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[03:09:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[03:10:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[03:11:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[03:12:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:12:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:12:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:12:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:12:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:12:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_pad"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[03:12:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[03:12:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:12:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:13:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:13:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:13:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:14:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:14:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:15:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:15:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:15:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:15:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:15:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:15:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:15:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |            
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[03:15:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_pad"
[03:15:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:15:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:16:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[03:16:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:17:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[03:19:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[03:20:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[03:21:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b13dfe8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b27b888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b104fb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b0ede78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b22baa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b15b148)]: 0 failure(s)
[03:22:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:22:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:22:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:22:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #0 has finished. Remaining task(s): 3
[03:22:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d_1"
[03:22:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:22:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:22:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:22:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:23:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:23:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:24:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:24:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:25:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:25:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:25:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:25:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:25:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:25:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[03:25:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_reshape"
[03:25:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:25:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:25:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:25:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:25:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:26:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:26:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:27:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:27:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:27:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:27:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:27:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:27:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:27:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_reshape"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |            
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |            
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[03:27:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_max_pool2d"
[03:27:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:27:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:27:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:27:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:28:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:28:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:29:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:30:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b1591e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b10cb88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b103a08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b10e658)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b2c9b28)]: 0 failure(s)
[03:30:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:30:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:31:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:31:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #2 has finished. Remaining task(s): 2
[03:31:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_reshape"
[03:31:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:31:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:31:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:31:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:31:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:31:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:31:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:32:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b102e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b147368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b2c8408)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b20a588)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1142c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b122198)]: 0 failure(s)
[03:32:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:32:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:32:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:32:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #6 has finished. Remaining task(s): 1
[03:32:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d_1"
[03:32:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:32:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:32:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:32:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:33:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:33:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:34:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:34:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:34:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:34:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:34:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:34:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[03:34:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[03:34:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_max_pool2d_1"
 ID |                          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-------------------------------------------------------------------------------------------------------------------------------------
  0 |                  fused_nn_pad |       1 |      1 |         0.0002 |       5.8394 |                5.8394 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_relu |  326144 |      1 |        93.2127 |       3.4989 |                3.4989 |     32 |          Y 
  2 |           fused_nn_max_pool2d |    6272 |      1 |         1.2491 |       5.0212 |                5.0212 |      5 |          Y 
  3 |                fused_nn_pad_1 |       1 |      1 |         0.0001 |      15.1788 |               15.1788 |      5 |          Y 
  4 | fused_nn_conv2d_add_nn_relu_1 | 1260672 |      1 |       247.3760 |       5.0962 |                5.0962 |     32 |          Y 
  5 |         fused_nn_max_pool2d_1 |    2304 |      1 |         0.6771 |       3.4028 |                3.4028 |      5 |            
  6 |                 fused_reshape |       1 |      1 |         0.0002 |       4.4990 |                4.4990 |      5 |          Y 
  7 |            fused_nn_dense_add |    5130 |      1 |         0.7769 |       6.6033 |                6.6033 |     32 |          Y 
-------------------------------------------------------------------------------------------------------------------------------------
Total trials: 121
Total latency (us): 49.1396

[03:34:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_max_pool2d_1"
[03:34:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:34:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[03:35:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:35:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[03:35:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:36:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:36:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:37:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56490b28eff8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56490b10d2c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56490b1ec1a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56490b1e8378)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56490b1214a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56490b29b9d8)]: 0 failure(s)
[03:37:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[03:37:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[03:37:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[03:37:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #5 has finished. Remaining task(s): 0
Starting to build with relay.
/home/yj/anaconda3/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[[-4.0810513  3.2317207  3.2099154 -1.0010906  2.2484517 -2.2399905
   1.8773543  0.5908688  2.1268446 -6.4082103]]
[[-4.0810513  3.231719   3.209916  -1.0010916  2.248454  -2.239992
   1.8773564  0.5908654  2.126845  -6.4082108]]
